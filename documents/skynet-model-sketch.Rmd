title: "Skynet Model Sketch"
output: html_notebook
---

```{r}
knitr::opts_chunk$set(eval = F)

library(tidyverse)
library(FishData)
library(stringr)
library(lubridate)
library(rfishbase)
```


This is a summary of the proposed model structure for version 1.0 of the "skynet" model. The objective of this model is to estimate spatio-temporal indicies of abundance using GFW effort data. The general method is an adaptation of @Miller2016, wherein the authors attempt to estimate quota lease prices by fitting observed patterns of effort to predicted patterns of effort based on the expected profitability of fishing different patch (subject to the constraint that marginal profits are equal in each patch). Your goal is to flip this around a bit, and now predicted relative abundance as a function of the effort, price, and cost for fishing that patch, again conditional on the same assumption as above. 


# Model and Data

$$Effort_{f,t,s} = \frac{1}{q_{f,t,s}}log(\frac{p_{f,t,s}B_{f,t,s}}{c_{f,t,s} + \bar{\pi}_{f,t}})$$

$$B = \frac{e^{Eq}(c + \pi)}{p}$$

Note that *c* here are variable costs per unit  effort ($/hour). You're omitting the quota price for now (though that might need to be incorporated in here at some point)

$$[p,c,q,\bar{\pi}| B] \propto [B |p,c,q,\bar{\pi},effort][priors]$$

In english, the likelihood of price, cost, q, and marginal profits conditional on Biomass is proportional to the likelihood of the biomass conditional on a given price, cost, q, and marginal profits and data effort and priors. 

So, how to fit this fun thing? 

notation. Let's call species *s*, location *l*, and time *t*, and fleet *f*

## price *p*

You probably don't need to actuall estimate *p*. You can match the species in the EBS trawl survey data to Tyler's price database as a starting point. So, in that world, you have $p_{s}$. You could also, if you can link gear types from GFW to type of product, in which case you would have $p_{s,f}$

## cost *c*

This is in units of $/hour (where hour is presumably fishing hour, not travel time), so that travel time is incororated into the cost of actual fishing. From @Miller2016, they compute VC, which you call *c*, as fuel, labor, ice, distance from port. As a starting point, you could focus on distance from port and vessel size. Something like 

$$c_{l,f} = f(distance,fuel,labor, misc)$$

You can get distance from GFW data. Fuel prices you could either estimate, take global averages, or better yet, scrape from [here](bunkerindex.com). Labor costs are a little tricker. You could just make it proportional to vessel size and estimate something from there. Or, you could use [BLS data](https://www.bls.gov/oes/current/oes453011.htm) for the US as a starting point, seems like a good idea. Misc costs I think you have to ignore for now. 

So, the way that they do it in @Miller2016 is to calculate the cost per trip, where that is presumably $fuel*distance + labor * time$, where *time* is a function of distance, that you might be able to get from GFW. You would then divide that total cost by the average number of fishing hours exerted at that patch, to get to a cost per unit fishing effort at that patch. That seems a little goofy to me though, since it says that say you have a reallllly far away patch that people fish at a lot though, that's going to say that the cost per unit effort is really low out there, where in fact it is really damn high. I could actually see the opposite, where per trip you fish a lot on a very far away place, since you're not going to go back and forth from port a bunch, but rather stay out there and fish. But, let's go with it for now, since it's what they do in that paper. Best thing to do is start with that. So, for each patch/fleet then, you'll calculate the average number of fishing hours per "trip" out there, which you'll need to poke at the data to really understand. So, to match @Miller2016 exactly. For each patch in the database, you would calcualte it's distance from port. For each fleet type, you would get something like miles per gallon. You would then get cost in fuel. For each fleet, you would also get average speed, and use that to get the time taken to get to each place, and multiply that by cost per hour times crew size. Add that to the fuel costs, and you have the total costs to fish at a site. Divide that by the average number of fishing hours at the site, and you've got yourself cost per unit fishing effort at said site. 

There's a few problems with that. I'm still not comfortable with the divide by average fishing hours per trip at site, since I feel like it's likely to really skew the cost for far away sites that people hang at for a long time. There's also the issue that you can string sites together, so the marginal cost of fishing around the priblofs is spread out across the sites out there. But let's maybe go with it for now as a starting point. 

As an alternative, you could estimate these. So, say cost is something like 

$$c = aDistance + bLength + cDistanceLength + constant$$

and you could fit those parameters as part of the model, possible with some informative priors

## q

Ah ye old catchability. You obviously can't have $q_{f,l}$, since that would perfectly fit your data. So, that leaves $q_{f}$. That's a straight nuisance paramter, though you could in theory make it a function of depth at the site or something like that as well if you wanted to try and make it spatial. 


## marginal profits

Set this to zero for now, estimate later, but really doesn't matter for within season comparison. 


## Likelihoods


The simplest model you could write of this then would be something like

$$ [q,\sigma|log(B)] \propto norm(log(B)|log(g(p,c,\Pi,q,E)),\sigma)gamma(q,scale,shape)inversegamma(\sigma,mean,sd) $$

So, you're just tuning fleet with catchability to solve this problem. Pretty damn minimalistic. Let's see if it works. as a starting point. 

From there, you can go into tuning the cost and q parameters, as well as changing marginal profits by patch and fleet. 

OK this is why you have weekends. There's no way that that can work, unless you have the cost parameters so damn tuned in that the expected biomass is literally just a solvable function of cost, price, and effort. All that one q parameter would do is scale the relationship between the amount of effort itself and the amount of effort and the amount of biomass expected. 

It would be worth exploring to see how essentially deterministic based on the model of @Miller2016 things are. Maybe the data and the model are just that good that it works that well! That would be pretty weird though. In reality then, you'll need a few more options. Let's brainstorm a bit here. Assuming that that doesn't work, you'll need to mess with a few other parameters. The simplest solution is basically that the model of @Miller2016 is "right", and you just need to be able to fine tune the parameters of the model. So, in that framework, you tune parameters to cost/price/q to better match the data. This is basically saying that there are costs/price/q factors that you can't get at just by downloading gas prices, and using the model to solve for those. You could think about informative priors in there to deal with colinearity among parameters (cost and q will be confounded for example, unless you make cost an informative function of distance or something). You could also think about making sub-regions of ideal free distributions to help hone in on things a bit. 

That approach assumes that the underlying model itself is right. There are a few alternatives though. You could think about more of a "choice" model, similar to Becca's urchin paper, though I'm not clear that in theory that wouldn't just get you to an ideal free distribution prediction. You'd need to do some literature reading to think through other general structures for thinking about fleet allocation. 

As another alternative, which I kind of like best as a starting point, is to explore a random forest framework. Your goal is out of sample prediction of CPUE as a function of a bunch of stuff, that may interact in unexpected and non-linear ways.  That sure soundes like a decision tree problem to me. Basically, let the massive amounts of data you have speak for itself. You could envision a thing where it's trying to predict biomass, and it makes decisions based on the amount of effort and interactions with vessel size, depth, distance from port, etc. Chris probably won't like it, but it would certainly be interesting to compare these results to a "structural" approach to prediction. If it performs substantially differently, either suggests that you need to do a better job estimating your cost parameters, or could iy maybe provide a clue as to the "true" structure of the complex system? I really think this is the way to start, and Plus, I've wanted to learn more about these processes for a while, so this seems like a goof excuse as any. 


The big improvement as a starting point of a random forest framework is that it's assumption free: It's goal is to get the best possible thing for out of sample prediction, which is really what you want in this case. There's still value in testing a more specific question, like would a structural approach get you either a) the same quality of prediction with some tractability behind the "why" and or b) an ability to "test" ability of standard econometric models. But, if the pure goal is use X to predict Y, why not use the best tool for the job!










## Issues

The simplest thing would be to predict the biomass of the species assemblage across a metier, repeated for each metier. The fancier model would try and aggregate the net f across fleets.... deal with this later. 

# EBS Trawl Data

Goal here is to explore how you want to parse apart, understand, and augment the EBS trawl survey data. 


```{r}

ebs_trawl <-
  FishData::download_catch_rates(survey = "EBSBTS", species_set = 100) %>%
  as_data_frame()

ebs_trawl_summary <- ebs_trawl %>%
  set_names(tolower(colnames(.))) %>%
  mutate(round_lat = round(lat, 1),
         round_lon = round(long, 1)) %>%
  group_by(year, round_lat, round_lon) %>%
  summarise(mean_cpue = mean(wt, na.rm = T)) %>%
  mutate(log_mean_cpue = log(mean_cpue))

ram_stocks <- readxl::read_excel("~/Box Sync/Databases/RAM v3.80/DB Files With Assessment Data/RLSADB v3.80 (assessment data only).xlsx",
                   sheet = 'stock') %>%
  mutate(scientificname = tolower(scientificname))

ram_biopars <- readxl::read_excel("~/Box Sync/Databases/RAM v3.80/DB Files With Assessment Data/RLSADB v3.80 (assessment data only).xlsx",
                   sheet = 'bioparams') 


ram_bio_data <- ram_biopars %>% 
  left_join(ram_stocks, by = 'stockid')

size_data <- ram_bio_data %>% 
  filter(str_detect(bioid,'Linf')) %>% 
  mutate(biovalue = as.numeric(biovalue)) %>% 
  mutate(biovalue = ifelse(str_detect(bioid,'mm'), biovalue/10, biovalue ))


simpleCap <- function(x) {
  s <- strsplit(x, " ")[[1]]
  paste(toupper(substring(s, 1,1))[1], substring(s, 2),
      sep="", collapse=" ")
}

ram_stocks <- ram_stocks %>% 
  mutate(stupid_fishbase_name = Hmisc::capitalize(scientificname))

depth_data <- species(species_list = unique(ram_stocks$stupid_fishbase_name), fields = species_fields$depth) %>% 
  mutate(sorted_sciname = fct_reorder(sciname, DepthRangeDeep))

depth_data %>% 
  ggplot(aes(sorted_sciname, ymin = DepthRangeShallow, ymax = DepthRangeDeep)) + 
  geom_linerange() + 
  coord_flip()

hist(size_data$biovalue %>% log() %>% pmin(1500))

ram_stocks$scientificname[ram_stocks$scientificname == 'theragra chalcogramma'] <- 'gadus chalcogrammus'

alaska_ram_stocks <- ram_stocks %>%
  filter(str_detect(region,'Alaska')) %>%
  dplyr::select(scientificname) %>%
  unique() %>%
  unlist() %>%
  as.character() %>%
  tolower()


ram_ebs_trawl <- ebs_trawl %>%
  set_names(tolower(colnames(.))) %>%
  mutate(round_lat = round(lat, 1),
         round_lon = round(long, 1),
         scientificname = str_replace(sci,'_',' ') %>% tolower()) %>%
  filter(scientificname %in% alaska_ram_stocks)


```
So, you can pretty easily get depth data and length data, which seem like two simple ways to fill in something like commonly captured species (size and depth overlap), as well as think a bit about cost by species (depth). 





```{r}


p = 10

c = 0

v = 3.4

pi = 6.2

b = 102

q = .2


effort <- 1/q * log( ((p - c) * b)/ (v + pi) )

bhat <- (exp(effort * q)* (v + pi) )/(p - c)

(p + c) / (pi * exp(-q*effort))

```

# Fitting technique


## Random forest development

the `randomforest` package does the bootstrapping for you, but doesn't do the learning curve (what proportion of the data are training and what are test) process (though the OOB is still out of sample). But, you still have the full sample size each step. So, learning curve then says if you only have access to 20% of the data, and want to predict the other 80%, how do you do. To get to that, check out `pipelearner`


```{r}
library(randomForest)
library(ggRandomForests)

set.seed(71)
iris.rf <- randomForest(Species ~ ., data=iris, importance=TRUE,
                        proximity=TRUE)

print(iris.rf)


load('results/0.75/ebs.Rdata')
load('results/0.75/ebs_ram_trawl.Rdata')

ram_ebs_trawl_summary <- ram_ebs_trawl %>%
  group_by(year, 
           , round_lon) %>%
  summarise(total_cpue = sum(wt, na.rm = T)) %>%
  mutate(log_total_cpue = log(total_cpue))


joint_ebs <- ebs %>%
  left_join(ram_ebs_trawl_summary, by = c('year', 'round_lat', 'round_lon')) %>%
  filter(is.na(total_cpue) == F, is.na(fishing_hours) == F)


holycow <- randomForest(total_cpue ~ ., data = joint_ebs %>% select(-log_total_cpue, -fishing_hours), importance = T)

hist(holycow$rsq)

randomForest::varImpPlot(holycow)


results <- joint_ebs %>% 
  select(-log_total_cpue, -fishing_hours) %>% 
  pipelearner() %>% 
  learn_models(
    c(lm, rpart::rpart, randomForest::randomForest),
    total_cpue ~ .) %>% 
  learn()

r_square <- function(model, data) {
  actual    <- eval(formula(model)[[2]], as.data.frame(data))
  residuals <- predict(model, data) - actual
  1 - (var(residuals, na.rm = TRUE) / var(actual, na.rm = TRUE))
}
add_rsquare <- function(result_tbl) {
  result_tbl %>% 
    mutate(rsquare_train = map2_dbl(fit, train, r_square),
           rsquare_test  = map2_dbl(fit, test,  r_square))
}

results %>%
  add_rsquare() %>%
  select(model, contains("rsquare")) %>%
  gather(source, rsquare, contains("rsquare")) %>%
  mutate(source = gsub("rsquare_", "", source)) %>% 
  ggplot(aes(model, rsquare, fill = source)) +
   geom_col(position = "dodge", size = .5) +
   labs(x = NULL, y = "R Squared") +
   coord_flip()
```


## MLE et al

This isn't a complex problem, `nlminb` to the rescue. Once you start to add more parameters, TMB might be interesting, as though might be `pipelearner` if you want to explore fitting a model whose goal is out of sample prediction



# Problems