---
title: "Skynet Lab Notebook"
date: "`r Sys.Date()`"
output: html_document
---

```{r}
knitr::opts_chunk$set(eval = F)
```


  * Talk with Dawn about their pollock work 

  * Measuring productivity as a potential latent variable 

  * Illegal guys turn it off selection problem 

  * Check out [this work on mapping effort](https://bluehub.jrc.ec.europa.eu/webgis_fish/)


# Practice Scraping GFW

Goal here is to work up a script querying GFW to get eastern bering sea data. 

The eastern bering sea (EBS) is located roughly within the bounding box of 175w to 155w, and 50n and 58 n. These correspond to 


```{r}
library(tidyverse)
library(bigrquery)
library(lubridate)
library(tmap)
library(leaflet)
library(rgdal)
library(FishData)
library(ggmap)

data("World")

project <- "ucsb-gfw"

library(bigrquery)

fishing_connection <- src_bigquery(project, "eastern_bering_sea") # This function initiliazes a connection with a BQ dataset

summer <- 6:8

ebs <- fishing_connection %>% 
  tbl("ebs") %>% 
  mutate(y_m_d = date(timestamp)) %>% 
  mutate(obs_year = year(y_m_d),
         obs_month = month(y_m_d),
         round_lat = round(lat, as.integer(1)),
         round_lon = round(lon, as.integer(1))) %>% 
  filter(obs_month %in% summer) %>% 
  group_by(round_lat,round_lon,obs_year) %>% 
  summarise(fishing_hours = sum(hours * measure_new_score),
            mean_dist_from_shore = mean(distance_from_shore),
            mean_dist_from_port = mean(distance_from_port)) %>% 
  collect() %>% 
  rename(year = obs_year)




```

Let's make a mep

```{r}

m <-  ebs %>% 
  leaflet() %>% 
  addTiles %>% 
  fitBounds(-175,65,-155,50) %>% 
  addCircleMarkers(~round_lon, ~round_lat)

coords <- SpatialPoints(ebs %>% dplyr::select(round_lon, round_lat) %>% ungroup())  

ebs_map <- SpatialPointsDataFrame(coords, ebs %>% ungroup())

proj4string(ebs_map) <- CRS("+proj=longlat +datum=WGS84")

# proj4string(World) <- CRS("+proj=longlat +datum=WGS84")

# ebs_border <- bbox(ebs_map)

# ebs_hdf <- get_map(location = c(lon = -170, lat = 55),zoom = 5)

alaska <- read_shape('~/Box Sync/Databases/alaska_coastline/alaska_coastline.shp')
# 
# ebs <- meow[meow$ECOREGION == 'Eastern Bering Sea',]

alaska <-  crop_shape(alaska, ebs_map)


ebs_map <- spTransform(ebs_map,crs(alaska))

# plot(alaska)
# plot(ebs_map, add = T)

gfw_ebs_map <- tm_shape(alaska)  +
  tm_lines(col = 'black') + 
  tm_shape(ebs_map) + 
  tm_bubbles(size = 'fishing_hours', col = 'fishing_hours',
             palette = c('blue','red'),
             legend.size.show = F) + 
    tm_legend(legend.outside.position = c("right"),
  legend.outside = TRUE) #format legend

save_tmap(gfw_ebs_map, filename = paste('../',run_dir,'/gfw_ebs_map.pdf', sep = ''))

# ebs_gfw_raster <- ebs %>% 
#   ungroup() %>% 
#   dplyr::select(round_lat, round_lon, fishing_hours) %>% 
#   spread(round_lon, fishing_hours) %>% 
#   dplyr::select(-round_lat) %>% 
#   as.matrix() %>% 
#   raster()
# extent(ebs_gfw_raster) <- extent(ebs_map)
# proj4string(ebs_gfw_raster)

# gfw_ebs_map_plot <- ggmap(ebs_hdf) + 
#   geom_point(data = ebs,size = .1, aes(round_lon, round_lat, color = log(fishing_hours))) + 
#   scale_color_continuous(low = 'blue',high = 'red') +
#     labs(title = 'GFW Fishing Hours')


gfw_ebs_map_plot


```



Now, let's try and get some fishviz data. 

```{r}

ebs_trawl <-  FishData::download_catch_rates( survey="EBSBTS", species_set=20 ) %>% 
  as_data_frame()

ebs_trawl_summary <- ebs_trawl %>% 
  set_names(tolower(colnames(.))) %>% 
  mutate(round_lat = round(lat,1),
         round_lon = round(long,1)) %>% 
  group_by(year,round_lat, round_lon) %>% 
  summarise(total_weight = sum(wt, na.rm = T)) %>% 
  mutate(log_total_weight = log(total_weight))

coords <- SpatialPoints(ebs_trawl_summary %>% ungroup() %>%  dplyr::select(round_lon, round_lat))  

fviz_ebs <- SpatialPointsDataFrame(coords, ebs_trawl_summary %>% ungroup())

proj4string(fviz_ebs) <- CRS("+proj=longlat +datum=WGS84")


# ebs_fviz_raster <- ebs_trawl_summary %>%
#   ungroup() %>%
#   dplyr::select(round_lat, round_lon, log_total_weight) %>%
#   spread(round_lon, log_total_weight) %>%
#   dplyr::select(-round_lat) %>%
#   as.matrix() %>%
#   raster()
# extent(ebs_gfw_raster) <- extent(ebs_map)
# proj4string(ebs_gfw_raster)
```

```{r}


fviz_ebs_map <- tm_shape(alaska)  +
  tm_lines(col = 'black') + 
  tm_shape(fviz_ebs) + 
  tm_bubbles(size = .05, alpha = 0.5, col = 'log_total_weight',
             palette = c('blue','red'),
             legend.size.show = F) + 
    tm_legend(legend.outside.position = c("right"),
  legend.outside = TRUE) #format legend

fviz_ebs_map

```


Hot damn! Let's merge these. Just do it over summer if possible

```{r}
joint_ebs <- ebs %>% 
  left_join(ebs_trawl_summary, by = c('year','round_lat','round_lon')) %>% 
  filter(is.na(total_weight) == F) 

joint_ebs %>% 
  ggplot(aes(log(fishing_hours), log(total_weight), fill = log(mean_dist_from_port))) + 
  geom_point(shape = 21) + 
  geom_smooth(method = 'lm') + 
  xlab('Log GFW Fishing Hours') + 
  ylab('Log Trawl Survey Biomass') + 
scale_fill_continuous(name = 'Distance from Port', low = 'green', high = 'blue') + 
  ggthemes::theme_fivethirtyeight() + 
  theme(axis.title = element_text())


```

```{r}

reg <- lm(total_weight ~ fishing_hours + mean_dist_from_port, data = joint_ebs)

```


# Getting Vessel Characteristics

Go to vessel_lists > YEAR_fishing_vessel_info

nn_max_label gives you what the neural net is 

max_score is how confident it is



# Interpolating EBS trawl survey data

The raw EBS trawl survey data from fishviz is at a relatively course (or true if you want to tihnk about it that way) scale. The problem is that while that's a pretty solid resolution, it's still way coarser than the global fishing watch data themselves. That leaves you with two options more or less. What you were doing was just "rounding" the lats and longs into conformity. That was just erasing a lot of data though, and is obviously just a convenient hack. Another option then would be to just rasterize and overlay both options. Basically, assign an interpolation function to the data. Seems like you really don't want to do that for the GFW data, since the data are in theory closer to a census than a survey. 

So, that then means you need to think about manipulating the trawl survey data. One option would just be to apply a simple interpolation to the data, and then interpolate the density at each point in the GFW data. That would be the simplest option, but also probably the most incorrect. Could mess around with it for a sec though as a starting point. 

From there, the better thing to do would be to mess around with using VAST to interpolate, to basically create the maps in the shiny app. I'm guessing you'd use VAST to do that, but will have to check with Jim. Let's try rasterizing first and see how it looks. 


```{r}
ebs_trawl <-  FishData::download_catch_rates( survey="EBSBTS", species_set=20 ) %>% 
  as_data_frame() %>% 
  set_names(colnames(.) %>% tolower())



pollock_2016 <- ebs_trawl %>% 
  filter(sci == 'Gadus_chalcogrammus') %>% 
  dplyr::select(towid,year,lat,long,wt) %>% 
  filter(year == max(year))

coords <- SpatialPoints(pollock_2016 %>% ungroup() %>%  dplyr::select(long, lat))  

fviz_ebs <- SpatialPointsDataFrame(coords, pollock_2016 %>% ungroup())

proj4string(fviz_ebs) <- CRS("+proj=longlat +datum=WGS84")

pollock_raster <- raster(fviz_ebs, resolution = 1/5)

pollock_raster <- raster(fviz_ebs)

raster_pollock_2016 <- rasterize(fviz_ebs,pollock_raster,'wt')

raster_pollock_2016@data@values


plot(fviz_ebs)

plot(raster_pollock_2016)

interp_pollock <- idw(formula = wt ~ 1, fviz_ebs, newdata = pollock_raster) 

```

# Random Forest Fitting

As a starting point, let's try and focus on on EBS pollock, iconic species, trawl fishery, large boats, should be somewhat doable. So the steps to make that happen. 

1. Use VAST to get density estimates at knots (start at 100 knots)

2. Get EBS GFW data for trawl vessels

3. Use `RANN:nn2` to snap each EBS GFW data point to a knot

```{r}

x1 <- runif(100, 0, 2*pi)
x2 <- runif(100, 0,3)
DATA <- data.frame(x1, x2)
nearest <- nn2(DATA,DATA, k  = 3)

```

4. Summarize the total effort at each knot

5. Fit model!

Here's the concept 
```{r}

library(tidyverse)
library(VAST)
library(ThorsonUtilities)
library(demons)
library(sf)
library(ggmap)
library(FishData)
library(viridis)
library(compiler)
library(TMB)
library(bigrquery)
library(stringr)

ebs_trawl <-
  FishData::download_catch_rates(survey = "EBSBTS", species_set = 50) %>%
  set_names(tolower(colnames(.)))
Version = "VAST_v2_4_0"
Method = c("Grid", "Mesh")[2]
grid_size_km = 25
n_x = c(100, 250, 500, 1000, 2000)[1] # Number of stations
Kmeans_Config = list( "randomseed"=1, "nstart"=100, "iter.max"=1e3 )  


FieldConfig = c("Omega1"=1, "Epsilon1"=1, "Omega2"=1, "Epsilon2"=1) 
RhoConfig = c("Beta1"=0, "Beta2"=0, "Epsilon1"=0, "Epsilon2"=0) 
OverdispersionConfig = c("Vessel"=0, "VesselYear"=0)
ObsModel = c(2,0)  

Options =  c("SD_site_density"=0, "SD_site_logdensity"=0, "Calculate_Range"=1, "Calculate_evenness"=0, "Calculate_effective_area"=1, "Calculate_Cov_SE"=0, 'Calculate_Synchrony'=0, 'Calculate_Coherence'=0)

Region <- 'Eastern_Bering_Sea'


DateFile = file.path(getwd(), 'VAST_output/')
dir.create(DateFile)




alaska_pollock <- ebs_trawl %>%
filter(sci == sci[1]) %>%
rename(
Year = year,
Lat = lat,
Lon = long,
Catch_KG = wt
) %>%
mutate(AreaSwept_km2 = 1, Vessel = 'missing') %>%
select(Year, Lat, Lon, Vessel, AreaSwept_km2, Catch_KG)

qmplot(
x = Lon,
y = Lat,
data = alaska_pollock,
color = log10(Catch_KG)
) +
scale_color_viridis() +
facet_wrap( ~ Year)

Record = ThorsonUtilities::bundlelist(
c(
"alaska_pollock",
"Version",
"Method",
"grid_size_km",
"n_x",
"FieldConfig",
"RhoConfig",
"OverdispersionConfig",
"ObsModel",
"Kmeans_Config"
)
)
save(Record, file = file.path(DateFile, "Record.RData"))
capture.output(Record, file = paste0(DateFile, "Record.txt"))
strata.limits <- data.frame('STRATA' = "All_areas")

Extrapolation_List = SpatialDeltaGLMM::Prepare_Extrapolation_Data_Fn(Region =
Region, strata.limits = strata.limits)

alaska_pollock <- alaska_pollock %>%
na.omit() %>%
mutate(Vessel = as.factor(Vessel))

Spatial_List = SpatialDeltaGLMM::Spatial_Information_Fn(
grid_size_km = grid_size_km,
n_x = n_x,
Method = Method,
Lon = alaska_pollock[, 'Lon'],
Lat = alaska_pollock[, 'Lat'],
Extrapolation_List = Extrapolation_List,
randomseed = Kmeans_Config[["randomseed"]],
nstart = Kmeans_Config[["nstart"]],
iter.max = Kmeans_Config[["iter.max"]],
DirPath = DateFile,
Save_Results = FALSE
)
# Add knots to Data_Geostat
alaska_pollock = cbind(alaska_pollock, "knot_i" = Spatial_List$knot_i)

TmbData = Data_Fn(
"Version" = Version,
"FieldConfig" = FieldConfig,
"OverdispersionConfig" = OverdispersionConfig,
"RhoConfig" = RhoConfig,
"ObsModel" = ObsModel,
"c_i" = rep(0, nrow(alaska_pollock)),
"b_i" = alaska_pollock[, 'Catch_KG'],
"a_i" = alaska_pollock[, 'AreaSwept_km2'],
"v_i" = as.numeric(alaska_pollock[, 'Vessel']) - 1,
"s_i" = alaska_pollock[, 'knot_i'] - 1,
"t_iz" = alaska_pollock[, 'Year'],
"a_xl" = Spatial_List$a_xl,
"MeshList" = Spatial_List$MeshList,
"GridList" = Spatial_List$GridList,
"Method" = Spatial_List$Method,
"Options" = Options
)

TmbList = Build_TMB_Fn(
"TmbData" = TmbData,
"RunDir" = DateFile,
"Version" = Version,
"RhoConfig" = RhoConfig,
"loc_x" = Spatial_List$loc_x,
"Method" = Method
)
Obj = TmbList[["Obj"]]

Opt = TMBhelper::Optimize(
obj = Obj,
lower = TmbList[["Lower"]],
upper = TmbList[["Upper"]],
getsd = TRUE,
savedir = DateFile,
bias.correct = FALSE
)

Report = Obj$report()
Save = list(
"Opt" = Opt,
"Report" = Report,
"ParHat" = Obj$env$parList(Opt$par),
"TmbData" = TmbData
)
save(Save, file = paste0(DateFile, "Save.RData"))

MapDetails_List = SpatialDeltaGLMM::MapDetails_Fn( "Region"=Region, "NN_Extrap"=Spatial_List$PolygonList$NN_Extrap, "Extrapolation_List"=Extrapolation_List )
# Decide which years to plot                                                   
Year_Set = seq(min(alaska_pollock[,'Year']),max(alaska_pollock[,'Year']))
Years2Include = which( Year_Set %in% sort(unique(alaska_pollock[,'Year'])))

Dens_xt <- SpatialDeltaGLMM::PlotResultsOnMap_Fn(plot_set=c(3), MappingDetails=MapDetails_List[["MappingDetails"]], Report=Report, Sdreport=Opt$SD, PlotDF=MapDetails_List[["PlotDF"]], MapSizeRatio=MapDetails_List[["MapSizeRatio"]], Xlim=MapDetails_List[["Xlim"]], Ylim=MapDetails_List[["Ylim"]], FileName=,,, Year_Set=Year_Set, Years2Include=Years2Include, Rotate=MapDetails_List[["Rotate"]], Cex=MapDetails_List[["Cex"]], Legend=MapDetails_List[["Legend"]], zone=MapDetails_List[["Zone"]], mar=c(0,0,2,0), oma=c(3.5,3.5,0,0), cex=1.8, plot_legend_fig=FALSE)

alaska_pollock_densities = cbind( "density"=as.vector(Dens_xt), "year"=Year_Set[col(Dens_xt)], "e_km"=Spatial_List$MeshList$loc_x[row(Dens_xt),'E_km'], "n_km"=Spatial_List$MeshList$loc_x[row(Dens_xt),'N_km'] ) %>% 
  as_data_frame() %>% 
  mutate(knot = as.numeric(factor(paste(e_km,n_km)))) %>% 
  arrange(knot,year)

project <- "ucsb-gfw"

fishing_connection <-
  src_bigquery(project, "skynet") # This function initiliazes a connection with a BQ dataset


ebs_raw <- fishing_connection %>%
  tbl("ebs_w_vessels") %>%
  collect(n = Inf)

ebs <- ebs_raw %>% 
  select(-b_mmsi, -b_year) %>% 
  set_names(str_replace_all(colnames(.), '(a_)|(b_)',''))

utm_coords <- SpatialDeltaGLMM::Convert_LL_to_UTM_Fn( Lon=ebs$rounded_lon, Lat=ebs$rounded_lat, zone=Extrapolation_List$zone, flip_around_dateline=Extrapolation_List$flip_around_dateline) %>% 
  rename(e_km = X, n_km = Y) %>% 
  select(e_km, n_km)

ebs <- ebs %>% 
  bind_cols(utm_coords)

ebs %>% 
  ggplot(aes(best_label)) +
  geom_bar()

knots <- alaska_pollock_densities %>% 
  select(knot,e_km,n_km) %>% 
  unique()

nearest_knot <- RANN::nn2(knots %>% select(-knot), utm_coords, k = 1)

ebs$knot <- knots$knot[nearest_knot$nn.idx]

ebs %>% 
  select(rounded_lat, rounded_lon, knot) %>% 
  unique() %>% 
  ggplot(aes(rounded_lat, rounded_lon, color = knot %>% factor())) + 
  geom_point() 

trawl_fishing_by_knot <- ebs %>% 
  filter(best_label == 'trawlers') %>%
  rename(th = total_hours) %>% 
  group_by(year,knot) %>% 
  summarise(total_hours = sum(th, na.rm = T),
            total_engine_hours = sum(th * inferred_engine_power, na.rm = T),
            num_vessels = length(unique(mmsi)),
            dist_from_shore = mean(mean_distance_from_shore, na.rm = T),
            dist_from_port = mean(mean_distance_from_port, na.rm = T),
            mean_vessel_length = mean(inferred_length, na.rm = T))

skynet_data <- trawl_fishing_by_knot %>% 
  left_join(alaska_pollock_densities, by = c('year','knot')) %>% 
  ungroup() %>% 
  mutate(vessel_hours = total_engine_hours * num_vessels)

# skynet_data <- skynet_data %>%
#   group_by(knot) %>% 
#   arrange(knot,year) %>% 
#   mutate(total_hours_lag_1 = lag(total_hours,1))
#   
# skynet_data <- skynet_data %>% 
#   remove_missing() %>% 
#   ungroup()

skynet_model <- randomForest::randomForest(density ~ ., data = skynet_data %>% select(-e_km, -n_km, -knot,-year,-total_hours))

randomForest::varImpPlot(skynet_model)

skynet_data <- skynet_data %>% 
  ungroup() %>% 
  mutate(density_hat = predict(skynet_model))

lm(density ~ total_hours, data = skynet_data) %>% summary() -> a

badshit <- skynet_data %>% 
  ggplot(aes(density, total_hours)) + 
  geom_point() + 
    geom_smooth(method = 'lm')+
  geom_abline(aes(slope = 1, intercept = 0)) + 
  labs(x = 'EBS Groundfish Survey Density', y = 'GFW Fishing Hours', 
       title = 'Alaska Pollock',
       caption = paste('R2 =',a$r.squared %>% round(2))) + 
  hrbrthemes::theme_ipsum() +
  scale_x_continuous(limits = c(0,NA)) + 
  scale_y_continuous(limits = c(0, NA))

HOLYSHIT <- skynet_data %>% 
  ggplot(aes(density, density_hat)) + 
  geom_point() + 
  geom_abline(aes(slope = 1, intercept = 0)) + 
  labs(x = 'EBS Groundfish Survey Density', y = 'GFW Model of Densities', 
       title = 'Alaska Pollock',
       caption = paste('R2 =',mean(skynet_model$rsq) %>% round(2))) + 
  hrbrthemes::theme_ipsum() +
  scale_x_continuous(limits = c(0,NA)) + 
  scale_y_continuous(limits = c(0, NA))
  


```

```{r fig.cap="HOLY SHIT IT'S ALIVE"}

HOLYSHIT

```

# 2017-06-05

Well holy shit, things seem to be working. Specifically, a random forest model seems capable of reasonably predicting densities in space and time. So, what's the next step here to make sense of all this. 

I think before you go to Chris you need to give the structural approach a bit more thought. So, I'd like to get a "structural" model built this week, with an obvious candidate being an ideal-free distribution model, wherein you assume that marginal profits are equal across scace for a given time unit, and then "solve" or estimate abundance to make this true, following methods from Miller et al. There are a couple odd constants here that you'd need to explore. 

Let's maybe first focus on getting the structural approach working. Once that's working, let's integrate those into a cohesive script and start thinking through the right set of diagnostics. 

At it's core... 

R^2^

Maps

Out of sample prediction in space, time, species

Bias

# Structural Fitting

The idea here goes back to the original premise: if marginal profits are indeed equal in space and time, 

This is a summary of the proposed model structure for version 1.0 of the "skynet" model. The objective of this model is to estimate spatio-temporal indicies of abundance using GFW effort data. The general method is an adaptation of @Miller2016, wherein the authors attempt to estimate quota lease prices by fitting observed patterns of effort to predicted patterns of effort based on the expected profitability of fishing different patch (subject to the constraint that marginal profits are equal in each patch). Your goal is to flip this around a bit, and now predicted relative abundance as a function of the effort, price, and cost for fishing that patch, again conditional on the same assumption as above. 


So, with that in mind, the fraction of trawl vulerable biomass left after fishing is just the survival of the stock times the fraction of the stock that is vulnerable to trawl survey.... That seems wrong. Why wouldn't it go into the survival itself?

Expected CPUE is based on the self reported CPUE in prior time period. In your case, you're going to try and estimate that value itself. 

They assume observation error here, in that there are other things influencing expected CPUE besides last years, that we cannot directly observe, so they apply log normal error term to the expected CPUE. What does that say about your formulation? 

The tobit process says that effort will be 0 if expected marginal profits if T were 0 are greater than the overall marginal profits across the system. 

So, overall, the model works by estimating the quota prices *c* that rationalize the equation. Can you think about it a bit more generally for your use. You don't need to think about the whole "fraction vulnerable to trawl thing". 


Note that this is "expected profits from a marginal unit of effort", meaning marginal profits, $d\pi/dE$


The key sentance that you're missing here is the "in which T units of effort have already been applied". So, this is basically accounting for the draw-down of the stock, which your standard method doesn't account for (just taking the usual marginal profits per unit E). So this is say, conditional on some already observed amount of effort, the expected profits per unit of one more unit of effort is price * expected 





So, what this is saying, with that lovely exponent in there, let's start with revenues. 

This is saying that the revenue of the first unit of effort is just $p*CPUE*E^{q1}$. And so, this decays 

```{r}

p <-  10
cpue <-  100
q = .1
effort = 0:100

marginal_revenue <- p * cpue * exp(-q * effort)

plot(effort, marginal_revenue)

```

So, those are "marginal" revenues, calculated in an interesting way which is a little different thatn the way that you would usually think about it, mostly since your usual approach assumes that catch is $pqEB$, and so the marginal revenue with respect to effort is constant $pqB$, meaning that conditional on B, you get the same revenues from the first to the last additional unit of effort, which seems wrong. While, with this version, you're acknolweding that marginal revenues go down as you kill the damn stock. SO, they should do about the same thing, except in your case, you usually assume that marginal revenues are constant, and increase cost per effort to give the thing some concavity. Where they assume that the cost of every unit of effort is the same, but the expected revenues go down as the stock is drawn down. 

So, they just say here are the revenues per unit effort for the Tth unit of effort, minus the variable cost. So, now just try and understand the variable cost. Becuase this is marginal, that is why it should be the cost per unit effort, since it's saying that marginal profits are the expected revenues under one more unit of effort minus the cost of the unit of effort. BOOYAH, I finally get why this thing works that way. 

So, what is the model then. 

If you did your math right 

$$B_{i,t} = \frac{e^{E_{i,t}q_{t}}(c_{i,t} + \pi)}{p_t}$$

So, what does this leave you to estimate? And let's start simple. 

*E* is known

*q* is certainly not known

*c* is partially known, you have some information vessel size, distance from shore, gas prices, etc. Look more carefully at how they define c, your standin for VC here. 

*$\pi$* You start off assuming zero

*p* this should really just be a scalar, take as data. 

So, regardless of how you parameterize it, it's down to q and c as the only options for model tuning. 

So what do you do with this? 

q is in units of.... something 1/kwH?. You can't estimate a q for each knot, so how would you go about estimating it? You could do one q by year, which doesn't really help a lot, or you could try and make q a function of vessel size, something like that. Doesn't seem like you have a whole lot of data to create a rational model of this besides treating it as a nuisance parameter that basically scales the relationship between effort and the magnitude of density. 

Given how troublesome q is normally, seems much more logical to think about tuning costs. In one world, you think of costs as being directly observable, and so the cost per kilowat hour at a given site is known perfectly. To think about it in simple terms, you could certainly get data on labor costs per kilowat hour and fuel costs per kwh for a given site, as a function of vessel size and distance from port. So, in that world cost c is just

$ c = labor*distance + fuel * distance$ or then

$c = distance*(labor + fuel)$

In that case, since c needs to be in \$ where kwh is one, so labor and fuel would have in units of \$/kwh, so labor and fuel costs would have to be in units of \$/kwh, where \$/kwh internalizes the travel costs, so it's something like (miles / MPG)*$(gallon).... but then that cost would in theory be spread over fishing hours, so gets trickier. 

Seems a little easier to try and estimate it. 

So, in that world, you say that cost is something like $\beta_{1}*(distance * vessel_size) + \beta_{2}*distance + \beta_{3}*vessel_size$

Now, how would you think about doing this. The "vessel size" thing is a little tough, since that's not a characteristic of the site, but of the vessels chosing to fish at that site. 

The core problem then is that the you think that different vessel sizes should have different cost per hour to fish at a given site. So, you could do something like break the vessels into fleets by vessel size. Assume one expected CPUE (abundance) common abundance at a site. Then, tune q and c for each fleet, such that they all produce the same expected CPUE. 

Or, make it a bit simpler. Take the observed CPUE, and for each fleet, estimate a q and a cost per unit distance (where the $\beta$ on distance would be in units of $/distance*hour...)?

Think about that. You need cost to be in units of \$

You're going to get cost as $\beta*distance$, where distance is units of km. So, $\beta$ would be a little tough to interpret, since it would have to be in units of $/km for a particular fleet. 


So, the marginal profits would be

$\pi_{i,j,t} = p*cpue_{j,t}*e^{-q_{j}T_{i,j,t}} - \beta_{j,t}*distance_{i,t}$

That model would be assuming that each fleet is targeting mutually exclusive parts of the stock... don't worry about that for now. So, in the single fleet world, the model is


$[q_{j}, \beta_{j,t} | cpue_{j}] \propto [cpue_{j} | q_{j},\beta_{j}][q_{j}][\beta_{j,t}]$

Where you can assign prior to beta_j based on fuel prices and labor costs. 

## What the hell units are things in in here?

price: $/kg
cpue: kg/kwh
q: 1/kwh
T: kwh

So if that's the case, then revenues have to be in \$/kwh, therefore cost has to be in units of \$/kwh as well. So, if you are scaling cost by distance from port for each patch, then \beta has to be in units of $/km*kwh, which is weird. 

Let's look at a bit more carefully at how those cost parameters are set in @Miller2016. 

" We compute variable costs (VC $/hr) using information on input costs (fuel, labor, ice, and distance from port)"

Because the unit of ob- servation is a patch and period rather than a tow, a single cost per unit effort must be assigned to each patch for each period. This required designating a single distance from port for each patch. This was deemed to be the distance from the center of the patch to the nearest port for which trips originating from that port entailed at least 100 tows in aggregate. That same port was used to match patches to fuel prices from the fuel cost survey. If fuel costs were not available for that port, prices from the nearest port included in the survey were used. Dividing the resulting cost per trip by the average number of tow hours per trip for all trips to that patch yielded our mea- sure of cost per tow hour. In a subsequent model, we allow the opportunity cost of time to vary with management (see Abbott and Wilen 2011). This is accomplished by adding an interaction between distance and management to the variable cost term"

So to match Miller more precisely, you would calculate variable cost directly. I'm arguing that this doesn't give you enough flexibility, though would be worth exploring this more if you either want to isolate specific parameters for optimization (e.g. just model "unobserved costs", taking the fuel/labor costs as data). 

So what would that look like? If costs need to be in units of $/KWH, then total cost could be....

1. Take all the kwh for a given knot, fleet, year

2. Calculate the fuel costs to operate those kwh

3. Calculate the labor costs to operate those kwh

4. Calculate the fuel and labor costs to get to a site 

5. Calculate the number of trips to a site... Can you get that? 

6. Use all that to get an average cost per kwh, as the fuel costs + labor costs while operating in the patch, + the fuel and labor costs of getting there, divided by the average kwh exereted at that knot. 

You could use this estimate as either a base, so that the actual cost per kwh is some multiplier times that thing. So, the idea would be that the relative magnitude of the costs would be right across knots, but you're consistently missing (or adding) something, and that multiplier would make up for that. 

An alternative to that would be to think of this estimate as a strongly informative prior on the cost parameter at each knot, and then estimate it. 

A last option would be to treat it as a latent variable, where you estimate costs in each knot and time step as coming from a distribution with mean derived from the empirical approach above, and some sigma. 

As one other approach, you could turn it into a reduced form equation, basically turning it into a linear regression


For now, let's go with simply setting priors on the thing based off of some rough sense of the magnitude of costs as priors. 

Ah god damn it though. You can't rely on a framework for just estimating costs, you need a model of costs. So what if you made it simple. 

Say that cost per unit hour is 

$\beta_{1}*distance + \beta_{2}*meanvesselsize +  \beta_{3}*distance*meanvesselsize$

Where the prior on $\beta_{1}$ is just \$/km, which you would get from fuel efficiency of vessel and price per gallon

the prior on $\beta_{2}$ would be crew wages per hour * number of crew

Unformative prior on $\beta_{3}$

So, the final model will be....

$ cpue_{k,t} = \frac{1}{p}e^{qE_{k,t}}(\beta_{1}*distance_{k} + \beta_{2}*meanvesselsize_{k,t} +  \beta_{3}*distance_{k}*meanvesselsize_{k,t} + \pi) + e_{i,j}$

prior on q: gamma

prior on $\beta$ normal with mean from data and sigma set to something 

prior on $\sigma$ inverse gamma. 




## Model and Data

Let's go back to @Miller2016 and make sure that you understand what's happening here. 

The crux: expected profits are price - quota cost * expected harvest, minus variable costs

Fishers are not assumed to consider the dynamic consequences of spatial fishing choices, but rather react to known prices and costs. 

X^o^ is the biological stock at the start of the time period 

```{r}

effort <-  100
pi <- 0
q <-  .1
b <- 10



```



**Check this after work**
$$Effort_{f,t,s} = \frac{1}{q_{f,t,s}}log(\frac{p_{f,t,s}B_{f,t,s}}{c_{f,t,s} + \bar{\pi}_{f,t}})$$

$$B = \frac{(c + \pi)}{pe^{-qEffort}}$$

check that math

```{r}

q <- .1
p <-  10
b <- 100
pi <- 103
c <- 1

effort <- 1/q * log( (p * b) / (pi + c))

bhat <- (c + pi) / (p * exp(-q *effort))

bhat2 <- ((c + pi) * exp(q *effort)) / (p)

```


Note that *c* here are variable costs per unit  effort ($/hour). You're omitting the quota price for now (though that might need to be incorporated in here at some point)

$$[p,c,q,\bar{\pi}| B] \propto [B |p,c,q,\bar{\pi},effort][priors]$$

In english, the likelihood of price, cost, q, and marginal profits conditional on Biomass is proportional to the likelihood of the biomass conditional on a given price, cost, q, and marginal profits and data effort and priors. 

So, how to fit this fun thing? 

notation. Let's call species *s*, location *l*, and time *t*, and fleet *f*

## price *p*

You probably don't need to actuall estimate *p*. You can match the species in the EBS trawl survey data to Tyler's price database as a starting point. So, in that world, you have $p_{s}$. You could also, if you can link gear types from GFW to type of product, in which case you would have $p_{s,f}$

## cost *c*

This is in units of $/hour (where hour is presumably fishing hour, not travel time), so that travel time is incororated into the cost of actual fishing. From @Miller2016, they compute VC, which you call *c*, as fuel, labor, ice, distance from port. As a starting point, you could focus on distance from port and vessel size. Something like 

$$c_{l,f} = f(distance,fuel,labor, misc)$$

You can get distance from GFW data. Fuel prices you could either estimate, take global averages, or better yet, scrape from [here](bunkerindex.com). Labor costs are a little tricker. You could just make it proportional to vessel size and estimate something from there. Or, you could use [BLS data](https://www.bls.gov/oes/current/oes453011.htm) for the US as a starting point, seems like a good idea. Misc costs I think you have to ignore for now. 

So, the way that they do it in @Miller2016 is to calculate the cost per trip, where that is presumably $fuel*distance + labor * time$, where *time* is a function of distance, that you might be able to get from GFW. You would then divide that total cost by the average number of fishing hours exerted at that patch, to get to a cost per unit fishing effort at that patch. That seems a little goofy to me though, since it says that say you have a reallllly far away patch that people fish at a lot though, that's going to say that the cost per unit effort is really low out there, where in fact it is really damn high. I could actually see the opposite, where per trip you fish a lot on a very far away place, since you're not going to go back and forth from port a bunch, but rather stay out there and fish. But, let's go with it for now, since it's what they do in that paper. Best thing to do is start with that. So, for each patch/fleet then, you'll calculate the average number of fishing hours per "trip" out there, which you'll need to poke at the data to really understand. So, to match @Miller2016 exactly. For each patch in the database, you would calcualte it's distance from port. For each fleet type, you would get something like miles per gallon. You would then get cost in fuel. For each fleet, you would also get average speed, and use that to get the time taken to get to each place, and multiply that by cost per hour times crew size. Add that to the fuel costs, and you have the total costs to fish at a site. Divide that by the average number of fishing hours at the site, and you've got yourself cost per unit fishing effort at said site. 

There's a few problems with that. I'm still not comfortable with the divide by average fishing hours per trip at site, since I feel like it's likely to really skew the cost for far away sites that people hang at for a long time. There's also the issue that you can string sites together, so the marginal cost of fishing around the priblofs is spread out across the sites out there. But let's maybe go with it for now as a starting point. 

As an alternative, you could estimate these. So, say cost is something like 

$$c = aDistance + bLength + cDistanceLength + constant$$

and you could fit those parameters as part of the model, possible with some informative priors


As one other alternative, you could collapse the distance and the labor into one thing....

Take distance, convert it to hours using average distance per hour for that vessel size. Take the hours *the number of workers for that vessel size * hourly wage. That's the estiamte of labor costs for that trip. Then, take distance, convert to gallons of fuel per the average distance/gallon for the average vessel size at that site. That's your estimate of "travel" costs or something like that, and then just have a coefficient times that. 

You can also take that same calculation and break apart in the same interaction mode you looked at above, so it would be fuel costs + labor costs + interaction or plus unknown... you can mess with different options here. 

For now, let's KISS and start with seeing if you can get the model to fit without additional data on fuel or labor costs, and then launch into that

## Fitting the structural model

Let's get to work!

Let's start with MLE with the no-frills methods. 

Then move to STAN no frills, make sure it works

Then start adding complexity and data to cost model 

```{r}


fit_sffs <- function(beta_distance,
                     beta_size,
                     beta_interaction,
                     q,
                     sigma,
                     dat,
                     price,
                     marginal_profits = 0,
                     use = 'mle') {
  
  old_sigma <- sigma
  sigma <-  exp(sigma)
  
  old_q <- q
  
  q <- exp(q)
  
  beta_distance <-  exp(beta_distance)
  
  beta_size <- exp(beta_size)
  
  beta_interaction <- exp(beta_interaction)
  
  # beta_intercept <- exp(beta_intercept)
  
  cost <-
  beta_distance * dat$dist_from_port + beta_size * dat$mean_vessel_length + beta_interaction * dat$dist_from_port * dat$mean_vessel_length
  
  estimated_abundance <-
  ((cost + marginal_profits)) / (price  * exp(-q * dat$total_engine_hours))
  if (use == 1){
  out <-
  -sum(stats::dlnorm(dat$density_lag1, log(estimated_abundance), sigma, log = T))
  } else {
    out <- estimated_abundance
  }
  return(out)
                     
}

skynet_data_2 <- skynet_data %>% 
  group_by(knot) %>% 
  arrange(knot,year) %>% 
  mutate(density_lag1 = lag(density, 1)) %>% 
  ungroup() %>% 
  remove_missing() %>% 
  filter(density_lag1 > 0)

 
mle_model <- stats4::mle(
  fit_sffs,
  start = list(
  beta_distance = log(100),
  beta_size = 100 %>% log,
  beta_interaction = 100 %>% log,
  sigma = log(sd(skynet_data$density)/100),
  q = log(1e-7)),
  fixed = list(price = 765, 
               dat = skynet_data_2 %>% filter(total_engine_hours > 0, density_lag1 > 0),
               marginal_profits = 0,
               use = 1),
  upper = list(sigma = log(100),
               q = log(1.5e-5)))

summary(mle_model)

mle_coefs <- coef(mle_model)

skynet_data_2 <- skynet_data_2 %>% 
  mutate(structural_density_hat  = fit_sffs(beta_distance = mle_coefs['beta_distance'],
                                            beta_size = mle_coefs['beta_size'],
                                            beta_interaction = mle_coefs['beta_interaction'],
                                            q = mle_coefs['q'],
                                            sigma = mle_coefs['sigma'],
                                           dat = .,
                                           price = 765,
                                           marginal_profits = 0,
                                           use = 0
                                           ))

structural_shit <- skynet_data_2 %>% 
  ggplot(aes(density, structural_density_hat)) + 
  geom_point() + 
  geom_abline(aes(slope = 1, intercept = 0)) + 
  labs(x = 'EBS Groundfish Survey Density', y = 'GFW Model of Densities', 
       title = 'Alaska Pollock') +
  hrbrthemes::theme_ipsum() +
  scale_x_continuous(limits = c(0,NA)) + 
  scale_y_continuous(limits = c(0, NA))
  
structural_shit
                                          


```

Well that doesn't work at all...

and the simplest method possible
```{r}



lm_model <- lm(density ~ dist_from_port + mean_vessel_length + dist_from_shore + total_engine_hours + num_vessels, data = skynet_data)

skynet_data <- skynet_data %>% 
  mutate(lm_density_hat = predict(lm_model))


a <- lm_model %>% summary()

simple_model <- skynet_data %>% 
  ggplot(aes(density, lm_density_hat)) + 
  geom_point() + 
  geom_abline(aes(slope = 1, intercept = 0)) + 
  labs(x = 'EBS Groundfish Survey Density', y = 'LM Model of Densities', 
       title = 'Alaska Pollock',
              caption = paste('R2 =',a$r.squared %>% round(2))) +
  hrbrthemes::theme_ipsum() +
  scale_x_continuous(limits = c(0,NA)) + 
  scale_y_continuous(limits = c(0, NA))
  
simple_model
                
HOLYSHIT     

```

Which works actually fairly similarly to the random forest... damnnation. That's no good. Well, you'll see how it does for out of sample prediction. 

At this point, it's time to leave the labbook. All methods have methods to be fit, question now is which one works best. Let's move to a new document that will be a "proof of concept" sketch for the skynet model, with a set of diagnostics comparing alternative methods for fitting the model. 








