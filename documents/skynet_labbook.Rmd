---
title: "Skynet Lab Notebook"
date: "`r Sys.Date()`"
output: html_document
---

```{r}
knitr::opts_chunk$set(eval = F)
```


  * Talk with Dawn about their pollock work 

  * Measuring productivity as a potential latent variable 

  * Illegal guys turn it off selection problem 

  * Check out [this work on mapping effort](https://bluehub.jrc.ec.europa.eu/webgis_fish/)


# Practice Scraping GFW

Goal here is to work up a script querying GFW to get eastern bering sea data. 

The eastern bering sea (EBS) is located roughly within the bounding box of 175w to 155w, and 50n and 58 n. These correspond to 


```{r}
library(tidyverse)
library(bigrquery)
library(lubridate)
library(tmap)
library(leaflet)
library(rgdal)
library(FishData)
library(ggmap)

data("World")

project <- "ucsb-gfw"

library(bigrquery)

fishing_connection <- src_bigquery(project, "eastern_bering_sea") # This function initiliazes a connection with a BQ dataset

summer <- 6:8

ebs <- fishing_connection %>% 
  tbl("ebs") %>% 
  mutate(y_m_d = date(timestamp)) %>% 
  mutate(obs_year = year(y_m_d),
         obs_month = month(y_m_d),
         round_lat = round(lat, as.integer(1)),
         round_lon = round(lon, as.integer(1))) %>% 
  filter(obs_month %in% summer) %>% 
  group_by(round_lat,round_lon,obs_year) %>% 
  summarise(fishing_hours = sum(hours * measure_new_score),
            mean_dist_from_shore = mean(distance_from_shore),
            mean_dist_from_port = mean(distance_from_port)) %>% 
  collect() %>% 
  rename(year = obs_year)




```

Let's make a mep

```{r}

m <-  ebs %>% 
  leaflet() %>% 
  addTiles %>% 
  fitBounds(-175,65,-155,50) %>% 
  addCircleMarkers(~round_lon, ~round_lat)

coords <- SpatialPoints(ebs %>% dplyr::select(round_lon, round_lat) %>% ungroup())  

ebs_map <- SpatialPointsDataFrame(coords, ebs %>% ungroup())

proj4string(ebs_map) <- CRS("+proj=longlat +datum=WGS84")

# proj4string(World) <- CRS("+proj=longlat +datum=WGS84")

# ebs_border <- bbox(ebs_map)

# ebs_hdf <- get_map(location = c(lon = -170, lat = 55),zoom = 5)

alaska <- read_shape('~/Box Sync/Databases/alaska_coastline/alaska_coastline.shp')
# 
# ebs <- meow[meow$ECOREGION == 'Eastern Bering Sea',]

alaska <-  crop_shape(alaska, ebs_map)


ebs_map <- spTransform(ebs_map,crs(alaska))

# plot(alaska)
# plot(ebs_map, add = T)

gfw_ebs_map <- tm_shape(alaska)  +
  tm_lines(col = 'black') + 
  tm_shape(ebs_map) + 
  tm_bubbles(size = 'fishing_hours', col = 'fishing_hours',
             palette = c('blue','red'),
             legend.size.show = F) + 
    tm_legend(legend.outside.position = c("right"),
  legend.outside = TRUE) #format legend

save_tmap(gfw_ebs_map, filename = paste('../',run_dir,'/gfw_ebs_map.pdf', sep = ''))

# ebs_gfw_raster <- ebs %>% 
#   ungroup() %>% 
#   dplyr::select(round_lat, round_lon, fishing_hours) %>% 
#   spread(round_lon, fishing_hours) %>% 
#   dplyr::select(-round_lat) %>% 
#   as.matrix() %>% 
#   raster()
# extent(ebs_gfw_raster) <- extent(ebs_map)
# proj4string(ebs_gfw_raster)

# gfw_ebs_map_plot <- ggmap(ebs_hdf) + 
#   geom_point(data = ebs,size = .1, aes(round_lon, round_lat, color = log(fishing_hours))) + 
#   scale_color_continuous(low = 'blue',high = 'red') +
#     labs(title = 'GFW Fishing Hours')


gfw_ebs_map_plot


```



Now, let's try and get some fishviz data. 

```{r}

ebs_trawl <-  FishData::download_catch_rates( survey="EBSBTS", species_set=20 ) %>% 
  as_data_frame()

ebs_trawl_summary <- ebs_trawl %>% 
  set_names(tolower(colnames(.))) %>% 
  mutate(round_lat = round(lat,1),
         round_lon = round(long,1)) %>% 
  group_by(year,round_lat, round_lon) %>% 
  summarise(total_weight = sum(wt, na.rm = T)) %>% 
  mutate(log_total_weight = log(total_weight))

coords <- SpatialPoints(ebs_trawl_summary %>% ungroup() %>%  dplyr::select(round_lon, round_lat))  

fviz_ebs <- SpatialPointsDataFrame(coords, ebs_trawl_summary %>% ungroup())

proj4string(fviz_ebs) <- CRS("+proj=longlat +datum=WGS84")


# ebs_fviz_raster <- ebs_trawl_summary %>%
#   ungroup() %>%
#   dplyr::select(round_lat, round_lon, log_total_weight) %>%
#   spread(round_lon, log_total_weight) %>%
#   dplyr::select(-round_lat) %>%
#   as.matrix() %>%
#   raster()
# extent(ebs_gfw_raster) <- extent(ebs_map)
# proj4string(ebs_gfw_raster)
```

```{r}


fviz_ebs_map <- tm_shape(alaska)  +
  tm_lines(col = 'black') + 
  tm_shape(fviz_ebs) + 
  tm_bubbles(size = .05, alpha = 0.5, col = 'log_total_weight',
             palette = c('blue','red'),
             legend.size.show = F) + 
    tm_legend(legend.outside.position = c("right"),
  legend.outside = TRUE) #format legend

fviz_ebs_map

```


Hot damn! Let's merge these. Just do it over summer if possible

```{r}
joint_ebs <- ebs %>% 
  left_join(ebs_trawl_summary, by = c('year','round_lat','round_lon')) %>% 
  filter(is.na(total_weight) == F) 

joint_ebs %>% 
  ggplot(aes(log(fishing_hours), log(total_weight), fill = log(mean_dist_from_port))) + 
  geom_point(shape = 21) + 
  geom_smooth(method = 'lm') + 
  xlab('Log GFW Fishing Hours') + 
  ylab('Log Trawl Survey Biomass') + 
scale_fill_continuous(name = 'Distance from Port', low = 'green', high = 'blue') + 
  ggthemes::theme_fivethirtyeight() + 
  theme(axis.title = element_text())


```

```{r}

reg <- lm(total_weight ~ fishing_hours + mean_dist_from_port, data = joint_ebs)

```


# Getting Vessel Characteristics

Go to vessel_lists > YEAR_fishing_vessel_info

nn_max_label gives you what the neural net is 

max_score is how confident it is



# Interpolating EBS trawl survey data

The raw EBS trawl survey data from fishviz is at a relatively course (or true if you want to tihnk about it that way) scale. The problem is that while that's a pretty solid resolution, it's still way coarser than the global fishing watch data themselves. That leaves you with two options more or less. What you were doing was just "rounding" the lats and longs into conformity. That was just erasing a lot of data though, and is obviously just a convenient hack. Another option then would be to just rasterize and overlay both options. Basically, assign an interpolation function to the data. Seems like you really don't want to do that for the GFW data, since the data are in theory closer to a census than a survey. 

So, that then means you need to think about manipulating the trawl survey data. One option would just be to apply a simple interpolation to the data, and then interpolate the density at each point in the GFW data. That would be the simplest option, but also probably the most incorrect. Could mess around with it for a sec though as a starting point. 

From there, the better thing to do would be to mess around with using VAST to interpolate, to basically create the maps in the shiny app. I'm guessing you'd use VAST to do that, but will have to check with Jim. Let's try rasterizing first and see how it looks. 


```{r}
ebs_trawl <-  FishData::download_catch_rates( survey="EBSBTS", species_set=20 ) %>% 
  as_data_frame() %>% 
  set_names(colnames(.) %>% tolower())



pollock_2016 <- ebs_trawl %>% 
  filter(sci == 'Gadus_chalcogrammus') %>% 
  dplyr::select(towid,year,lat,long,wt) %>% 
  filter(year == max(year))

coords <- SpatialPoints(pollock_2016 %>% ungroup() %>%  dplyr::select(long, lat))  

fviz_ebs <- SpatialPointsDataFrame(coords, pollock_2016 %>% ungroup())

proj4string(fviz_ebs) <- CRS("+proj=longlat +datum=WGS84")

pollock_raster <- raster(fviz_ebs, resolution = 1/5)

pollock_raster <- raster(fviz_ebs)

raster_pollock_2016 <- rasterize(fviz_ebs,pollock_raster,'wt')

raster_pollock_2016@data@values


plot(fviz_ebs)

plot(raster_pollock_2016)

interp_pollock <- idw(formula = wt ~ 1, fviz_ebs, newdata = pollock_raster) 

```

# Random Forest Fitting

As a starting point, let's try and focus on on EBS pollock, iconic species, trawl fishery, large boats, should be somewhat doable. So the steps to make that happen. 

1. Use VAST to get density estimates at knots (start at 100 knots)

2. Get EBS GFW data for trawl vessels

3. Use `RANN:nn2` to snap each EBS GFW data point to a knot

```{r}

x1 <- runif(100, 0, 2*pi)
x2 <- runif(100, 0,3)
DATA <- data.frame(x1, x2)
nearest <- nn2(DATA,DATA, k  = 3)

```

4. Summarize the total effort at each knot

5. Fit model!

Here's the concept 
```{r}

library(tidyverse)
library(VAST)
library(ThorsonUtilities)
library(demons)
library(sf)
library(ggmap)
library(FishData)
library(viridis)
library(compiler)
library(TMB)
library(bigrquery)
library(stringr)

ebs_trawl <-
  FishData::download_catch_rates(survey = "EBSBTS", species_set = 50) %>%
  set_names(tolower(colnames(.)))
Version = "VAST_v2_4_0"
Method = c("Grid", "Mesh")[2]
grid_size_km = 25
n_x = c(100, 250, 500, 1000, 2000)[1] # Number of stations
Kmeans_Config = list( "randomseed"=1, "nstart"=100, "iter.max"=1e3 )  


FieldConfig = c("Omega1"=1, "Epsilon1"=1, "Omega2"=1, "Epsilon2"=1) 
RhoConfig = c("Beta1"=0, "Beta2"=0, "Epsilon1"=0, "Epsilon2"=0) 
OverdispersionConfig = c("Vessel"=0, "VesselYear"=0)
ObsModel = c(2,0)  

Options =  c("SD_site_density"=0, "SD_site_logdensity"=0, "Calculate_Range"=1, "Calculate_evenness"=0, "Calculate_effective_area"=1, "Calculate_Cov_SE"=0, 'Calculate_Synchrony'=0, 'Calculate_Coherence'=0)

Region <- 'Eastern_Bering_Sea'


DateFile = file.path(getwd(), 'VAST_output/')
dir.create(DateFile)




alaska_pollock <- ebs_trawl %>%
filter(sci == sci[1]) %>%
rename(
Year = year,
Lat = lat,
Lon = long,
Catch_KG = wt
) %>%
mutate(AreaSwept_km2 = 1, Vessel = 'missing') %>%
select(Year, Lat, Lon, Vessel, AreaSwept_km2, Catch_KG)

qmplot(
x = Lon,
y = Lat,
data = alaska_pollock,
color = log10(Catch_KG)
) +
scale_color_viridis() +
facet_wrap( ~ Year)

Record = ThorsonUtilities::bundlelist(
c(
"alaska_pollock",
"Version",
"Method",
"grid_size_km",
"n_x",
"FieldConfig",
"RhoConfig",
"OverdispersionConfig",
"ObsModel",
"Kmeans_Config"
)
)
save(Record, file = file.path(DateFile, "Record.RData"))
capture.output(Record, file = paste0(DateFile, "Record.txt"))
strata.limits <- data.frame('STRATA' = "All_areas")

Extrapolation_List = SpatialDeltaGLMM::Prepare_Extrapolation_Data_Fn(Region =
Region, strata.limits = strata.limits)

alaska_pollock <- alaska_pollock %>%
na.omit() %>%
mutate(Vessel = as.factor(Vessel))

Spatial_List = SpatialDeltaGLMM::Spatial_Information_Fn(
grid_size_km = grid_size_km,
n_x = n_x,
Method = Method,
Lon = alaska_pollock[, 'Lon'],
Lat = alaska_pollock[, 'Lat'],
Extrapolation_List = Extrapolation_List,
randomseed = Kmeans_Config[["randomseed"]],
nstart = Kmeans_Config[["nstart"]],
iter.max = Kmeans_Config[["iter.max"]],
DirPath = DateFile,
Save_Results = FALSE
)
# Add knots to Data_Geostat
alaska_pollock = cbind(alaska_pollock, "knot_i" = Spatial_List$knot_i)

TmbData = Data_Fn(
"Version" = Version,
"FieldConfig" = FieldConfig,
"OverdispersionConfig" = OverdispersionConfig,
"RhoConfig" = RhoConfig,
"ObsModel" = ObsModel,
"c_i" = rep(0, nrow(alaska_pollock)),
"b_i" = alaska_pollock[, 'Catch_KG'],
"a_i" = alaska_pollock[, 'AreaSwept_km2'],
"v_i" = as.numeric(alaska_pollock[, 'Vessel']) - 1,
"s_i" = alaska_pollock[, 'knot_i'] - 1,
"t_iz" = alaska_pollock[, 'Year'],
"a_xl" = Spatial_List$a_xl,
"MeshList" = Spatial_List$MeshList,
"GridList" = Spatial_List$GridList,
"Method" = Spatial_List$Method,
"Options" = Options
)

TmbList = Build_TMB_Fn(
"TmbData" = TmbData,
"RunDir" = DateFile,
"Version" = Version,
"RhoConfig" = RhoConfig,
"loc_x" = Spatial_List$loc_x,
"Method" = Method
)
Obj = TmbList[["Obj"]]

Opt = TMBhelper::Optimize(
obj = Obj,
lower = TmbList[["Lower"]],
upper = TmbList[["Upper"]],
getsd = TRUE,
savedir = DateFile,
bias.correct = FALSE
)

Report = Obj$report()
Save = list(
"Opt" = Opt,
"Report" = Report,
"ParHat" = Obj$env$parList(Opt$par),
"TmbData" = TmbData
)
save(Save, file = paste0(DateFile, "Save.RData"))

MapDetails_List = SpatialDeltaGLMM::MapDetails_Fn( "Region"=Region, "NN_Extrap"=Spatial_List$PolygonList$NN_Extrap, "Extrapolation_List"=Extrapolation_List )
# Decide which years to plot                                                   
Year_Set = seq(min(alaska_pollock[,'Year']),max(alaska_pollock[,'Year']))
Years2Include = which( Year_Set %in% sort(unique(alaska_pollock[,'Year'])))

Dens_xt <- SpatialDeltaGLMM::PlotResultsOnMap_Fn(plot_set=c(3), MappingDetails=MapDetails_List[["MappingDetails"]], Report=Report, Sdreport=Opt$SD, PlotDF=MapDetails_List[["PlotDF"]], MapSizeRatio=MapDetails_List[["MapSizeRatio"]], Xlim=MapDetails_List[["Xlim"]], Ylim=MapDetails_List[["Ylim"]], FileName=,,, Year_Set=Year_Set, Years2Include=Years2Include, Rotate=MapDetails_List[["Rotate"]], Cex=MapDetails_List[["Cex"]], Legend=MapDetails_List[["Legend"]], zone=MapDetails_List[["Zone"]], mar=c(0,0,2,0), oma=c(3.5,3.5,0,0), cex=1.8, plot_legend_fig=FALSE)

alaska_pollock_densities = cbind( "density"=as.vector(Dens_xt), "year"=Year_Set[col(Dens_xt)], "e_km"=Spatial_List$MeshList$loc_x[row(Dens_xt),'E_km'], "n_km"=Spatial_List$MeshList$loc_x[row(Dens_xt),'N_km'] ) %>% 
  as_data_frame() %>% 
  mutate(knot = as.numeric(factor(paste(e_km,n_km)))) %>% 
  arrange(knot,year)

project <- "ucsb-gfw"

fishing_connection <-
  src_bigquery(project, "skynet") # This function initiliazes a connection with a BQ dataset


ebs_raw <- fishing_connection %>%
  tbl("ebs_w_vessels") %>%
  collect(n = Inf)

ebs <- ebs_raw %>% 
  select(-b_mmsi, -b_year) %>% 
  set_names(str_replace_all(colnames(.), '(a_)|(b_)',''))

utm_coords <- SpatialDeltaGLMM::Convert_LL_to_UTM_Fn( Lon=ebs$rounded_lon, Lat=ebs$rounded_lat, zone=Extrapolation_List$zone, flip_around_dateline=Extrapolation_List$flip_around_dateline) %>% 
  rename(e_km = X, n_km = Y) %>% 
  select(e_km, n_km)

ebs <- ebs %>% 
  bind_cols(utm_coords)

ebs %>% 
  ggplot(aes(best_label)) +
  geom_bar()

knots <- alaska_pollock_densities %>% 
  select(knot,e_km,n_km) %>% 
  unique()

nearest_knot <- RANN::nn2(knots %>% select(-knot), utm_coords, k = 1)

ebs$knot <- knots$knot[nearest_knot$nn.idx]

ebs %>% 
  select(rounded_lat, rounded_lon, knot) %>% 
  unique() %>% 
  ggplot(aes(rounded_lat, rounded_lon, color = knot %>% factor())) + 
  geom_point() 

trawl_fishing_by_knot <- ebs %>% 
  filter(best_label == 'trawlers') %>%
  rename(th = total_hours) %>% 
  group_by(year,knot) %>% 
  summarise(total_hours = sum(th, na.rm = T),
            total_engine_hours = sum(th * inferred_engine_power, na.rm = T),
            num_vessels = length(unique(mmsi)),
            dist_from_shore = mean(mean_distance_from_shore, na.rm = T),
            dist_from_port = mean(mean_distance_from_port, na.rm = T),
            mean_vessel_length = mean(inferred_length, na.rm = T))

skynet_data <- trawl_fishing_by_knot %>% 
  left_join(alaska_pollock_densities, by = c('year','knot')) %>% 
  ungroup() %>% 
  mutate(vessel_hours = total_engine_hours * num_vessels)

# skynet_data <- skynet_data %>%
#   group_by(knot) %>% 
#   arrange(knot,year) %>% 
#   mutate(total_hours_lag_1 = lag(total_hours,1))
#   
# skynet_data <- skynet_data %>% 
#   remove_missing() %>% 
#   ungroup()

skynet_model <- randomForest::randomForest(density ~ ., data = skynet_data %>% select(-e_km, -n_km, -knot,-year,-total_hours))

randomForest::varImpPlot(skynet_model)

skynet_data <- skynet_data %>% 
  ungroup() %>% 
  mutate(density_hat = predict(skynet_model))

lm(density ~ total_hours, data = skynet_data) %>% summary() -> a

badshit <- skynet_data %>% 
  ggplot(aes(density, total_hours)) + 
  geom_point() + 
    geom_smooth(method = 'lm')+
  geom_abline(aes(slope = 1, intercept = 0)) + 
  labs(x = 'EBS Groundfish Survey Density', y = 'GFW Fishing Hours', 
       title = 'Alaska Pollock',
       caption = paste('R2 =',a$r.squared %>% round(2))) + 
  hrbrthemes::theme_ipsum() +
  scale_x_continuous(limits = c(0,NA)) + 
  scale_y_continuous(limits = c(0, NA))

HOLYSHIT <- skynet_data %>% 
  ggplot(aes(density, density_hat)) + 
  geom_point() + 
  geom_abline(aes(slope = 1, intercept = 0)) + 
  labs(x = 'EBS Groundfish Survey Density', y = 'GFW Model of Densities', 
       title = 'Alaska Pollock',
       caption = paste('R2 =',mean(skynet_model$rsq) %>% round(2))) + 
  hrbrthemes::theme_ipsum() +
  scale_x_continuous(limits = c(0,NA)) + 
  scale_y_continuous(limits = c(0, NA))
  


```

```{r fig.cap="HOLY SHIT IT'S ALIVE"}

HOLYSHIT

```

# 2017-06-05

Well holy shit, things seem to be working. Specifically, a random forest model seems capable of reasonably predicting densities in space and time. So, what's the next step here to make sense of all this. 

I think before you go to Chris you need to give the structural approach a bit more thought. So, I'd like to get a "structural" model built this week, with an obvious candidate being an ideal-free distribution model, wherein you assume that marginal profits are equal across scace for a given time unit, and then "solve" or estimate abundance to make this true, following methods from Miller et al. There are a couple odd constants here that you'd need to explore. 

Let's maybe first focus on getting the structural approach working. Once that's working, let's integrate those into a cohesive script and start thinking through the right set of diagnostics. 

At it's core... 

R^2^

Maps

Out of sample prediction in space, time, species

Bias

# Structural Fitting

The idea here goes back to the original premise: if marginal profits are indeed equal in space and time, 

This is a summary of the proposed model structure for version 1.0 of the "skynet" model. The objective of this model is to estimate spatio-temporal indicies of abundance using GFW effort data. The general method is an adaptation of @Miller2016, wherein the authors attempt to estimate quota lease prices by fitting observed patterns of effort to predicted patterns of effort based on the expected profitability of fishing different patch (subject to the constraint that marginal profits are equal in each patch). Your goal is to flip this around a bit, and now predicted relative abundance as a function of the effort, price, and cost for fishing that patch, again conditional on the same assumption as above. 


So, with that in mind, the fraction of trawl vulerable biomass left after fishing is just the survival of the stock times the fraction of the stock that is vulnerable to trawl survey.... That seems wrong. Why wouldn't it go into the survival itself?

Expected CPUE is based on the self reported CPUE in prior time period. In your case, you're going to try and estimate that value itself. 

They assume observation error here, in that there are other things influencing expected CPUE besides last years, that we cannot directly observe, so they apply log normal error term to the expected CPUE. What does that say about your formulation? 

The tobit process says that effort will be 0 if expected marginal profits if T were 0 are greater than the overall marginal profits across the system. 

So, overall, the model works by estimating the quota prices *c* that rationalize the equation. Can you think about it a bit more generally for your use. You don't need to think about the whole "fraction vulnerable to trawl thing". 


Note that this is "expected profits from a marginal unit of effort", meaning marginal profits, $d\pi/dE$


The key sentance that you're missing here is the "in which T units of effort have already been applied". So, this is basically accounting for the draw-down of the stock, which your standard method doesn't account for (just taking the usual marginal profits per unit E). So this is say, conditional on some already observed amount of effort, the expected profits per unit of one more unit of effort is price * expected 





So, what this is saying, with that lovely exponent in there, let's start with revenues. 

This is saying that the revenue of the first unit of effort is just $p*CPUE*E^{q1}$. And so, this decays 

```{r}

p <-  10
cpue <-  100
q = .1
effort = 0:100

marginal_revenue <- p * cpue * exp(-q * effort)

plot(effort, marginal_revenue)

```

So, those are "marginal" revenues, calculated in an interesting way which is a little different thatn the way that you would usually think about it, mostly since your usual approach assumes that catch is $pqEB$, and so the marginal revenue with respect to effort is constant $pqB$, meaning that conditional on B, you get the same revenues from the first to the last additional unit of effort, which seems wrong. While, with this version, you're acknolweding that marginal revenues go down as you kill the damn stock. SO, they should do about the same thing, except in your case, you usually assume that marginal revenues are constant, and increase cost per effort to give the thing some concavity. Where they assume that the cost of every unit of effort is the same, but the expected revenues go down as the stock is drawn down. 

So, they just say here are the revenues per unit effort for the Tth unit of effort, minus the variable cost. So, now just try and understand the variable cost. Becuase this is marginal, that is why it should be the cost per unit effort, since it's saying that marginal profits are the expected revenues under one more unit of effort minus the cost of the unit of effort. BOOYAH, I finally get why this thing works that way. 

So, what is the model then. 

If you did your math right 

$$B_{i,t} = \frac{e^{E_{i,t}q_{t}}(c_{i,t} + \pi)}{p_t}$$

So, what does this leave you to estimate? And let's start simple. 

*E* is known

*q* is certainly not known

*c* is partially known, you have some information vessel size, distance from shore, gas prices, etc. Look more carefully at how they define c, your standin for VC here. 

*$\pi$* You start off assuming zero

*p* this should really just be a scalar, take as data. 

So, regardless of how you parameterize it, it's down to q and c as the only options for model tuning. 

So what do you do with this? 

q is in units of.... something 1/kwH?. You can't estimate a q for each knot, so how would you go about estimating it? You could do one q by year, which doesn't really help a lot, or you could try and make q a function of vessel size, something like that. Doesn't seem like you have a whole lot of data to create a rational model of this besides treating it as a nuisance parameter that basically scales the relationship between effort and the magnitude of density. 

Given how troublesome q is normally, seems much more logical to think about tuning costs. In one world, you think of costs as being directly observable, and so the cost per kilowat hour at a given site is known perfectly. To think about it in simple terms, you could certainly get data on labor costs per kilowat hour and fuel costs per kwh for a given site, as a function of vessel size and distance from port. So, in that world cost c is just

$ c = labor*distance + fuel * distance$ or then

$c = distance*(labor + fuel)$

In that case, since c needs to be in \$ where kwh is one, so labor and fuel would have in units of \$/kwh, so labor and fuel costs would have to be in units of \$/kwh, where \$/kwh internalizes the travel costs, so it's something like (miles / MPG)*$(gallon).... but then that cost would in theory be spread over fishing hours, so gets trickier. 

Seems a little easier to try and estimate it. 

So, in that world, you say that cost is something like $\beta_{1}*(distance * vessel_size) + \beta_{2}*distance + \beta_{3}*vessel_size$

Now, how would you think about doing this. The "vessel size" thing is a little tough, since that's not a characteristic of the site, but of the vessels chosing to fish at that site. 

The core problem then is that the you think that different vessel sizes should have different cost per hour to fish at a given site. So, you could do something like break the vessels into fleets by vessel size. Assume one expected CPUE (abundance) common abundance at a site. Then, tune q and c for each fleet, such that they all produce the same expected CPUE. 

Or, make it a bit simpler. Take the observed CPUE, and for each fleet, estimate a q and a cost per unit distance (where the $\beta$ on distance would be in units of $/distance*hour...)?

Think about that. You need cost to be in units of \$

You're going to get cost as $\beta*distance$, where distance is units of km. So, $\beta$ would be a little tough to interpret, since it would have to be in units of $/km for a particular fleet. 


So, the marginal profits would be

$\pi_{i,j,t} = p*cpue_{j,t}*e^{-q_{j}T_{i,j,t}} - \beta_{j,t}*distance_{i,t}$

That model would be assuming that each fleet is targeting mutually exclusive parts of the stock... don't worry about that for now. So, in the single fleet world, the model is


$[q_{j}, \beta_{j,t} | cpue_{j}] \propto [cpue_{j} | q_{j},\beta_{j}][q_{j}][\beta_{j,t}]$

Where you can assign prior to beta_j based on fuel prices and labor costs. 

## What the hell units are things in in here?

price: $/kg
cpue: kg/kwh
q: 1/kwh
T: kwh

So if that's the case, then revenues have to be in \$/kwh, therefore cost has to be in units of \$/kwh as well. So, if you are scaling cost by distance from port for each patch, then \beta has to be in units of $/km*kwh, which is weird. 

Let's look at a bit more carefully at how those cost parameters are set in @Miller2016. 

" We compute variable costs (VC $/hr) using information on input costs (fuel, labor, ice, and distance from port)"

Because the unit of ob- servation is a patch and period rather than a tow, a single cost per unit effort must be assigned to each patch for each period. This required designating a single distance from port for each patch. This was deemed to be the distance from the center of the patch to the nearest port for which trips originating from that port entailed at least 100 tows in aggregate. That same port was used to match patches to fuel prices from the fuel cost survey. If fuel costs were not available for that port, prices from the nearest port included in the survey were used. Dividing the resulting cost per trip by the average number of tow hours per trip for all trips to that patch yielded our mea- sure of cost per tow hour. In a subsequent model, we allow the opportunity cost of time to vary with management (see Abbott and Wilen 2011). This is accomplished by adding an interaction between distance and management to the variable cost term"

So to match Miller more precisely, you would calculate variable cost directly. I'm arguing that this doesn't give you enough flexibility, though would be worth exploring this more if you either want to isolate specific parameters for optimization (e.g. just model "unobserved costs", taking the fuel/labor costs as data). 

So what would that look like? If costs need to be in units of $/KWH, then total cost could be....

1. Take all the kwh for a given knot, fleet, year

2. Calculate the fuel costs to operate those kwh

3. Calculate the labor costs to operate those kwh

4. Calculate the fuel and labor costs to get to a site 

5. Calculate the number of trips to a site... Can you get that? 

6. Use all that to get an average cost per kwh, as the fuel costs + labor costs while operating in the patch, + the fuel and labor costs of getting there, divided by the average kwh exereted at that knot. 

You could use this estimate as either a base, so that the actual cost per kwh is some multiplier times that thing. So, the idea would be that the relative magnitude of the costs would be right across knots, but you're consistently missing (or adding) something, and that multiplier would make up for that. 

An alternative to that would be to think of this estimate as a strongly informative prior on the cost parameter at each knot, and then estimate it. 

A last option would be to treat it as a latent variable, where you estimate costs in each knot and time step as coming from a distribution with mean derived from the empirical approach above, and some sigma. 

As one other approach, you could turn it into a reduced form equation, basically turning it into a linear regression


For now, let's go with simply setting priors on the thing based off of some rough sense of the magnitude of costs as priors. 

Ah god damn it though. You can't rely on a framework for just estimating costs, you need a model of costs. So what if you made it simple. 

Say that cost per unit hour is 

$\beta_{1}*distance + \beta_{2}*meanvesselsize +  \beta_{3}*distance*meanvesselsize$

Where the prior on $\beta_{1}$ is just \$/km, which you would get from fuel efficiency of vessel and price per gallon

the prior on $\beta_{2}$ would be crew wages per hour * number of crew

Unformative prior on $\beta_{3}$

So, the final model will be....

$ cpue_{k,t} = \frac{1}{p}e^{qE_{k,t}}(\beta_{1}*distance_{k} + \beta_{2}*meanvesselsize_{k,t} +  \beta_{3}*distance_{k}*meanvesselsize_{k,t} + \pi) + e_{i,j}$

prior on q: gamma

prior on $\beta$ normal with mean from data and sigma set to something 

prior on $\sigma$ inverse gamma. 




## Model and Data

Let's go back to @Miller2016 and make sure that you understand what's happening here. 

The crux: expected profits are price - quota cost * expected harvest, minus variable costs

Fishers are not assumed to consider the dynamic consequences of spatial fishing choices, but rather react to known prices and costs. 

X^o^ is the biological stock at the start of the time period 

```{r}

effort <-  100
pi <- 0
q <-  .1
b <- 10



```



**Check this after work**
$$Effort_{f,t,s} = \frac{1}{q_{f,t,s}}log(\frac{p_{f,t,s}B_{f,t,s}}{c_{f,t,s} + \bar{\pi}_{f,t}})$$

$$B = \frac{(c + \pi)}{pe^{-qEffort}}$$

check that math

```{r}

q <- .1
p <-  10
b <- 100
pi <- 103
c <- 1

effort <- 1/q * log( (p * b) / (pi + c))

bhat <- (c + pi) / (p * exp(-q *effort))

bhat2 <- ((c + pi) * exp(q *effort)) / (p)

```


Note that *c* here are variable costs per unit  effort ($/hour). You're omitting the quota price for now (though that might need to be incorporated in here at some point)

$$[p,c,q,\bar{\pi}| B] \propto [B |p,c,q,\bar{\pi},effort][priors]$$

In english, the likelihood of price, cost, q, and marginal profits conditional on Biomass is proportional to the likelihood of the biomass conditional on a given price, cost, q, and marginal profits and data effort and priors. 

So, how to fit this fun thing? 

notation. Let's call species *s*, location *l*, and time *t*, and fleet *f*

## price *p*

You probably don't need to actuall estimate *p*. You can match the species in the EBS trawl survey data to Tyler's price database as a starting point. So, in that world, you have $p_{s}$. You could also, if you can link gear types from GFW to type of product, in which case you would have $p_{s,f}$

## cost *c*

This is in units of $/hour (where hour is presumably fishing hour, not travel time), so that travel time is incororated into the cost of actual fishing. From @Miller2016, they compute VC, which you call *c*, as fuel, labor, ice, distance from port. As a starting point, you could focus on distance from port and vessel size. Something like 

$$c_{l,f} = f(distance,fuel,labor, misc)$$

You can get distance from GFW data. Fuel prices you could either estimate, take global averages, or better yet, scrape from [here](bunkerindex.com). Labor costs are a little tricker. You could just make it proportional to vessel size and estimate something from there. Or, you could use [BLS data](https://www.bls.gov/oes/current/oes453011.htm) for the US as a starting point, seems like a good idea. Misc costs I think you have to ignore for now. 

So, the way that they do it in @Miller2016 is to calculate the cost per trip, where that is presumably $fuel*distance + labor * time$, where *time* is a function of distance, that you might be able to get from GFW. You would then divide that total cost by the average number of fishing hours exerted at that patch, to get to a cost per unit fishing effort at that patch. That seems a little goofy to me though, since it says that say you have a reallllly far away patch that people fish at a lot though, that's going to say that the cost per unit effort is really low out there, where in fact it is really damn high. I could actually see the opposite, where per trip you fish a lot on a very far away place, since you're not going to go back and forth from port a bunch, but rather stay out there and fish. But, let's go with it for now, since it's what they do in that paper. Best thing to do is start with that. So, for each patch/fleet then, you'll calculate the average number of fishing hours per "trip" out there, which you'll need to poke at the data to really understand. So, to match @Miller2016 exactly. For each patch in the database, you would calcualte it's distance from port. For each fleet type, you would get something like miles per gallon. You would then get cost in fuel. For each fleet, you would also get average speed, and use that to get the time taken to get to each place, and multiply that by cost per hour times crew size. Add that to the fuel costs, and you have the total costs to fish at a site. Divide that by the average number of fishing hours at the site, and you've got yourself cost per unit fishing effort at said site. 

There's a few problems with that. I'm still not comfortable with the divide by average fishing hours per trip at site, since I feel like it's likely to really skew the cost for far away sites that people hang at for a long time. There's also the issue that you can string sites together, so the marginal cost of fishing around the priblofs is spread out across the sites out there. But let's maybe go with it for now as a starting point. 

As an alternative, you could estimate these. So, say cost is something like 

$$c = aDistance + bLength + cDistanceLength + constant$$

and you could fit those parameters as part of the model, possible with some informative priors


As one other alternative, you could collapse the distance and the labor into one thing....

Take distance, convert it to hours using average distance per hour for that vessel size. Take the hours *the number of workers for that vessel size * hourly wage. That's the estiamte of labor costs for that trip. Then, take distance, convert to gallons of fuel per the average distance/gallon for the average vessel size at that site. That's your estimate of "travel" costs or something like that, and then just have a coefficient times that. 

You can also take that same calculation and break apart in the same interaction mode you looked at above, so it would be fuel costs + labor costs + interaction or plus unknown... you can mess with different options here. 

For now, let's KISS and start with seeing if you can get the model to fit without additional data on fuel or labor costs, and then launch into that

## Fitting the structural model

Let's get to work!

Let's start with MLE with the no-frills methods. 

Then move to STAN no frills, make sure it works

Then start adding complexity and data to cost model 

```{r}


fit_sffs <- function(beta_distance,
                     beta_size,
                     beta_interaction,
                     q,
                     sigma,
                     dat,
                     price,
                     marginal_profits = 0,
                     use = 'mle') {
  
  old_sigma <- sigma
  sigma <-  exp(sigma)
  
  old_q <- q
  
  q <- exp(q)
  
  beta_distance <-  exp(beta_distance)
  
  beta_size <- exp(beta_size)
  
  beta_interaction <- exp(beta_interaction)
  
  # beta_intercept <- exp(beta_intercept)
  
  cost <-
  beta_distance * dat$dist_from_port + beta_size * dat$mean_vessel_length + beta_interaction * dat$dist_from_port * dat$mean_vessel_length
  
  estimated_abundance <-
  ((cost + marginal_profits)) / (price  * exp(-q * dat$total_engine_hours))
  if (use == 1){
  out <-
  -sum(stats::dlnorm(dat$density_lag1, log(estimated_abundance), sigma, log = T))
  } else {
    out <- estimated_abundance
  }
  return(out)
                     
}

skynet_data_2 <- skynet_data %>% 
  group_by(knot) %>% 
  arrange(knot,year) %>% 
  mutate(density_lag1 = lag(density, 1)) %>% 
  ungroup() %>% 
  remove_missing() %>% 
  filter(density_lag1 > 0)

 
mle_model <- stats4::mle(
  fit_sffs,
  start = list(
  beta_distance = log(100),
  beta_size = 100 %>% log,
  beta_interaction = 100 %>% log,
  sigma = log(sd(skynet_data$density)/100),
  q = log(1e-7)),
  fixed = list(price = 765, 
               dat = skynet_data_2 %>% filter(total_engine_hours > 0, density_lag1 > 0),
               marginal_profits = 0,
               use = 1),
  upper = list(sigma = log(100),
               q = log(1.5e-5)))

summary(mle_model)

mle_coefs <- coef(mle_model)

skynet_data_2 <- skynet_data_2 %>% 
  mutate(structural_density_hat  = fit_sffs(beta_distance = mle_coefs['beta_distance'],
                                            beta_size = mle_coefs['beta_size'],
                                            beta_interaction = mle_coefs['beta_interaction'],
                                            q = mle_coefs['q'],
                                            sigma = mle_coefs['sigma'],
                                           dat = .,
                                           price = 765,
                                           marginal_profits = 0,
                                           use = 0
                                           ))

structural_shit <- skynet_data_2 %>% 
  ggplot(aes(density, structural_density_hat)) + 
  geom_point() + 
  geom_abline(aes(slope = 1, intercept = 0)) + 
  labs(x = 'EBS Groundfish Survey Density', y = 'GFW Model of Densities', 
       title = 'Alaska Pollock') +
  hrbrthemes::theme_ipsum() +
  scale_x_continuous(limits = c(0,NA)) + 
  scale_y_continuous(limits = c(0, NA))
  
structural_shit
                                          


```

Well that doesn't work at all...

and the simplest method possible
```{r}



lm_model <- lm(density ~ dist_from_port + mean_vessel_length + dist_from_shore + total_engine_hours + num_vessels, data = skynet_data)

skynet_data <- skynet_data %>% 
  mutate(lm_density_hat = predict(lm_model))


a <- lm_model %>% summary()

simple_model <- skynet_data %>% 
  ggplot(aes(density, lm_density_hat)) + 
  geom_point() + 
  geom_abline(aes(slope = 1, intercept = 0)) + 
  labs(x = 'EBS Groundfish Survey Density', y = 'LM Model of Densities', 
       title = 'Alaska Pollock',
              caption = paste('R2 =',a$r.squared %>% round(2))) +
  hrbrthemes::theme_ipsum() +
  scale_x_continuous(limits = c(0,NA)) + 
  scale_y_continuous(limits = c(0, NA))
  
simple_model
                
HOLYSHIT     

```

Which works actually fairly similarly to the random forest... damnnation. That's no good. Well, you'll see how it does for out of sample prediction. 

At this point, it's time to leave the labbook. All methods have methods to be fit, question now is which one works best. Let's move to a new document that will be a "proof of concept" sketch for the skynet model, with a set of diagnostics comparing alternative methods for fitting the model. 



# Check in the Steve

Pull in chlorophyl, depth, and wind data

Try also adding in the effort in the surrounding nodes

Try it by joining to the effort instead of collapsing down

See how influential past observed abundance actually is


# 2017-06-17 Check in with Chris

Agenda: 

Thoughts on progress so far

What's the best path forward

Angles

  - Machine learning focused, trends and distributions of density around the planet
      - Species, climate, etc. 
  
  - Pursue structural approach
  
  - Model comparisons: Show performance of alternative models (machine learning vs structural)
  
  - Value of information
      - Compare a catch only model to a "abundance only" model to RAM
      - Simulation framework to think about "how off" can the index of abundance be in the final years to still be useful, scaled to error rates observed by model
      - See how much fit improves comparing COM to a surplus production style model fit to paired catch and GFW data
      
  - Abandon ship

# 2017-07 plan of attack

OK, what needs to happen to make this project work. 

The paper: 

**A global assessment of distributions of fish abundance in space and time**

What needs to happen?

You need to decide on the model structure. 

Performance of this model will be judged by the ability to predict observed abundances from FishData. 

Pretty clear that you can ditch the strutural approach for the time being. 

So, what steps need to go into making the machine learning approach work well. 

One key question that you need to address is "what's the dependent variable". Is it all the fished species? Subsets by gear type? 

So far you've been focusing on one species, which isn't really all that useful in practice. I propose two new approaches. 

For all, let's focus on making it work for EBS, and then extend the framework for all areas. 

## What species?

### All fished species

Filter out the abundance of fished species from FishData. Create a net abundance index which is the sum of the densities across species. 

Fit the model to that. 

### Creating species assemblages by gear type

The idea here is to look at each of the gear type classifications available in GFW, and do a lit review on the either habitat or depth profile of that gear type. Then, use fishbase or something similar to classify the depth profile/habitat for each species in FishData. Then, generate, for each type of gear, an index of say "deep water demersals" "mid-water pelagics" for each species group. Then, train the model on that index, using the appropriate vessel grouping from GFW. 

Another thing to deal with here is going to be 

## What are we measuring

With either species grouping, you then want to run the model once predicting absolute abundance, and another predicint "delta abundance", where that is the change in abundance in that space since the last year. 

## CPUE

This one is simpler, work with tyler to try and match catch to GFW, and create CPUE indicies. But, this would really only be at the country or LME level if you match to SAUP. 

## Metrics

Judegement will be passed on ability of the model to predict out of spatial sample, out of time sample, and out of species sample. The last one means, suppose that you fit the model on a subset of fished species, and now need to predict the abundance with another fished species that wasn't in your data in there, how well does it do. 

You will test

- Absolute abundance
    - in space
    - in time
- Delta Abundance
  - in space
  - in time

Your objective right now is to get to metrics. Specifically, compare performance of different models to achieve the above, and write up a summary of what works and what doesn't, and then see what you can do to improve the model. 

# 2017-08-10 - alpha version alive

OK, the model now works from raw data to model fitting with just about every bell and whistle you could possible need. Eventually will need to bring in some more surveys, and tune up the VAST process a lot more carefully, but you're in business. 

So what are the next steps? At it's face the model is looking seriously promising: R^2^ is about 0.75 for an off the shelf guess, and can make the case at least that it's a hell of a lot more reliable so far than the PRM ever looked. 

But, there's still a long way to go. 

Step 1, need to make a concrete set of diagnostic metrics and plots for each model run. Once that is established and running smooth, ou can then pass any number of crazy model runs to the diagnostic functions. 

Once that is up and running, you then need to run a guantlet of alternate model forms and data inputs (e.g. leaving out regions and years, running on deltas instead of raws, ranks instead of raw values etc. )

# 2016-08-15 - model fitting

Rather than trying insane combinations of independent variables, why not build the filtration process in; will take a bit longer for each individual run but saves a lot of time in hunting over random variables. 

For tree-based methods; pass a model with all the ind vars of a certain type and  a random variable. Then, drop all variables less important than the random variable, and retune and re-predict.

For linear regression, do your backwards selection

For structural, well just the usual there I suppose. 

Going to borrow heavily from run-skynet-poc to get this working. 

Also, what the hell do to with missing values. 

The point of this phase is in deciding which model to use. 

At the end of the day, presumably, you're going to want OOB predictions of the best model fit to all the data. 

How you pick the best model is a bit tricky. Suppose that you pick the best model as the one that does the best job predicting WCGHL while trained on all others. How then do you want to produce the "final" model? Presumably trained on as much data as possible, so including WCGHL, though only useing OOB samples for final "obs vs predicted" plots. i.e. it really wouldn't seem to make much sense to train the model, and arbitrarily leave out some data as training in the final model structure? But I should read the book on this one some more. 

So if that's the vission. DON'T OVERENGINEER THIS THING. If at the end of the day you're going to try and include all the data from the "best" model, then don't go crazy on data selection on the tuning phase. 

Given that data aren't missing randomly (e.g. the lags), it doesn't seem fair to compare performance across models that fundamentally have access to different kinds of data. 


So with that in mind, what the hell do you want to do about. 

could also use `caret:preProcess` to fill in missing data, but let's tackle that next, since that would just be a special case of the process below

- Break the data into data classes no lags, lag 1, lag 2, delta. 
  - pass a 'must_have' variable in there somewhow
- Filter down to candidate variables, and run na.omit
- That will leave you with a complete dataset for each data class
- Then, run variable selection on a model tuned to those data
- Use that tuned model to predict training data (either kfold, other regions, other years, etc.)
- Take what you judge to be the best model. Tune it to all the data, and use for global domination. 

# 2017-08-16 v1.0 live

it livessss

OK, so things aren't looking great for the out of sample world at the moment. Model works great when it's job is to predict things inside the study region, even if outside of study time. But, it falls apart trying to be fit on alaska and then predict the west coast, as you somewhat would suspect

So, solutions

- center and scale by region rather than globally, so that variables are extreme for the region, not locally. 

- Include log-density in center-scaling so that it's about relative magnitude, and trends, not absolute values. i.e. predicting high density as a funciton of high effort, low temperature, low productivity, etc. 

- Let's just take a look at that and see what happens

- the other thing is that you're missing tons of wind data... something is going wrong in the wind vector vonverson, but the data aren't working 

- Make and check a delta method, that is predicting changes in abundance instead of current values

- Make a GFW-only model 

- Test out structural and linear models as well. 

- Do something else for a bit today. 

# 2017-08-28 Getting structural model running

What needs to happen to get the structural model up and running? 

First things first let's get the bare bones version up and running. 

That will involve passing a set of "structural variables" to the function, which will go into the cost function and the q function. 

Then good old MLE

From there, you need to develop a way to do it "fleet by fleet", but let's walk before we run

Per Juan's suggestion, you can try running it a) without the variable tuning and b) tuning but based off of conditional importance from party::varimp

So, you can certainly play around with a few other options in terms of the algorithm and model selection, specifically trying out cforest, gbm, etc., see if that works


# 2017-10-27 Update from David

Number of satellites has changed dramatically over time, with more and more coming up. That means that in the early years there were way less satellites running around

Read "the use and misuse of global AIS data"

# 2017-11-28 Check in with Juan and David

Check and make sure that the query returns the same number of rows if you comment out the MPA thing

Don't use best_label

Might be good to add an "or" statement to both inferred and known vessels 

Move things over to cloud storage to save space

Add join to "good_segments"

Can also filter out spoofing  (greater than 1.01) and offsetting (true or false, you want false) both in vessel_info table

Might be good to get rid of "at port" fishing activity

So filter out by distance from shore/port (meters) and by speed (knots)

# 1/18 check in with chris and steve

What if you start with a paper on the relative importance of environmental/economic drivers to fish abundance

are fishermen being inefficient?

Do the fishermen know more than the survey?

How fast does the predictive ability decay?

# Getting more data

I would love to have a bit more data to show next meeting. Let's do a little poking to see what it would take to start geting some of the ICES data usine icesDataras


```{r, eval = F}

library(icesDatras)

surveys <- icesDatras::getSurveyList()

test <- icesDatras::getDATRAS(record = "CA",survey = surveys[1], years = 2012:2015, quarters = c(1,4))

data(aphia, package = 'icesDatras')
#Species codes are contained in that aphia database, for some reason. 

test <- test %>% 
  left_join(aphia, by =c('SpecCode' = 'aphia_code' )) %>% 
  filter(!is.na(species))


```

That seems to work, but it also seems to be reallllly idiosynchratic, and the resulting data is far from straightforward. Seems like this would be a good thing to rope Jim into if he's already done it, rather than reinventing the wheel here. Let's send him a writeup of where you are with a request for help a) checking your VAST operations and b) accessing north sea data



```{r, eval = F}
download_datras = function(survey="NS-IBTS", species_set=10, years=1981:2015, gear="Gov", quarters=1, localdir=getwd(), verbose=TRUE ){
  # NOTES
  # CatCatchWgt varies among species within the same haul
  # CatCatchWgt seems to have many cases where it is implausibly low given fish lengths in that tow (e.g., HaulID="1991.1.ARG.NA.13" for "Gadus morhua")

  # Area-swept notes:
  # Irish West Coast Gov gear has a door width of 48 meters (https://datras.ices.dk/Home/Descriptions.aspx)
  # Fresh survey has door width of 20 meters (https://datras.ices.dk/Home/Descriptions.aspx)
  # Duration is in minutes

  # Download data if necessary
  if( !file.exists(paste0(localdir,"/",survey,"_hh.RData")) ){
    hh = icesDatras::getDATRAS("HH", survey=survey, years=years, quarters=quarters)
    save( hh, file=paste0(localdir,"/",survey,"_hh.RData"))
  }else{
    load( file=paste0(localdir,"/",survey,"_hh.RData"))
  }
  if( !file.exists(paste0(localdir,"/",survey,"_hl.RData")) ){
    hl = icesDatras::getDATRAS("HL", survey=survey, years=years, quarters=quarters)
    save( hl, file=paste0(localdir,"/",survey,"_hl.RData"))
  }else{
    load( file=paste0(localdir,"/",survey,"_hl.RData"))
  }
  if( !file.exists(paste0(localdir,"/",survey,"_ca.RData")) ){
    ca = icesDatras::getDATRAS("CA", survey=survey, years=years, quarters=quarters)
    save( ca, file=paste0(localdir,"/",survey,"_ca.RData"))
  }else{
    load( file=paste0(localdir,"/",survey,"_ca.RData"))
  }

  # Restrict to a given gear
  if( verbose==TRUE ){
    print( paste0("Rows in hh: ", nrow(hh)) )
    print( paste0("Rows in hh with Gear=",gear,": ", sum(tolower(hh$Gear) %in% tolower(gear))) )
  }
  hh = hh[ which(tolower(hh$Gear) %in% tolower(gear)), ]

  # Load species name key
  data( aphia, package="icesDatras" )
  #on.exit( remove("aphia"), add=TRUE )
  #worms <- icesVocab::getCodeList("SpecWoRMS")
  unique_species = data.frame('code'=na.omit(unique(hl$Valid_Aphia)))
  unique_species$genus_species = with(unique_species, aphia[ match(unique_species$code,aphia$aphia_code), 'species'])
  unique_species$num_weight_at_age_measurements = with(unique_species, NA)
  #unique_species$genus_species = with(unique_species, aphia[ match(unique_species$code,worms$Key), 'species'])

  # Add uniqueID
  hl$HaulID <- with(hl, paste(Year, Quarter, Ship, StNo, HaulNo, sep = "."))
  ca$HaulID <- with(ca, paste(Year, Quarter, Ship, StNo, HaulNo, sep = "."))
  hh$HaulID <- with(hh, paste(Year, Quarter, Ship, StNo, HaulNo, sep = "."))
  key <- c("Year", "Quarter", "Ship", "StNo", "HaulNo", "HaulID")

  # Add Lat/Lon
  hl = cbind(hl, hh[match(hl$HaulID,hh$HaulID), c("ShootLat","ShootLong","DataType","HaulDur")])

  # Add length units
  # from `DATRAS::addExtraVariables` here:  https://www.rforge.net/DATRAS/svn.html
  LngtCode2cm <- c(. = 0.1, "0" = 0.1, "1" = 1, "2" = 1, "5" = 1)
  hl$Length_units <- with(hl, sapply(hl$LngtCode, FUN=function(char){LngtCode2cm[match(char,names(LngtCode2cm))]}) )
  ca$Length_units <- with(ca, sapply(ca$LngtCode, FUN=function(char){LngtCode2cm[match(char,names(LngtCode2cm))]}) )

  # Loop though species and predict weight where missing
  hl$predicted_weight = with( hl, NA )
  # based on `DATRAS::addWeightByHaul`
  # BUT I don't predict to middle of LngtClass in `hl`
  for( pI in 1:nrow(unique_species)){
    # Observations with sufficient data for weight-length key
    DB = na.omit(ca[which(ca$Valid_Aphia==unique_species[pI,'code']),c('IndWgt','LngtClass','Length_units')])
    DB = DB[ which(DB[,'IndWgt']>0), ]
    unique_species[pI,'num_weight_at_age_measurements'] = nrow(DB)

    # Only predict if data are sufficient
    if( nrow(DB)>10 ){
      # Fit weight-length key
      Lm = lm(log(IndWgt) ~ I(log(LngtClass*Length_units)), data=DB )
      if( verbose==TRUE ){
        plot( x=log(DB$LngtClass*DB$Length_units), y=log(DB$IndWgt) )
        abline( a=Lm$coef[1], b=Lm$coef[2] )
      }

      # Only predict if relationship makes sense
      if( Lm$coef['I(log(LngtClass * Length_units))']<4 | Lm$coef['I(log(LngtClass * Length_units))']>2.5 ){
        # Use weight-length key to predict missing weights
        hl$predicted_weight[which(hl$Valid_Aphia==unique_species[pI,'code'])] = exp(Lm$coef[1] + Lm$coef[2]*log(hl$LngtClass*hl$Length_units)[which(hl$Valid_Aphia==unique_species[pI,'code'])] )
      }else{
        warning("Possible problem with ", unique_species[pI,'genus_species'])
      }
    }
  }

  # Decide on set of species
  if( is.numeric(species_set) ){
    Num_occur = tapply( ifelse(hl[,'HLNoAtLngt']>0,1,0), INDEX=hl[,'Valid_Aphia'], FUN=sum, na.rm=TRUE )
    species_set = names(sort(Num_occur, decreasing=TRUE)[ 1:min(species_set,length(Num_occur)) ])
    species_set = data.frame( 'aphia'=species_set, 'species'=aphia[ match(species_set,aphia$aphia_code), 'species'])
  }else{
    species_set = data.frame( 'aphia'=aphia[match(species_set,aphia$species),'aphia_code'], 'species'=species_set)
  }

  # Add expansion factors
  # TotalNo = sum( HLNoAtLngt*SubFactor )  # http://datras.ices.dk/Data_products/FieldDescription.aspx?Fields=HLNoAtLngt&SurveyID=2341
  # TotalNo depends upon DataType (C is scaled to 1-hour tows, whereas other codes depend upon SubFactor)  # Pers. Comm. from Casper Berg
  hl$expansion_factor = with( hl, ifelse(hl[,'DataType']=="C", hl[,'HaulDur']/60, hl[,'SubFactor']) )
  hl$expanded_number = with( hl, hl[,'HLNoAtLngt']*hl[,'expansion_factor'] )    #
  hl$expanded_weight = with( hl, hl[,'predicted_weight']*hl[,'HLNoAtLngt']*hl[,'expansion_factor'] )    #

  # Add missing zeros, and compress accross length bins
  DF = FishData::add_missing_zeros( data_frame=hl, unique_sample_ID_colname="HaulID", sample_colname="expanded_weight", species_subset=species_set[,'aphia'], species_colname="Valid_Aphia", Method="Fast", if_multiple_records="Combine", error_tol=1e-2, verbose=verbose)
  DF2 = FishData::add_missing_zeros( data_frame=hl, unique_sample_ID_colname="HaulID", sample_colname="expanded_number", species_subset=species_set[,'aphia'], species_colname="Valid_Aphia", Method="Fast", if_multiple_records="Combine", error_tol=1e-2, verbose=verbose)
  DF$species = with( DF, species_set[match(x=DF[,'Valid_Aphia'], table=species_set[,'aphia']), 'species'])
  DF2$species = with( DF2, species_set[match(x=DF2[,'Valid_Aphia'], table=species_set[,'aphia']), 'species'])

  # Check for missing data
  for( pI in 1:nrow(species_set)){
    if( all(is.na(hl[ which( hl$Valid_Aphia==species_set[pI,'aphia'] ), 'expanded_weight'])) ){
      DF[ which(DF[,'Valid_Aphia']==species_set[pI,'aphia']), 'expanded_weight'] = NA
    }
  }

  # Replace with known catch weight when possible
  Match = match( paste(DF$HaulID,DF$Valid_Aphia,sep="_"), paste(hl$HaulID,hl$Valid_Aphia,sep="_") )
  DF$observed_weight = with( DF, hl[Match,'CatCatchWgt'] )
  DF$catch_weight = with( DF, ifelse(is.na(DF$observed_weight),DF$expanded_weight,DF$observed_weight) )

  # Sub in
  DF[,'TotalNo'] = DF2[,'TotalNo'] = with( DF, hl[Match,'TotalNo'] )

  # Return hl for selected species
  Return = list("hl"=hl, "DF"=DF, "DF2"=DF2, "species_set"=species_set)
  return(Return)
}

```


Sort of works, but 

```{r, eval = F}

```

# paper update

basically what you have, but adding in 


- JUST the economic data

Macro level effects: sum things up at the regional scale, e.g. GOA, and just say, does the total effort in a given region say something about the total biomass in a region. 

i.e. is there a clear signal that yes, there are more fish in the eastern bering sea

# check out

https://podaac.jpl.nasa.gov/OceanEvents/2018_02_01_Modelling_marine_fish_species_spatiotemporal_distributions

# 2018 PhD Symposium Planning

So what's youre messaging here

Your introduction is pretty good, so there are a few areas that you need to flesh things out

A moment on methods, basically after the ganets movie. 

Could do a simple plot of total abundace vs total effort at a variety of spatial scales

We want to know how well we can predict. To do that, we're going to compare the ability of different models to predict "out of sample"

Show the google art thing as an example and inside and outside of sample

So that's how we're going to judge, what are we going to judge?

Machine learning vs structural 

Observed vs Predicted Plots 

Case studies: EBS predictions in space and time, which one works?

SHow that it works pretty damn well withing sample region

But now, what happens if we go waaaay out of sample? DOes the structural model work better?

Big fat nope, but neither does machine learning really

So where does that leave us?

Structural vs machine learning approahes: people are predictable but in a very complicated way

So what?

Augmenting existing survey techniques?

If we train big can we predict big?

Support slides

What variables are important

A few other regions/models

Spatial residuals 

Example of aggregation 

Think about aggregating up your predictions: instead of predicting that small place sum them up and look at observed vs predicted at different spatial scales


```{r, eval = F}

agg_results <- uc_skynet_data %>% 
  group_by(survey, year) %>% 
  summarise(engine_hours = sum(total_engine_hours, na.rm = T),
            abundance = sum(density, na.rm = T)
            )


uc_skynet_data %>% 
  ggplot(aes(log(total_engine_hours), log(density))) + 
  geom_point()
  
agg_results %>% 
  ggplot(aes(log(engine_hours), log(abundance), color = survey)) + 
  geom_point() #+ 
  # scale_x_log10() + 
  # scale_y_log10()
  # 
```

# Estimated abundance vs
# Doing the aggregation test right

So what's it going to take to get a function that takes the data right at the model building phase and adjusts the resolution?

I think this needs to come in before generate_test_training

data_sources is a container for the highest resolution data you've got, let's just takea  look at skynet. 

So what would it takt to make this happen. The real challenge is in the fact that the survey data and gfw are on different "resolutions", or rather that the abundance data doesn't have a resolution per say, where global fishing watch does. 

So, the result is that there are multiple GFW entries for every abundance index entry. 

So, this gets back to your problem before: you can't just round lat/lon and aggregate up, since you have duplicate entries of biomass. So, if there are 100 GFW entries in an area, you're going to inflate the biomass artifically. 

So, you basically need to separate out the aggregation process for the gfw and survey data. 

The simplest solution would be going back to the actual nearest-knot exercise really. 

For each resolution, you'd aggregate up the GFW, aggregate up the knots, and then do nearest neighbor again. 

On the GFW side this is pretty easy. But, it's a little trickier to think of on the survey side. 

A hackier solution that would fit much better into your exisiting framework. If you assume each knot as a unique biomass that is different than very other knot, then when you aggregate up, instead of sum(biomass), you could just do sum(unique(biomass)). Let's explore this

```{r}

data <- data_sources$data[data_sources$data_subset == "skynet"][[1]]

 data %>% 
   group_by(year, survey) %>% 
   summarise(n_maybe_knots = n_distinct(density),
             n_knots = n_distinct(knot))

```

booya, that checks out, and you could put a warning in there if that assumption is violated.  though also suggests part of the damn fitting problem, in most years you barely have any knots...  Might be worth dropping out wghhl in general, damn. That's because of the nearest knot behavior; if all the fishing is around one knot, then your effective sample size will be 1. d'oh. Can you add the nearest knot back in? Seems like the unique thing is working, given that there should be at max 200 for most of these. 

Though also seems worthwhile to investigate running VAST with mesh size instead of knots? Seems like this is a Jim question. Let's not fuck with this for now. 

So now, what would rounding look like.... 

FUUUUUUCK. That won't work, since it's going to inflate some points. You'd need to do something like round some things up and some things down... 


OK, what would it take to just make a few different resolution data, do the clipping, joining, etc.... 

Would be easy to run that part, but the problem would be putting alllll the other processing steps inside of a function. 

THe other problem is that your logic so far has basically been that the GFW data is at a finer resolution than the survey data. So, it makes sense to put the GFW data "inside" of a knot. 

But, once you start aggregating up, this gets tought, since now you'd actually be snapping knots to nearest resolution point...  FOOOOOOOK. 

OK. stop. this is getting crazy. 

What's the key thing you're trying to estiamte here. Ideally, RMSE by "resolution". I think the problem here is that you haven't done a good job of defining "resolution", since it's not at all obvious.  given the knots-to-gfw problem. 


The simplest solution would be to keep the model fit as it is, and come up with a clever way to "aggregate up"

So what does "aggregate up" mean in this context... The problem you're having now is that you can't just sum up the observed and predicted at different resolutions 

Let's work at the map level, and just think abotu creating coarser and coarser grids from the map, ideally actually using maps to generate centers from equally sized plots. 

Now, snap the data to that grid. 

For the survey data, sum up all the unique observed biomasses

For the GFW predictions, take the average at each knot, then sum across knots. AHA! That might be the key. THe idea is that at each knot, the fleet shold be sampling from one biomass. Once you have the biomass at that knot, then you can sum up. 

This is making more sense. 

There's still a problem here though that you might have way more knots in some areas than others, and so when you sum up, the performance is going to be better.... 

You need to check whether the knots are estimating density or biomass. If it's density... then to do the density across the new area you'd average. If it's biomass then you sum. And hopefully if it's biomass, then the closer together, the smaller the biomass in general... Could work



```{r}

degree_resolution <- 0.5

rounded_data <- data %>% 
  ungroup() %>% 
  mutate(noise = sample(c(-.1,.1),nrow(.), replace = T)) %>% 
  mutate(new_lon = round((rounded_lon + noise) * (1 / degree_resolution)) / (1 / degree_resolution)) %>% 
  select(rounded_lon, new_lon, knot) %>% 
  group_by(new_lon) %>% 
  summarise(nk = n_distinct(knot)) %>% 
  ungroup() %>% 
  mutate(decimal = new_lon - floor(new_lon)) %>% 
  group_by(decimal) %>% 
  summarise(tnk = sum(nk))

```

OK I think I solved it. The key thing is that from the perspective of "truth", the knots are as high resolution as it gets. So, stop trying to go finer than that, or different than that. 

So, to aggregate up, take your current model predictions, and calcualte the expected density/biomass per knot as the average of all the predictions at each knot. And not you're in business. You can then aggregate the knots up either by a spatial grid, or by neareset neighbors, or whatever. So instead of all the knots, you group each knot with it's nearest neighbor and try that way, and so on and so forth. The only problem with this is how do you scale up when you don't know the knots, somewhere else for example. In that sense, you'd want to scale up by area..... CRAPPPPPP. 

From the model fitting perspective, the easiest thing to do would be to replicate the same process. You're going to lose a ton of data, but maybe that's not the worst thing in the world. 

Though that is the appeal of density as the dependent variable, it's easier to scale up. Part of the problem right now is that what you are predicting is the average density across an area. So, as a core observed vs. predicted, great: that's saying that the "density", in whatever the fugde units this stuff is in kg/km2?, at that specific spot. So now, suppose that you only had two spots: what's the observed vs predicted then? The true density over those two spots would be the average of them. So, the hypothesis then is as you scale up, the places with more effort should have higher average density of fish. On the totals side, you just sum up, but that becomes a bit more of a problem from the going from GFW to abundance side, since now each GFW estimate is a sample from the abundance of an unknown area, so it's hard to know what you'd average (samples from the mean) vs what you'd sum (biomass across different area). Since, in reality, you're not going to know what "knot" a given observation should correspond to. 

So, from the GFW side then, it is just a question of averaging up, which is basically what you have right now. As you aggregate up, this predicts the average density per unit area in that aggregated area. 

The slightly harder part then is the fish survey side of things. The hackiest approach would be to overlay the grid from the above example on top of the knots, and assign each knot to a grid. The problem here would be split knots. 

Really, would be better if you could just run the damn this as a grid. See if you can do that. Right now seems to be crashing errrrrything, and same crash as happened with the other example. 

So, the hacky solution would be to rasterize the knot data to the resolution that you want. 

So, I think this is a job for sf. Create a raster with ideally constant area but if not that constant degrees. Then snap both the GFW data and fish data to that, and move on up. 

So what about the simplest thing you could do, which is aggregate up by survey. For observed abundance, you could just sum the biomasses, or average the densities over say the survey area. But if you're averaging the densities they'd need to be weighted by the knot area, which you should have after this run. 

For the effort, it's just the sum of effort. So, we'll play with that once you get this run done. 

So, what did you learn from all this? You reminded yourself that you are working with densities. The advantage there is that it makes aggregating and interpreting the predictions much clearer. If you have 10 GFW predictions in one knot, and the "true" is 100 *biomass*, then the expected biomass would be the mean of the 10 predictions, not the sum. The biomass would be the expected density * the area. So, when you're aggregating up, you're taking means, not sums. 

If you predicted in units of biomass, it's confusing, since that would be predicting the biomass within that knot, but for prediction, you won't know what knot you're referring to, so you wouldn't know what things you should average (things within a knot) and which things you should add (things across knots). So, I think you have to work in density units. That's a win for understanding. 

So, as you aggregate up in area from the GFW perspective, it's just a mean. 

Similar from the survey data, but the problem here is that it's not trivial to think of how to aggregate up the knots. What you'd want would be a weighted average density, where the weighting is the relative area of each knot within the aggregated grid. 




# density vs abundance?

At the moment you are fitting everything to density. Wouls abundance make more sense? On one hand, might explain some odd behavior where a large are with the same density could come up as a "bad" place to be, even if there are actually lots of fish in there, whereas the model wants to put lots of people in an area with high density but low abundance. 

From the structural side, the density thing might play a role in the poor fit. Though nope, density is right for the structural model, since the model is based on expected CPUE. So broadly just seems like this is a good opportunity to spit out both pieces of information along with the area and make your call from there as to what the best "depvar" is


## Snapping to different resolution grids

Example code below.

```{r}
library(tidyverse)
library(sf)
knots <- vast_fish %>%
  select(survey, vasterized_data) %>%
  mutate(knots = map(vasterized_data, c('spatial_list', 'loc_x_lat_long'))) %>%
  select(-vasterized_data) %>%
  unnest() %>%
  mutate(recenter_lon = ifelse(approx_long < 0, 180 + (180 - abs(approx_long)), approx_long))
  
  pacific_map <- global_map %>%
  as("Spatial") %>%
  maptools::nowrapRecenter() %>%
  sf::st_as_sf()
  
  
  map_knots <-  knots %>%
  dplyr::mutate(geometry = purrr::map2(recenter_lon, approx_lat, ~ sf::st_point(x = c(.x, .y), dim = 'XY'))) %>%
  ungroup() %>%
  mutate(geometry = sf::st_sfc(geometry, crs =
  "+proj=longlat +datum=WGS84 +no_defs")) %>%
  sf::st_sf()
  
  test <- st_make_grid(map_knots, cellsize = 2, what = "centers") %>% 
    sf::st_sf()
  
  
  bbox <- sf::st_bbox(map_knots)
  
 survey_names <- tribble(
  ~ survey,
  ~ name,
  'ebsbts',
  'Eastern Bering Sea',
  'aibts',
  'Aleutian Islands',
  'goabts',
  'Gulf of Alaska',
  'wcgbts',
  'West Coast',
  'wcghl',
  "SB Channel"
)
  
  knot_map <- map_knots %>%
    left_join(survey_names, by = "survey") %>% 
  ggplot() +
  geom_sf(data = pacific_map, fill = 'grey60') +
  geom_sf(data = test) +
  geom_sf(aes(color = name), size = 1, alpha = 0.5) +
  coord_sf(xlim = c(bbox['xmin'], bbox['xmax']),
  ylim = c(bbox['ymin'], bbox['ymax'])) +
  theme_light(base_size = 18) +
  scale_color_viridis_d(name = element_blank())
```


SO what's the psuedocode for the process you're envisioning. 

1. Write a function called create_grid that spits back the lat/lon of the centroids of a raster of a given resolution

2. Find the nearest neighbor of each point in gfw/survey and assign that point to that new centroid

Since the new points are spaced X resolution apart, but aren't always in units of the spacing, I don't think you have a rounding problem, but you should check that the picture just evolves into a coarser resolution picture over the coarse of this thing 

3. Group by year, point, and summarize mean (with option for area weighting) density or sum biomass at each new point

4. Return the new new frame 

5. Do that for the observed and predicted data, and calcualte RMSE as a function of resolution 


So that's the idea for that. 

You can then use the same function to calculate total effort / total abundance at different spatial scales and compare the naive estimate. 

Let's play with this idea. 


```{r}

tester <- skynet_models %>% 
  filter(model %in% c('ranger'), test_set == 'random', train_set == 'random',
                      data_subset == "skynet", dep_var == "log_density") %>% {
                        .$training_data[[1]]
                      }

test_resolution <- data_frame(data = list(skynet_data), resolution = seq(25,200, by = 25)) %>% 
  mutate(new_grid  = map2(data,  resolution, create_grid, lon_name = rounded_lon, lat_name = rounded_lat))


```

Sweeeeeeeeet. Now, you need a function called `snap_to_grid` that takes your data and calculates the new values at the grid

```{r}

test_resolution <-  test_resolution %>% 
  mutate(new_data = map2(data, new_grid, snap_to_grid,
                         old_lon_name = rounded_lon,
                         old_lat_name = rounded_lat, 
                         new_lon_name = lon, 
                         new_lat_name = lat))


check <- test_resolution %>% 
  select(resolution, new_data) %>% 
  unnest()

library(gganimate)

check_plot <- check %>% 
  filter(year == 2012) %>% 
  ggplot(aes(lon, lat, color = log(agg_total_hours), frame = resolution )) + 
  geom_point() + 
  scale_color_viridis() + 
  labs(subtitle = "km^2 resolution")

check_plot2 <- check %>% 
  group_by(resolution) %>% 
  ggplot(aes(log(agg_total_hours), log(agg_total_biomass), color = year, frame = resolution )) + 
  geom_point() + 
  geom_smooth(method = "lm") +
  scale_color_viridis() + 
  labs(subtitle = "km^2 resolution")

gganimate(check_plot2)

gganimate(check_plot, "booya.gif")

r2foo <- function(data){

  mod <-  lm(log(agg_mean_density) ~ log(agg_total_effort), data = data %>% filter(agg_total_effort > 0))
 
 r2 <- modelr::rsquare(mod, data = data %>% filter(agg_total_effort > 0))
 
 # rmse <- modelr::rmse(mod,  data = data %>% filter(agg_total_effort > 0))
}

check2 <- test_resolution %>% 
  select(resolution, new_data) %>% 
  mutate(r2 = map_dbl(new_data, r2foo))

check2 %>% 
  mutate(nobs = map_dbl(new_data, nrow)) %>% 
  ggplot(aes(resolution, r2, color = nobs)) + 
  geom_point() + 
  ggrepel::geom_label_repel(aes(label = nobs))

```


# 2018-02-22 GFW Update with Tony


Bayesian updating of global temperature profiles and global fleet distribution. 

If you had competing climate models that would predict different outcomes, could you use the signal from GFW to update your prior over which state of the world you're in for model fitting

Why would in activity = overcapacity? Easily just seasons, either biological or market based

Ren's work: Why the hell is skipjack going down under BAU, fishery is fine? Also, how do you deal with the fact that the estiamte of status ignores that IUU, so you're gettting a free lunch by subtracting catch that wasn't incorporated into the stock assessment in the first place?  

You need to get this team working and talking with someone about why tuna are where they are. THere's a risk in modeling tunas and MPAs and thinking of them as sources from a recruitment perspective. 

Think about centering and scaling by year to get around changes in effort as a function in improvements in GFW data

# 2018-02-22 last idea push 

OK, it's time to stop tinkering and provide some results in draft form at least. 

Here are the last things I want to add. 

  - I want to add an option for a dependent variable that is last years biomass
  
  - ... that's about it that I can think of on that end. 
  

The more complicated thing is the resolution question. You have a choice between upscaling a model fit at a fine resolution, or downscaling the resolution the model is fit to. Upscaling the model fits is easy. downscaling the model resolution is a bit harder. 

Actually not that bad, easiest solution will be to slot in at generate test_train and nest that in another function that produces data at different resolutions, say raw, 50, and 100 for now?

Whoops no nevermind, that goes in "data_sources", should actually slot in really easily that way.... Just need to automate generation of the lower/higher res data


# 2018-02-23

Partitiontime

you need to be careful about your partitiontime statements 

# 2019-02-24 - Figure Sketches

```{r}
library(gbm)
library(randomForest)
library(stringr)
library(modelr)
library(broom)
library(hrbrthemes)
library(viridis)
library(sf)
library(ggmap)
library(caret)
library(extrafont)
library(tidyverse)
demons::load_functions(func_dir = here::here("functions"))

# run options -------------------------------------------------------------

run_name <- 'v2.1'

run_dir <- here::here('results',run_name)

fig_theme <- theme_ipsum(base_size = 14, axis_title_size = 18)

theme_set(fig_theme)

load(here::here("results", run_name,'skynet_data.Rdata'))

load(here::here("results", run_name,'processed_skynet_models.Rdata'))

load(here::here("data","vast_fish.Rdata"))

load(file = here::here("data","global_map.Rdata"))

load(file = here::here('data','fish_prices.Rdata'))

# load(file = here::here('results',run_name,'gfw_data.Rdata'))

map_size <- 1
```

## Resolution Figures

Explore model performance under different metrics for different resolutions

```{r}

r2_comparison_plot <- skynet_models %>% 
  filter(dep_var == "log_density") %>% 
  mutate(test_name = glue::glue("trained: {train_set} | tested: {test_set}"),
         model = forcats::fct_reorder(model, desc(r2))) %>%  
  ggplot(aes(model, r2, fill = test_name)) + 
    geom_point(shape = 21, size = 2) + 
  theme(axis.text.x = element_text(angle = 45, vjust = .1),
        text = element_text(size = 8),
        strip.text = element_text(size = 6)) + 
  facet_grid(dep_var ~ data_subset)


r2_comparison_plot

```

OK, looks like gbm is a solid place to start exploting, though also apparent that for the out-of-region sampling, there's actualyl a small boost in the structural department, which is pretty interesting. 

Let's take a look at upscaling the results form the finest resolution model 


```{r}

upscale_foo <- function(data){

test_resolution <- data_frame(data = list(data), resolution = seq(25,200, by = 25)) %>% 
  mutate(new_grid  = map2(data,  resolution, create_grid, lon_name = rounded_lon, lat_name = rounded_lat))

test_resolution <-  test_resolution %>% 
  mutate(new_data = map2(data, new_grid, snap_to_grid,
                         old_lon_name = rounded_lon,
                         old_lat_name = rounded_lat, 
                         new_lon_name = lon, 
                         new_lat_name = lat))

}



resolution_test <- skynet_models %>% 
  filter(data_subset == "skynet", dep_var == "log_density", weight_surveys == T) %>% 
  mutate(upscaled_data = map(test_data, upscale_foo))

resolution_test <- resolution_test %>% 
  select(model, test_set, train_set, upscaled_data) %>% 
  unnest()

resolution_test <- resolution_test %>% 
  select(model, test_set, train_set, resolution, new_data) %>% 
  unnest()

resolution_summary <- resolution_test %>% 
  nest(-model, -test_set, -train_set,-resolution) %>% 
  mutate(r2 = map_dbl(data,~yardstick::rsq(.x, agg_mean_log_density, agg_pred)),
         rmse = map_dbl(data,~yardstick::rmse(.x, agg_mean_log_density, agg_pred)))

resolution_summary %>% 
  ggplot(aes(resolution, r2, color = model)) + 
  geom_line() + 
  geom_point() + 
  facet_wrap(~test_set, scales = "free_y")

```

Huh, well that is shockingly uninformative. trends in some areas, not in others, some models better sometimes, sigh. 

## Aggregate trends

Compare true vs predicted CPUE trends

```{r}

true_index <- vast_fish %>% 
  mutate(time_index = map(vasterized_data,"time_index")) %>% 
  select(-data,-vasterized_data) %>% 
  unnest() %>% 
  group_by(survey,survey_region,Year) %>% 
  summarise(total_biomass = sum(abundance))

true_index %>% 
  ungroup() %>% 
  filter(Year < 2017) %>% 
  ggplot(aes(Year,total_biomass, color = survey)) + 
  geom_line() + 
  geom_point() + 
  facet_wrap(~survey, scales = "free")

```

Would be good to get a longer time series and to keep the interpolated years

So now, what's the right way to aggregate up your predictions. 

What you have is mean predicted density, in units of something per something, out of vast. 

So step one, what the fudge are these in units of? kg per unit area... what the hell is area in original units of? supposedly km^2^. So, at the models rawest level you're estimating kg/km^2^ of fish. 

So, now at the regional scale, you want not the mean density across the region, but the total index of abundance for the region. 

So, the simplest thing then would be to upscale to a resolution, like 25km^2, take the mean density across that region, multiply by 25 (or whatever the resolution is), and then sum across the area. 

```{r}

predicted_abundance <- resolution_test %>% 
  filter(resolution == 25) %>% 
  group_by(model, test_set, train_set, approx_survey, year) %>% 
  summarise(abundance = sum(exp(agg_pred) * resolution),
            true_abundance = sum(exp(agg_mean_log_density) * resolution))
  

check <- predicted_abundance %>% 
  filter(train_set == "year_leq_than_2014")
  
predicted_abundance %>% 
  ggplot(aes(year, abundance, color = approx_survey)) + 
  geom_line() + 
  geom_point() + 
  facet_grid(test_set ~model) + 
  theme_minimal()


```

That's just annoying. Let's focus on the EBSBTS for now to make sure that you're somewhere in the ballpark. 

Let's take a look at the OOB training estiamte performance within region. 

```{r}

abundance <- predicted_abundance %>%
  ungroup() %>%
  # filter(train_set == "random", model != "hours") %>%
  select(model,
  train_set,
  test_set,
  year,
  abundance,
  true_abundance,
  approx_survey) %>%
  group_by(model, train_set, approx_survey) %>%
  mutate(
  scaled_abundance = (abundance - mean(abundance)) / (sd(abundance)),
  scaled_true_abundance = (true_abundance - mean(true_abundance)) / (sd(true_abundance))
  ) %>%
  ungroup()


ebsbts_true_abundance <- true_index %>% 
  ungroup() %>% 
  filter(survey == "ebsbts", Year < 2017) %>% 
  mutate(model = "observed") %>% 
  rename(year = Year, abundance = total_biomass) %>% 
  select(model, year, abundance)

```

Machine learning only


```{r}

abundance %>% 
  filter(model %in% c("ranger","gbm"), train_set == "random") %>% 
  ggplot(aes(year, abundance, color = model)) + 
  geom_line(aes(year, true_abundance), color = "black", linetype = 2, size = 1.25) +
  geom_line() + 
  geom_point() + 
  facet_wrap(~approx_survey, scales = "free_y") + 
  theme_minimal()


```

What aboure relative trends?

```{r}


abundance %>% 
  filter(test_set == "random") %>%
  ggplot(aes(year, scaled_abundance, color = model)) + 
  geom_line(aes(year, scaled_true_abundance), color = "black", linetype = 2, size = 1.25) +
  geom_line() + 
  geom_point() + 
  facet_wrap(~approx_survey, scales = "free_y") + 
  theme_minimal()



```

Well that is fascinating, it has a really hard time with the absolute abundance but not a bad time at all with the general trend, this might be a thing worth exploring more. 

Let's look at the performance now out-of-time


```{r}

abundance %>% 
  filter(model %in% c("ranger","gbm"), train_set == "year_leq_than_2014") %>%
  ggplot(aes(year, abundance, color = model)) + 
  geom_line(aes(year, true_abundance), color = "black", linetype = 2, size = 1.25) +
  geom_point(aes(year, true_abundance), color = "black", shape = 17, size = 1.25) +
  geom_line() + 
  geom_point() + 
  facet_wrap(~approx_survey, scales = "free_y") + 
  theme_minimal()



```


And out of space?


```{r}

# abundance %>% 
#   filter(model %in% c("ranger","gbm"), train_set == "not_west_coast") %>%
#   ggplot(aes(year, abundance, color = model)) + 
#   geom_line(aes(year, true_abundance), color = "black", linetype = 2, size = 1.25) +
#   geom_point(aes(year, true_abundance), color = "black", shape = 17, size = 1.25) +
#   geom_line() + 
#   geom_point() + 
#   facet_wrap(~approx_survey, scales = "free_y") + 
#   theme_minimal()


```

```{r}

# abundance %>% 
#   filter( train_set == "not_west_coast") %>%
#   ggplot(aes(year, scaled_abundance, color = model)) + 
#   geom_line(aes(year, scaled_true_abundance), color = "black", linetype = 2, size = 1.25) +
#   geom_line() + 
#   geom_point() + 
#   facet_wrap(~approx_survey, scales = "free_y") + 
#   theme_minimal()



```

```{r}

abundance %>% 
  filter( train_set == "year_leq_than_2014") %>%
  ggplot(aes(year, scaled_abundance, color = model)) + 
  geom_line(aes(year, scaled_true_abundance), color = "black", linetype = 2, size = 1.25) +
  geom_line() + 
  geom_point() + 
  facet_wrap(~approx_survey, scales = "free_y") + 
  theme_minimal()



```

UMMMMMM HOLY SHIT THAT WORKS FOR THE TRENDS. 


```{r}

abundance %>% 
  filter(train_set == "west_coast") %>%
  ggplot(aes(year, scaled_abundance, color = model)) + 
  geom_line(aes(year, scaled_true_abundance), color = "black", linetype = 2, size = 1.25) +
  geom_line() + 
  geom_point() + 
  facet_wrap(~approx_survey, scales = "free_y") + 
  theme_minimal()

```

  
```{r}

abundance %>% 
  filter(train_set == "ebsbts") %>%
  ggplot(aes(year, scaled_abundance, color = model)) + 
  geom_line(aes(year, scaled_true_abundance), color = "black", linetype = 2, size = 1.25) +
  geom_line() + 
  geom_point() + 
  facet_wrap(~approx_survey, scales = "free_y") + 
  theme_minimal()

``` 

Let's try this map idea. 


```{r}

  pacific_map <- global_map %>%
  as("Spatial") %>%
  maptools::nowrapRecenter() %>%
  sf::st_as_sf()


 skynet_box <- skynet_data %>% 
    group_by(knot, year, rounded_lat, rounded_lon) %>% 
    summarise(total_density = sum(density)) %>% 
    ungroup() %>% 
      mutate(recenter_lon = ifelse(rounded_lon < 0, 180 + (180 - abs(rounded_lon)), rounded_lon)) 

  
  skynet_map <-  skynet_box %>%
  dplyr::mutate(geometry = purrr::map2(recenter_lon, rounded_lat, ~ sf::st_point(x = c(.x, .y), dim = 'XY'))) %>%
  ungroup() %>%
  mutate(geometry = sf::st_sfc(geometry, crs =
                                 "+proj=longlat +datum=WGS84 +no_defs")) %>%
  sf::st_sf()

bbox <- sf::st_bbox(skynet_map)


base_map <- pacific_map %>% 
ggplot() + 
  geom_sf() + 
  coord_sf(xlim = c(bbox['xmin'], bbox['xmax']),
  ylim = c(bbox['ymin']*.75, bbox['ymax'] * 1.1), expand = F) + 
theme_ipsum() +
  theme(panel.grid = element_blank(),
        text = element_blank(),
         plot.margin = unit(c(0,1,0,1),"cm"))

survey_positions <- skynet_data %>% 
  group_by(survey) %>% 
  summarise(mean_lat = median(rounded_lat),
            mean_lon = median(rounded_lat)) %>% 
  ungroup() %>% 
  arrange(desc(mean_lat)) 

random_gbm_abundance <- abundance %>% 
  filter(train_set == "random", model == "gbm") %>% 
  group_by(year, approx_survey) %>% 
  gather(abundance_source, abundance, contains("abundance")) %>% 
  left_join(survey_positions, by = c("approx_survey" = "survey"))

trend_plot <- random_gbm_abundance %>% 
  ungroup() %>% 
  filter(str_detect(abundance_source, "scaled"),
         approx_survey != "goabts") %>% 
  ggplot(aes(year + -mean_lon * 0.3, 2.5*abundance + mean_lat, color = approx_survey, linetype = abundance_source)) + 
  geom_smooth(show.legend = F) + 
  geom_point(show.legend = F, size = 1.5) + 
  theme_ipsum() +
    theme(panel.grid = element_blank(),
        text = element_blank(),
        plot.margin = unit(c(0,0,0,0),"cm")) 

  trend_map <-  ggdraw(base_map) + 
  draw_plot(trend_plot, y = -.01,width = 0.7, height = 0.9)

ggsave("test.png", trend_map, width = 8, height = 6)



```





# 2018-02-27 - FOOOOOOK

Of course the new VAST data broke everything. 

fuck. 

```{r}

library(gbm)
library(randomForest)
library(stringr)
library(modelr)
library(broom)
library(hrbrthemes)
library(viridis)
library(sf)
library(ggmap)
library(caret)
library(extrafont)
library(tidyverse)
demons::load_functions(func_dir = here::here("functions"))

# run options -------------------------------------------------------------

run_name <- 'v2.0'

run_dir <- here::here('results',run_name)

fig_theme <- theme_ipsum(base_size = 14, axis_title_size = 18)

theme_set(fig_theme)

load(here::here("results", run_name,'skynet_data.Rdata'))

load(here::here("results", run_name,'processed_skynet_models.Rdata'))

load(here::here("data","vast_fish.Rdata"))

load(file = here::here("data","global_map.Rdata"))

load(file = here::here('data','fish_prices.Rdata'))

load(file = here::here('results',run_name,'gfw_data.Rdata'))

  pacific_map <- global_map %>%
  as("Spatial") %>%
  maptools::nowrapRecenter() %>%
  sf::st_as_sf()
  

map_size <- 1


spatial_check <- vast_fish %>% 
  mutate(thing = map(vasterized_data, "spatial_densities")) %>% 
  select(survey, survey_region, knots, thing)

mapfoo <- function(data){
  
  data <- data %>% 
    group_by(knot, year, approx_long, approx_lat) %>% 
    summarise(total_density = sum(density)) %>% 
    ungroup() %>% 
      mutate(recenter_lon = ifelse(approx_long < 0, 180 + (180 - abs(approx_long)), approx_long)) 

  
  data_map <-  data %>%
  dplyr::mutate(geometry = purrr::map2(recenter_lon, approx_lat, ~ sf::st_point(x = c(.x, .y), dim = 'XY'))) %>%
  ungroup() %>%
  mutate(geometry = sf::st_sfc(geometry, crs =
                                 "+proj=longlat +datum=WGS84 +no_defs")) %>%
  sf::st_sf()

years <- expand.grid(id = 1:nrow(pacific_map), year = unique(data_map$year))

yearly_map <- pacific_map %>% 
  mutate(id = 1:nrow(.)) %>% 
  right_join(years, by = "id")
  
bbox <- sf::st_bbox(data_map)


data_mappy <- data_map %>%
  ggplot() +
  geom_sf(aes(color = log(total_density)), show.legend = F) +
  geom_sf(data = yearly_map, fill = 'grey60') +
  scale_color_viridis() +
  coord_sf(xlim = c(bbox['xmin'], bbox['xmax']),
  ylim = c(bbox['ymin'], bbox['ymax'])) +
  labs(x = "Longitude", y = "Latitude") +
  facet_wrap( ~ year) +
  theme_ipsum(
  base_size = 18,
  axis_title_size = 20,
  subtitle_size = 18,
  axis_text_size = 18,
  plot_title_size = 20,
  strip_text_size = 20
  ) +
  theme(plot.margin = unit(c(.25, .25, .25, .25), "in"))
  
}

spatial_check <- spatial_check %>% 
  mutate(fishmap = map(thing, mapfoo))

spatial_check$fishmap[[5]] + theme_minimal()

```

Aha, that could be it, the interpolated years have very weird behavior. So as an initial point fitting the models without those 


# 2018-02-28 - we're alive again, final model steps

There are two more things that I'd like to try. 

First, you've got a bit of a problem by region, where you have waaaay more samples in the alaska region, so of course the model is going to do best up there. 

```{r}
skynet_data %>% 
  group_by(survey) %>% 
  count() %>% 
  arrange(desc(n))
```

So, I'd like to downsample the data a bit and see how that works, or maybe upsample? Let's check out applied predictive modeling and see what it says. 

Second, for the final map, you be using "all" the data. So tune the model in the best way you can, but then predict it on the new data. Ideally, every one of those predictions would be OOB.... but maybe punt on that one for the moment?

Eh though the trends aren't that far off from random, so might the biggest deal in the world. Let's focus on the down/upsampling furst as one last hail mary for out-of-sample power.  


```{r}

vast_index <- vast_fish %>% 
  mutate(index = map(vasterized_data,"time_index")) %>% 
  select(survey, index) %>% 
  ungroup() %>% 
  unnest() %>% 
  group_by(survey, Year) %>% 
  summarise(abundance = sum(abundance)) %>% 
  filter(abundance > 0) %>% 
  group_by(survey) %>% 
  mutate(cs_abundance = (abundance - mean(abundance)) / sd(abundance)) %>% 
  ungroup()

vast_index %>% 
  ggplot(aes(Year, cs_abundance)) + 
  geom_line() + 
  geom_point() + 
  facet_wrap(~survey)


```


Actually, looks like gbm takes weights as an option... as does ranger. Maybe try that first, and then try 

Could also use caret's upSample/downSample functions to test those. So, that's three more runs ha, upsampled, downsampled, and weighted. d'oh. 

# Check weighting effect

weighting doens't seem to make all that much of a difference.... might just be though that it's not working? though they are different. 
```{r}

skynet_models %>% 
  filter(model == "gbm", test_set == "random") %>% 
  ggplot(aes(weight_surveys, r2)) + 
  geom_boxplot()

```


# Solving the scale of prediction

This came about after noticing discrepencies between the time series from the random split vs the historic split. Obviously, the random had less data in the splite, since it's a sampling of the data, while the historic split has the full data in each split (so the historic split is more accurate in that sense)

So, that begs the question of when you want to be fitting to "all" the data. The answer is never I think. Suppose that you were actually interested in predicting the west coast, and so you do all your test and training splits to demonstrate its out of sample performance. But, for your final morel, you used all the data to predict the region. That's not the same model anymore, and so you wouldn't be able to make the same statements about predictive accuracy that you made about the "oob" tested model. So, in the case of say predicting Africa. you could of course fit the model using every scrap of data that you have, and then use that to predict Africa, but you no longer have a reliable metric of performance of that model, you're kind of back to the panel model problem. 

So, from the perspective of this paper, the point of the figure that you're showing is to demonstrate out of sample performance. To predict Africa then, the most reasonable thing then seems to be to use the random splitting, test performance there, and apply to somewhere else and see how it goes. 

So, I think the temporal plot you have in there works!

It would be good then to also do a map showing out of region sampling performance. 

From there, you need to get your hands on some other survey data. 

It would be good to get things in meaningful units too 

# Track units in paper

WHAT THE HELL UNITS ARE THESE VAST OUTPUTS THINGS IN

from line 699 of VAST_v4_0_0.cpp, it seems like metric tons per km2?

Index_xcyl(x,c,y,l) = D_xcy(x,c,y) * a_xl(x,l) / 1000;  // Convert from kg to metric tonnes

# Questions for Jim

turn omega and epsilon to zero if some species standard errors are going to zero

TMB::optimzie(bias.correct = FALSE)

pros and cons of multispecies vs single species version, more sensitive 

0-1 for single species,0-numspecies in multispecies context. So if you are doing 50 species, put in as many factors as you have for number of species. Choose number of factors a priori to be a lower number. You can add factors until the next factor explains a very small proportion of 5-8 factors might be the better option. 5,5,5,5 for field config for example. 

You get an improvement for rare species with the VAST version, so probably better to do individual species, since the rare species probably don't make that much of a difference. 

overdispersion config = 1,1 for the west coast example. For the other ones I would fix it to 0,0

double check that D_xcy spits out the rank scale densities, not log, but there's no bias correction

extract plotIndex to see the bias since 

look at qqplot and parameter_score.ests

ecology letter fitting models with machine learning since it gives you a measure of the max predictive accuracy 

loadings matrix times its transpose.... something. plotFactors function

- # of knots and tuning parameters

  - Test out increased number of knots for GOABTS
  
  - 

- Batch level diagnostics

- including environmental covariates?


# One more stab at a better model


After messing around with the ram prediction stuff, I'm realizing that what these machine learning things are great at is filling in gaps iwthin the universe of their training data, but not really for things that aren't of that universe. It's the "skin tone hand dryer problem"

That is especially apparent now with the out-of-region sampling regimes, which are pure garbage for the test set. 

  - look through applied predictive modeling a bit more
  
  - double check the dependent variable side of thigns, is there something you could be doing better there?
  
  - on the dependent variable side of things, what if you tried to predict the percentile rank of density? That poses a challenge of scale, but it might work better. So, there you would transform density into percentil rank of density. The dependent variables are getting centered and scaled, though I want to make sure that add_predictions is doing things right....  but if it is then I don't think that transforming effort into a rank would make that much of a difference... Though to an extent it still might. At the moment, let's say you train on alaska and center and scale there. Then, the west coast is going to look like it has really small effort, relative to the mean of the alaska data. So, might be better to center and scale the data at the splitting point. SO, the predictor is regional average effort. 

On the centering and scaling side, the key question is really how is caret/add recipes dealing with the centering and scaling: are they doing what recipes does and storing the mean and sd for the training data and using that to rescale the testing data, or are they recalculating on the fly?

So here's what I think you need to do. 

Raw data that is centered and scaled locally for the v-fold cross validation step. Once you have the best parameters, then use those parameters to train on the already centered and scaled data, and then predict on the the pre (and locally) centered and scaled testing data. 

Aha, from the caret documentation

"Note that, in all cases, the preProcess function estimates whatever it requires from a specific data set (e.g. the training set) and then applies these transformations to any data set without recomputing the values"... so that's not really what you want. So, I think sadly here you are going to have to roll your own.... though maybe use caret for the model tuning? 

that's an idea. Use caret for model tuning on the training data. Then, use your own calls, using the tuned parameters, to gbm, ranger, whatever, on the locally centered and scaled test and training data 

What about time there? In that case, you would still center and scale by region

Aha, as I suspected, the centered ones can't beat the best of the log density, but they don't get as bad. There's progress to be made here. 

# Where to from here


The fact that the models actually work better on the unfished species than the fished is troubling to say the least. It seems to suggest to me that what the models are basically doing is habitat mapping, and that might explain why understanding the unfished is actually easier (less regulations to distort behavior). Given just GFW data then, our hypothesis is that behavior is reflective of economic priorities, which include abundance. Given that the model is just as good at picking up species that aren't fished, it seems like it's just tuning to patterns that aren't reflective of economic choices per say, and as such are likely to have a hard time generalizing. This could just be a mismatch of scale, but point being it's hard to get good evidence from this that we can uSE GFW to predict where and when fish are in places without the surveys. 

So, the options

  - Tinker some more. Can still mess around with a few more transformations of the data
    - more data
  
  - Go with a negative result
    - Effort data is not a good predictor of spatio-temporal distribution of fish
    - Effort provide a predictable but idosynchratic link to abundance
    - Important to stake a well defended claim to the idea before someone publishes something simpler and wrong
    - Is there just a "well duh, the data don't match well enough" problem here?
    - Definitely don't believe that effort is random, so that would mean either that
      - Mismatch between abundance data and resulting effort
    
  - KISS
    - Go back to RAM and look at gross biomass v. effort
    - Integrate with catch data, SP models?
    - Hard since can't really allocate effort to different sectors. 
    

# data filtering

Runs to create options for
- GFW vars only
- fished/unfished only
- survey months only
- lagged biomass

# Final Analyses

Unless things take a serious turn, you're not getting any better results from the analysis. So, what do you need to do?

## filling in spatial gaps

Can you use GFW to augment assessments by filling in spatial gaps in the survey? Evidence so far suggest not really if it's the case of going from Alaska to GOA, and you already have that run. But you haven't really explored the within-survey side of things much. 

So, what you need is a thing that does test and training internally, e.g subdivide each region, either by splitting or spatially correlated sampling. 

From there, you should also do historic, but by region as well. 
