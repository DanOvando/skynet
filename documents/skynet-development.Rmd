---
title: "Skynet Development"
output: html_document
---

This is a sketchpad for developing the tools and code needed to evaluate project skynet. As much as possible, use functions to do all the critical steps that you will then call into use for the formal run script. Use this space to think outloud and examine/debug outputs of scripts. 

# Load things

```{r}
set.seed(42)
knitr::opts_chunk$set(message = F, warning = F)
library(bigrquery)
library(lubridate)
library(tmap)
library(leaflet)
library(rgdal)
library(FishData)
library(ggmap)
library(tidyverse)
library(stringr)
library(viridis)
library(purrr)
library(trelliscopejs)

demons::load_functions('../functions')

run_name <- 'testing'

run_description <- 'development of full paper'

run_dir <- file.path('../results', run_name, '')

if (dir.exists(run_dir) == F) {
  dir.create(run_dir)
}

write(run_description, file = paste0(run_dir, 'description.txt'))


```

# Build databases

This section will be used to devlop functions to get Fishdata and GFW data, and compile the "final" databases that you will use. 

Let's talk resolution for a moment here. For everything except FishData, you can specify the lat-lon resolution. So, to make life a little easier from the get-go, let's make this variable and pass that along the queries to make sure that they are all in the same units. 

```{r}

lat_lon_res <- 0.25 # round data to intervels of 0.25 degrees lat lon, as in 56.25

```


## FishData

Let's start with getting the FishData, since you'll use the locations from FishData to decide which places you want to aggregate and scrape from GFW. 

Let's focus on getting the compelte database up and running now, so that the actual script can have an option to either load the compiled data from box, or to reload it from scratch. 

`download_catch_rates` is the function you need from FishData, the only problem is that it's a little unclear what the potential list of surveys are. 

Looking through FishData, seems like Alaska is really what you have available to you. SOAB. You're going to need some non-alaska places, but at least you also do have West Coast. There is also some irish data availabe in `download_datras`

```{r}

fishdata_names <- tribble(
  ~survey, ~survey_name,
  #--/--
  'EBSBTS', 'eastern bering sea bottom trawl survey',
  'WCGBTS','west coast groundfish bottom trawl survey',
  'WCGHL', 'West_coast_groundfish_hook_and_line',
  'GOABTS', 'gulf of alaska bottom trawl survey',
  'AIBTS', 'aluetian islands bottom trawl survey'
)

```


Note: the error_tol is a bit finicky. Somewhere in the missing zeros thing it's messing up the addition of new things by a few decimal places. Change to 1e-2 and that seems to do it, off by a few decimals, but not really a problem. 

```{r, eval = F}

fish_data <- fishdata_names %>% 
  mutate(survey_data = map(survey, ~download_catch_rates(.x, species_set = 50,
                           error_tol = 1e-2)))

fish_data <- unnest(fish_data)
  
save(file = '../data/fish_data.Rdata', fish_data)
  
```

OK, that gives you the data that you need across all the surveys. Now, you need to get the GFW data that fall under those coordinates.... This should be fun

Let's make some plots just to check your data a bit


```{r}
load("../data/fish_data.Rdata")

fish_maps <- fish_data %>% 
    set_names(tolower(colnames(.))) %>% 
  mutate(rlat = round(lat * (1/lat_lon_res)) / ((1/lat_lon_res)),
         rlon = round(long * (1/lat_lon_res)) / ((1/lat_lon_res))) %>% 
  group_by(survey, year, sci, rlat, rlon) %>% 
  summarise(mean_density = mean(wt, na.rm = T)) %>% 
  group_by(survey, year, rlat, rlon) %>% 
  summarise(total_mean_density = sum(mean_density)) %>% 
  na.omit()

mapfoo <- function(dens){
  
  qmplot(
  x = rlon,
  y = rlat,
  data = dens,
  color = log(total_mean_density),
  maptype = 'toner-lite',
  source = 'google'
) +
  scale_color_viridis(guide = F) +
  facet_wrap( ~ year) +
  theme_classic() + 
  labs(caption = 'Total Mean Species Density')
  
}


a <- fish_maps %>% 
  ungroup() %>% 
  filter(rlon < 0,year >=2012) %>% 
  nest(-survey, .key = densities) %>% 
  mutate(density_map = map_plot(densities, mapfoo))

trelliscope(a, name = 'blah', panel_col = 'density_map', self_contained = F)
  
#   
# quick_map(dat = dat, lat_var = quo(rlat), lon_var = quo(rlon),
#           plot_var = quo(total_mean_density),facet_var = quo(year))


```

Great, that all looks good (finally), though there is something a little off with the AIBTS data on the other side of the international date line... should examine that a little later. 

## GFW data

Now what you need are the bounding boxes for each of the regions, to provide a bit of constraint to your GFW queries. 

```{r}

survey_bbox <- fish_data %>% 
  filter(Long <0) %>% 
  group_by(survey) %>% 
  summarise(min_year = min(Year, na.rm = T),
            max_year = max(Year, na.rm = T),
            min_lat = min(Lat, na.rm = T),
            max_lat = max(Lat, na.rm = T),
            min_lon = min(Long, na.rm = T),
            max_lon = max(Long, na.rm = T))

survey_bbox

survey_names <- paste0(fishdata_names$survey %>% tolower(),'_gfw')

```

OK I went into google bigquery and set up a series of queries that produce tables of the form *<survey>_gfw*, and saved them under the skynet project in our bigquery account. Let's try and grab those things now. 

```{r, eval = F}


gfw_data <- data_frame(survey = survey_names)

project <- "ucsb-gfw"

fishing_connection <- DBI::dbConnect(bigrquery::dbi_driver(),
                         project = project,
                         dataset = 'skynet')

gfw_data <- gfw_data %>% 
  mutate(gfw_data = map(survey, ~fishing_connection %>%
  tbl(.x) %>%
  collect(n = Inf) %>%
  select(-b_mmsi,-b_year) %>%
  set_names(str_replace_all(colnames(.), '(a_)|(b_)', '')) %>%
  filter(is.na(on_fishing_list_nn) == F) %>%
  arrange(year, mmsi)

))

save(file = '../data/gfw_data.Rdata', gfw_data)

```

Examining the data a bit to make sure that there's some rhyme and reason to the data

```{r}

load(file = '../data/gfw_data.Rdata')

gfw_plot_foo <- function(dat){
  
  mapdat <- dat %>% 
    group_by(year, rounded_lat, rounded_lon) %>% 
    summarise(fishing_hours = sum(total_hours, na.rm = T)) %>% 
    ungroup() %>% 
    rename(rlon = rounded_lon, rlat = rounded_lat)

  
   qmplot(
  x = rlon,
  y = rlat,
  data = mapdat,
  color = log(fishing_hours),
  maptype = 'toner-lite',
  source = 'google'
) +
  scale_color_viridis(guide = F) +
  facet_wrap( ~ year) +
  theme_classic() + 
  labs(caption = 'Total Fishing Hours')
  
  
}

gfw_data <- gfw_data %>% 
  mutate(plots = map_plot(gfw.data, gfw_plot_foo))


trelliscope(
  gfw.data,
  name = 'blah',
  panel_col = 'plots',
  self_contained = F
  )
  

```

Ok, working the 0.25 degree unit seems to work nicely. Those GFW data are saved. 

At some point though you'll need to make a more intelligent decision about the spatial scale that you want to aggregate things to

## CHL-A



```{r, eval = F}

chl_data <- survey_bbox %>% 
  mutate(chl_a = pmap(list(min_year = pmax(2012,min_year), 
                         max_year = max_year,
                         min_lat = min_lat,
                         max_lat = max_lat, 
                         min_lon = min_lon,
                         max_lon = max_lon
                         ), query_erddap,
                    desired_data = 'chl-a',
                    date_interval = 1,
                    space_interval = 1,
                    runit = lat_lon_res))

chl_plot_foo <- function(dat){


   qmplot(
  x = rlon,
  y = rlat,
  data = dat,
  color = log(mean_chlorophyll),
  maptype = 'toner-lite',
  source = 'google'
) +
  scale_color_viridis(guide = F) +
  facet_wrap( ~ year) +
  theme_classic() + 
  labs(caption = 'SST(c)')
  
  
}

chl_data <-  chl_data %>% 
  mutate(plots = map_plot(chl_a, chl_plot_foo))
  
trelliscope(
  chl_data,
  name = 'blah',
  panel_col = 'plots',
  self_contained = F
  )
  
save(file = '../data/chl_data.Rdata', chl_data)


```

## And SST

and sst
```{r}
sst_data <- survey_bbox %>% 
  mutate(sst = pmap(list(min_year = pmax(2012,min_year), 
                         max_year = max_year,
                         min_lat = min_lat,
                         max_lat = max_lat, 
                         min_lon = min_lon,
                         max_lon = max_lon
                         ), query_erddap,
                    desired_data = 'sst',
                    date_interval = 14,
                    space_interval = 25,
                    runit = lat_lon_res))

sst_plot_foo <- function(dat){


   qmplot(
  x = rlon,
  y = rlat,
  data = dat,
  color = log(mean_analysed_sst),
  maptype = 'toner-lite',
  source = 'google'
) +
  scale_color_viridis(guide = F) +
  facet_wrap( ~ year) +
  theme_classic() + 
  labs(caption = 'SST(c)')
  
  
}

sst_data <-  sst_data %>% 
  mutate(plots = map_plot(sst, sst_plot_foo))
  
trelliscope(
  sst_data,
  name = 'blah',
  panel_col = 'plots',
  self_contained = F
  )

save(file = '../data/sst_data.Rdata', sst_data)

```


## MPA coverage

You can get this in GFW from the "regions" column, which you'll need to generate into your data. You can use a regex that Juan sent you in slack to parse out the MPA coverage

here's the regex

LEFT(REGEXP_EXTRACT(regions,‘\“(fao:.*?)\“‘),INSTR(REGEXP_EXTRACT(regions,‘\“(fao:.*?)\“‘),“.”)-1),REGEXP_EXTRACT(regions,‘\“(fao:.*?)\“‘)), ‘[^0-9 ]‘,’‘))

but instead of FAO, sub in mparu (restricted use) mpant (no take)

lookup tables live in region_ids in the UCSB thing. You've already included mparu and mpant as variables in the GFW data, let's get the lookup table here

```{r}


project <- "ucsb-gfw"

mpa_ids <- data_frame(mpa_type = c('no_take_mpas_id', 'restricted_use_mpas_id'))

fishing_connection <- DBI::dbConnect(bigrquery::dbi_driver(),
                         project = project,
                         dataset = 'regions_ids')

mpa_ids <- mpa_ids %>% 
  mutate(data = map(mpa_type, ~fishing_connection %>%
  tbl(.x) %>%
  collect(n = Inf))) %>% 
  unnest()

mpa_ids <- mpa_ids %>% 
  mutate(mpa_type = str_split(region_id, ':', simplify = T)[,1],
         mpa_id = str_split(region_id, ':', simplify = T)[,2])

save(file = '../data/mpa_ids.Rdata', mpa_ids)

```



## Waves

NWW3_Global_Best
```{r}

wave_data <- survey_bbox %>% 
  mutate(wave_height = pmap(list(min_year = pmax(2012,min_year), 
                         max_year = max_year,
                         min_lat = min_lat,
                         max_lat = max_lat, 
                         min_lon = min_lon,
                         max_lon = max_lon
                         ), query_erddap,
                    desired_data = 'waves',
                    date_interval = 24*14, #1 hour interval
                    space_interval = 0.5,
                    runit = lat_lon_res))


wave_data <- wave_data %>% 
  mutate(plots = map_plot(wave_height, ~quick_map(.x, lat_var = quo(rlat), 
                                                 lon_var = quo(rlon),
                                                 plot_var = quo(mean_Thgt),
                                                 facet_var = quo(year))))


trelliscope(
  wave_data,
  name = 'blah',
  panel_col = 'plots',
  self_contained = F
  )



save(file = '../data/wave_data.Rdata', wave_data)


```

## Wind


```{r}
wind_data <- survey_bbox %>% 
  mutate(wind_speed = pmap(list(min_year = pmax(2012,min_year), 
                         max_year = max_year,
                         min_lat = min_lat,
                         max_lat = max_lat, 
                         min_lon = min_lon,
                         max_lon = max_lon
                         ), query_erddap,
                    desired_data = 'wind',
                    date_interval = 1, #1 hour interval
                    space_interval = 1,
                    runit = lat_lon_res))


wind_data <- wind_data %>%
  mutate(wind_vectors = map(
  wind_speed, ~ .x %>%
  spread(wind_direction, mean_wind_speed) %>%
  mutate(wind_vector = map2(
  x_wind, y_wind, ~ vectorize_wind(x_wind = .x, y_wind = .y)
  ),
  wind_speed = map_dbl(wind_vector,"wind_speed"),
  wind_angle = map_dbl(wind_vector,"wind_angle")) %>% 
    select(-wind_vector, -x_wind, -y_wind)
  )) %>% 
  select(-wind_speed)
         


wind_data <- wind_data %>% 
  mutate(plots = map_plot(wind_vectors, ~quick_map(.x, lat_var = quo(rlat), 
                                                 lon_var = quo(rlon),
                                                 plot_var = quo(wind_speed),
                                                 facet_var = quo(year))))


trelliscope(
  wind_data,
  name = 'blah',
  panel_col = 'plots',
  self_contained = F
  )


save(file = '../data/wind_data.Rdata', wind_data)

```

## Bathymetry


# Fit Models

Fit a series of models on the data at different aggregation levels

# Diagnose models










