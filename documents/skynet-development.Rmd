---
title: "Skynet Development"
output: html_document
---


This is a sketchpad for developing the tools and code needed to evaluate project skynet. As much as possible, use functions to do all the critical steps that you will then call into use for the formal run script. Use this space to think outloud and examine/debug outputs of scripts. 

# Load things

```{r}
set.seed(42)
knitr::opts_chunk$set(message = F, warning = F)
library(bigrquery)
library(lubridate)
library(leaflet)
library(rgdal)
library(FishData)
library(ggmap)
library(stringr)
library(viridis)
library(purrr)
library(purrrlyr)
library(modelr)
library(VAST)
library(TMB)
library(trelliscopejs)
library(ggjoy)
library(modelr)
library(caret)
library(tidyverse)


demons::load_functions('../functions')

run_name <- 'testing'

run_description <- 'development of full paper'

run_dir <- file.path('../results', run_name, '')

if (dir.exists(run_dir) == F) {
  dir.create(run_dir)
}

write(run_description, file = paste0(run_dir, 'description.txt'))


```

# Build databases

This section will be used to devlop functions to get Fishdata and GFW data, and compile the "final" databases that you will use. 

Let's talk resolution for a moment here. For everything except FishData, you can specify the lat-lon resolution. So, to make life a little easier from the get-go, let's make this variable and pass that along the queries to make sure that they are all in the same units. 

```{r}

lat_lon_res <- 0.25 # round data to intervels of 0.25 degrees lat lon, as in 56.25

```


## FishData

Let's start with getting the FishData, since you'll use the locations from FishData to decide which places you want to aggregate and scrape from GFW. 

Let's focus on getting the compelte database up and running now, so that the actual script can have an option to either load the compiled data from box, or to reload it from scratch. 

`download_catch_rates` is the function you need from FishData, the only problem is that it's a little unclear what the potential list of surveys are. 

Looking through FishData, seems like Alaska is really what you have available to you. SOAB. You're going to need some non-alaska places, but at least you also do have West Coast. There is also some irish data availabe in `download_datras`

```{r}

fishdata_names <- tribble(
  ~survey, ~survey_name,
  #--/--
  'EBSBTS', 'eastern bering sea bottom trawl survey',
  'WCGBTS','west coast groundfish bottom trawl survey',
  'WCGHL', 'West_coast_groundfish_hook_and_line',
  'GOABTS', 'gulf of alaska bottom trawl survey',
  'AIBTS', 'aluetian islands bottom trawl survey'
)

```


Note: the error_tol is a bit finicky. Somewhere in the missing zeros thing it's messing up the addition of new things by a few decimal places. Change to 1e-2 and that seems to do it, off by a few decimals, but not really a problem. 

```{r, eval = F}

fish_data <- fishdata_names %>% 
  mutate(survey_data = map(survey, ~download_catch_rates(.x, species_set = 50,
                           error_tol = 1e-2)))

fish_data <- unnest(fish_data)
  
save(file = '../data/fish_data.Rdata', fish_data)
  
```

OK, that gives you the data that you need across all the surveys. Now, you need to get the GFW data that fall under those coordinates.... This should be fun

Let's make some plots just to check your data a bit


```{r}
load("../data/fish_data.Rdata")

fish_maps <- fish_data %>% 
    set_names(tolower(colnames(.))) %>% 
  mutate(rlat = round(lat * (1/lat_lon_res)) / ((1/lat_lon_res)),
         rlon = round(long * (1/lat_lon_res)) / ((1/lat_lon_res))) %>% 
  group_by(survey, year, sci, rlat, rlon) %>% 
  dplyr::summarise(mean_density = mean(wt, na.rm = T)) %>% 
  group_by(survey, year, rlat, rlon) %>% 
  summarise(total_mean_density = sum(mean_density)) #%>% 
  # na.omit()

mapfoo <- function(dens){
  
  # 
  # qmplot(rlon, rlat, color = total_mean_density, data = dens) + 
  # facet_wrap(~year)
  
  qmplot(
  x = rlon,
  y = rlat,
  data = dens,
  color = log(total_mean_density)
  # maptype = 'toner-lite',
  # source = 'google'
) +
  scale_color_viridis(guide = F) +
  facet_wrap( ~ year) +
  theme_classic() +
  labs(caption = 'Total Mean Species Density')
  
}

```

Great, that all looks good (finally), though there is something a little off with the AIBTS data on the other side of the international date line... should examine that a little later. 

## GFW data

Now what you need are the bounding boxes for each of the regions, to provide a bit of constraint to your GFW queries. 

```{r}

survey_bbox <- fish_data %>% 
  filter(Long <0) %>% 
  group_by(survey) %>% 
  summarise(min_year = min(Year, na.rm = T),
            max_year = max(Year, na.rm = T),
            min_lat = min(Lat, na.rm = T),
            max_lat = max(Lat, na.rm = T),
            min_lon = min(Long, na.rm = T),
            max_lon = max(Long, na.rm = T))

survey_bbox

survey_names <- paste0(fishdata_names$survey %>% tolower(),'_gfw')

```



OK I went into google bigquery and set up a series of queries that produce tables of the form *<survey>_gfw*, and saved them under the skynet project in our bigquery account. Let's try and grab those things now. 

```{r, eval = F}


gfw_data <- data_frame(survey = survey_names)

project <- "ucsb-gfw"

fishing_connection <- DBI::dbConnect(bigrquery::dbi_driver(),
                         project = project,
                         dataset = 'skynet')

gfw_data <- gfw_data %>% 
  mutate(gfw_data = map(survey, ~fishing_connection %>%
  tbl(.x) %>%
  collect(n = Inf) %>%
  select(-b_mmsi,-b_year) %>%
  set_names(str_replace_all(colnames(.), '(a_)|(b_)', '')) %>%
  filter(is.na(on_fishing_list_nn) == F) %>%
  arrange(year, mmsi)

))

save(file = '../data/gfw_data.Rdata', gfw_data)

```

Examining the data a bit to make sure that there's some rhyme and reason to the data

```{r}

load(file = '../data/gfw_data.Rdata')

gfw_plot_foo <- function(dat){
  
  mapdat <- dat %>% 
    group_by(year, rounded_lat, rounded_lon) %>% 
    summarise(fishing_hours = sum(total_hours, na.rm = T)) %>% 
    ungroup() %>% 
    rename(rlon = rounded_lon, rlat = rounded_lat)

  
   qmplot(
  x = rlon,
  y = rlat,
  data = mapdat,
  color = log(fishing_hours),
  maptype = 'toner-lite',
  source = 'google'
) +
  scale_color_viridis(guide = F) +
  facet_wrap( ~ year) +
  theme_classic() + 
  labs(caption = 'Total Fishing Hours')
  
  
}

gfw_data <- gfw_data %>% 
  mutate(plots = map_plot(gfw_data, gfw_plot_foo))


trelliscope(
  gfw_data,
  name = 'blah',
  panel_col = 'plots',
  self_contained = F
  )
  

```

Ok, working the 0.25 degree unit seems to work nicely. Those GFW data are saved. 

At some point though you'll need to make a more intelligent decision about the spatial scale that you want to aggregate things to

finding bounding boxes from data


## CHL-A



```{r, eval = F}

chl_data <- survey_bbox %>% 
  mutate(chl_a = pmap(list(min_year = pmax(2012,min_year), 
                         max_year = max_year,
                         min_lat = min_lat,
                         max_lat = max_lat, 
                         min_lon = min_lon,
                         max_lon = max_lon
                         ), query_erddap,
                    desired_data = 'chl-a',
                    date_interval = 1,
                    space_interval = 1,
                    runit = lat_lon_res))

chl_plot_foo <- function(dat){


   qmplot(
  x = rlon,
  y = rlat,
  data = dat,
  color = log(mean_chlorophyll),
  maptype = 'toner-lite',
  source = 'google'
) +
  scale_color_viridis(guide = F) +
  facet_wrap( ~ year) +
  theme_classic() + 
  labs(caption = 'SST(c)')
  
  
}

chl_data <-  chl_data %>% 
  mutate(plots = map_plot(chl_a, chl_plot_foo))
  
trelliscope(
  chl_data,
  name = 'blah',
  panel_col = 'plots',
  self_contained = F
  )
  
save(file = '../data/chl_data.Rdata', chl_data)


```

## And SST

and sst
```{r, eval = F}
sst_data <- survey_bbox %>% 
  mutate(sst = pmap(list(min_year = pmax(2012,min_year), 
                         max_year = max_year,
                         min_lat = min_lat,
                         max_lat = max_lat, 
                         min_lon = min_lon,
                         max_lon = max_lon
                         ), query_erddap,
                    desired_data = 'sst',
                    date_interval = 14,
                    space_interval = 25,
                    runit = lat_lon_res))

sst_plot_foo <- function(dat){


   qmplot(
  x = rlon,
  y = rlat,
  data = dat,
  color = log(mean_analysed_sst),
  maptype = 'toner-lite',
  source = 'google'
) +
  scale_color_viridis(guide = F) +
  facet_wrap( ~ year) +
  theme_classic() + 
  labs(caption = 'SST(c)')
  
  
}

sst_data <-  sst_data %>% 
  mutate(plots = map_plot(sst, sst_plot_foo))
  
trelliscope(
  sst_data,
  name = 'blah',
  panel_col = 'plots',
  self_contained = F
  )

save(file = '../data/sst_data.Rdata', sst_data)

```


## MPA coverage

You can get this in GFW from the "regions" column, which you'll need to generate into your data. You can use a regex that Juan sent you in slack to parse out the MPA coverage

here's the regex

LEFT(REGEXP_EXTRACT(regions,‘\“(fao:.*?)\“‘),INSTR(REGEXP_EXTRACT(regions,‘\“(fao:.*?)\“‘),“.”)-1),REGEXP_EXTRACT(regions,‘\“(fao:.*?)\“‘)), ‘[^0-9 ]‘,’‘))

but instead of FAO, sub in mparu (restricted use) mpant (no take)

lookup tables live in region_ids in the UCSB thing. You've already included mparu and mpant as variables in the GFW data, let's get the lookup table here

```{r, eval = F}


project <- "ucsb-gfw"

mpa_ids <- data_frame(mpa_type = c('no_take_mpas_id', 'restricted_use_mpas_id'))

fishing_connection <- DBI::dbConnect(bigrquery::dbi_driver(),
                         project = project,
                         dataset = 'regions_ids')

mpa_ids <- mpa_ids %>% 
  mutate(data = map(mpa_type, ~fishing_connection %>%
  tbl(.x) %>%
  collect(n = Inf))) %>% 
  unnest()

mpa_ids <- mpa_ids %>% 
  mutate(mpa_type = str_split(region_id, ':', simplify = T)[,1],
         mpa_id = str_split(region_id, ':', simplify = T)[,2])

save(file = '../data/mpa_ids.Rdata', mpa_ids)

```

Check MPAs

```{r, eval = F}

gfw_data <- gfw_data %>% 
  mutate(gfw_data = map(gfw_data, ~.x %>% mutate(any_mpa = (!is.na(mparu)) | (!is.na(mpant)))))


mpa_plots <- gfw_data %>% 
  mutate(plots = map_plot(gfw_data, ~quick_map(.x, lat_var = quo(rounded_lat), 
                                                 lon_var = quo(rounded_lon),
                                                 plot_var = quo(any_mpa),
                                                 facet_var = quo(year))))

mpa_plots$plots[[5]]
```


## Waves

NWW3_Global_Best
```{r, eval = F}

wave_data <- survey_bbox %>% 
  mutate(wave_height = pmap(list(min_year = pmax(2012,min_year), 
                         max_year = max_year,
                         min_lat = min_lat,
                         max_lat = max_lat, 
                         min_lon = min_lon,
                         max_lon = max_lon
                         ), query_erddap,
                    desired_data = 'waves',
                    date_interval = 24*14, #1 hour interval
                    space_interval = 0.5,
                    runit = lat_lon_res))


wave_data <- wave_data %>% 
  mutate(plots = map_plot(wave_height, ~quick_map(.x, lat_var = quo(rlat), 
                                                 lon_var = quo(rlon),
                                                 plot_var = quo(mean_Thgt),
                                                 facet_var = quo(year))))


trelliscope(
  wave_data,
  name = 'blah',
  panel_col = 'plots',
  self_contained = F
  )



save(file = '../data/wave_data.Rdata', wave_data)


```

## Wind


```{r, eval = F}
wind_data <- survey_bbox %>% 
  mutate(wind_speed = pmap(list(min_year = pmax(2012,min_year), 
                         max_year = max_year,
                         min_lat = min_lat,
                         max_lat = max_lat, 
                         min_lon = min_lon,
                         max_lon = max_lon
                         ), query_erddap,
                    desired_data = 'wind',
                    date_interval = 1, #1 hour interval
                    space_interval = 1,
                    runit = lat_lon_res))


wind_data <- wind_data %>%
  mutate(wind_vectors = map(
  wind_speed, ~ .x %>%
  spread(wind_direction, mean_wind_speed) %>%
  mutate(wind_vector = map2(
  x_wind, y_wind, ~ vectorize_wind(x_wind = .x, y_wind = .y)
  ),
  wind_speed = map_dbl(wind_vector,"wind_speed"),
  wind_angle = map_dbl(wind_vector,"wind_angle")) %>% 
    select(-wind_vector, -x_wind, -y_wind)
  )) %>% 
  select(-wind_speed)
         


wind_data <- wind_data %>% 
  mutate(plots = map_plot(wind_vectors, ~quick_map(.x, lat_var = quo(rlat), 
                                                 lon_var = quo(rlon),
                                                 plot_var = quo(wind_speed),
                                                 facet_var = quo(year))))


trelliscope(
  wind_data,
  name = 'blah',
  panel_col = 'plots',
  self_contained = F
  )


save(file = '../data/wind_data.Rdata', wind_data)

```

## Topography


```{r, eval = F}

topo_data <- survey_bbox %>% 
  mutate(altitude = pmap(list(min_year = pmax(2012,min_year), 
                         max_year = max_year,
                         min_lat = min_lat,
                         max_lat = max_lat, 
                         min_lon = min_lon,
                         max_lon = max_lon
                         ), query_erddap,
                    desired_data = 'topography',
                    date_interval = 1, #1 hour interval
                    space_interval = 1,
                    runit = lat_lon_res))

topo_data  <- topo_data %>% 
  mutate(altitude = map(altitude, ~ .x %>% 
                             mutate(m_below_sea_level = -1*mean_altitude)))
  


topo_data <- topo_data %>% 
  mutate(plots = map_plot(altitude, ~quick_map(.x, lat_var = quo(rlat), 
                                                 lon_var = quo(rlon),
                                                 plot_var =quo(m_below_sea_level),
                                                 facet_var = quo(mean_altitude_units))))


trelliscope(
  topo_data,
  name = 'blah',
  panel_col = 'plots',
  self_contained = F
  )



save(file = '../data/topo_data.Rdata', topo_data)


```


# Check data

```{r}

data_files <- list.files("../data/")

data_files <-
  paste0('../data/', data_files[str_detect(data_files, '_data.Rdata')])

for (i in seq_along(data_files)) {
  load(data_files[i])
  
}

chl_data %>% 
  unnest() %>% 
  ggplot(aes(x = mean_chlorophyll, y = factor(year), fill = survey)) + 
  geom_joy(alpha = 0.75) + 
  theme_joy()

sst_data %>% 
  unnest() %>% 
  ggplot(aes(x = mean_analysed_sst, y = factor(year), fill = survey)) + 
  geom_joy(alpha = 0.75) + 
  theme_joy()

topo_data %>% 
  select(survey, altitude) %>% 
  unnest() %>% 
  ggplot(aes(x = m_below_sea_level, y = survey, fill = survey)) + 
  geom_joy(alpha = 0.75) + 
  theme_joy()

wind_data %>% 
  select(survey, wind_vectors) %>% 
  unnest() %>% 
  ggplot(aes(x = wind_speed, y = survey, fill = survey)) + 
  geom_joy(alpha = 0.75) + 
  theme_joy()

wave_data %>% 
  select(survey, wave_height) %>% 
  unnest() %>% 
  ggplot(aes(x = mean_Thgt, y = factor(year), fill = survey)) + 
  geom_joy(alpha = 0.75) + 
  theme_joy()

```

All in all looks pretty good. THere's some odd stuff in the west coast data since technically speaking the bounding box encompasses the sierras, but that shouldn't be a problem once you're matching to GFW, just makes the plots look a little weird. But, should check the data one more time once they are merged



# Vastify FishData

Good lord it's amazing how long that can take. OK, now that you've got all the data in here, let's run it through `spatial-delta-GLM`. Before you do that, let's put together a vignette explaining what the hell it is actually doing, since I confess to viewing it as a black box at this point. 

The original article is "Geostatistical delta-generalized linear mixed models improve precision for estimated abundance indices for West Coast groundfishes", @Thorson2015

"Data derived from fishery-independent surveys are generally processed to generate an index that is intended to be pro- portional to population abundance"

" U.S. National Marine Fisheries Service primarily use design-based estimators, which generally calculate average catch rates within each predetermined sampling stratum in the sampling design, then generate an area-weighted sum of abundance in each stratum"

So this is an alternative to relying on design (e.g. stratified random sampling and averaging across stratum) or GLM based statistical techniques for extracting the "true" index of abundance

A design based method would say that mean abundance is fixed over a stratum, and so the more variation there is within the stratum, that just increases the variance in the estimate of the mean

This can cause spikes (and drops) to be more of a function of sampling locations than of actual changes in abundance. Geostatistical methods account for that (see @Shelton2014)

"The goal of applying a geostatistical model to data from fisheries research surveys is to explain the catches of each species recorded in the survey, and hence to infer population density throughout the domain of the survey design"

"Specifically, a random field defines the expected value, variance and covariance of a multivariate realization from a stochastic process. In this case, the stochastic process represents the aggregate impact of environmental and biological factors that are not directly observed but still contribute to the distribution and density of the target species"

" To accomplish this, the user pre-specifies the desired number of “knots” nj that are used to approximate random field E over domain V, such that the model tracks the value of E at each knot. The value of E at any location s is equal to its value at the nearest knot (hence the piecewise-constant approximation). Therefore, the value of E at location si (the location for the ith sample) is determined from the value EJ(i) at the knot J(i) that is nearest to si. The location of all nj knots is determined by applying a cluster algorithm (in this case, a k-means algorithm) to the loca- tions of available sampling data."

"This clustering algorithm results in a distribution of knots with density that is proportional to the sampling intensity of the survey design. The area aj associated with each knot j is then calculated using the Voronoi tool in the PBSmapping package in R (Schnute et al., 2013). After being calcu- lated, the number and location of all knots is held fixed during par- ameter estimation. The prespecified number of knots controls the accuracy of this piecewise-constant approximation, and can be used to achieve a balance of accuracy and computational speed. This approximation also simplifies calculating the integral across the random field, as is necessary when calculating the index of abun- dance (as discussed in detail later)."

"Specifically, this model is based on the interpretation that population density across sampling domain V in a given year is a realizations of a random function. This random functions arises from the combin- ation of measured covariates x and random fields v and 1, and assumptions about these random fields (i.e. that they stationary but have geometric anisotropy) result in the properties of the random function used to describe spatial variation in population density. The stochastic process of sampling local densities using bottom trawl gear then introduces another source of variability, which follows a Bernoulli or gamma distribution (Eqs. (1) and (2)), and vessels have random variation in catch efficiency, thus representing a third source of variability. The treatment of space via random functions allows for inference similar to that for “intrin- sic” geostatistics, such that the geostatistical model (Eqs. (1)–(7)) can also be applied to non-randomized sampling designs (Petitgas, 2001), subject to the assumption the process of selecting sampling locations is independent of the process generating differ- ence in population density"

We use 1000 knots for all case-study runs of the geostatistical model (i.e. nj 1⁄4 1000)

In english: the density at any given point is equal to the predicted density at its nearest knot. The expected density at a given knot is modeled throuhg a "delta" process that encompasses the probability of seeing anything and the density conditional on seeing something. Those probabilities are calculated using fixed effects for things like ansiotropy, and random effects for things like the spatial residuals, that take into account that the residuals are distributed multivariate normal, with a covariance matrix defined by something like a variogram (nearby places are more similar)

OK, so in a nutshell then, you get density estimates by knot, which you then aggregate up into a system-wide density through a process that I'm a little unclear on. They calculate the total expected biomass across the region as the sum of the area weighted densities in each knot (i.e. if there are two patches of 1km2 each, and then density in 1 is 10 and the other is 5), then the total abundance estimate is 15, though you need to be careful in your units on this, i.e. this only works since the density is in the units of the cells. 

So, to translate. You use spatial-delta-glm to estimate a bunch of densities at each knot. You then calculate the total abundance across the domain as the sum of the density per unit area in that knot times the area encompassed by that knot. So, that gives you both a total abundance index at the spatial resolution that you want, along with the spatial distribution of that abundance. 

Cool. 

Let's take a look at a single-species example using VAST just to make sure that I'm understanding all the things that get spit out, comparing them to the "raw" survey estimates of biomass. 



```{r test-vast, eval=F}

raw_arrow <- fish_data %>% 
  filter(survey == 'WCGBTS', Sci == "Atheresthes_stomias") %>% 
  dplyr::rename(
    Lon = Long,
    Catch_KG = Wt,
    spp  = Sci
  ) %>%
  mutate(AreaSwept_km2 = AreaSept_ha / 1e2) %>% 
  select(Year, Lat, Lon, Vessel, AreaSwept_km2, Catch_KG,spp) %>% 
  na.omit() %>% 
  set_names(tolower(colnames(.))) %>% 
  mutate(Vessel = as.factor(vessel),
         spp = as.factor(spp)) %>% 
  as.data.frame() #note that this breaks if it's a tibble, should make a bit more flexible


test <-
  vasterize_index(
  raw_data = raw_arrow,
  run_dir = run_dir,
  nstart = 10,
  region = 'California_current',
  n_x = 1000
  )
  
```

OK, that all works, you'll need to convert that to a useful function, but it works. 

I'd like to understand better how it's calculating the overall index, and see if I can write my own function to do it, so that I can apply that same function to your GFW estimates... Aha. It's basically just the sum of the knots plus the bias correction. You'll need to think about the bias correction problem with the GFW, but you'll cross that bridge once you get there. 

So, to compare once you get GFW working, it will basically be the sum of the predicted densities at each point scaled by the area of the point. So, if the knot density is 10 fish per km2, and there are 10km2 per knot, then the total abundance at that knot it 100. You'll just need to be careful about units. 

## Multispecies

Let's take a look and see how different it is. Hopefully it's the same, in which case your next order of business if to build a useful wrapper for VAST for single or multi-species cases. Looks like it runs about the same, with the exception that for some unholy reason c_i appears to the the species index, aha, standing for species category. Good lord. see `?Data_Fn` for better explanations


```{r, eval = F}

# , Sci %in% c("Atheresthes_stomias", 'Microstomus_pacificus')
raw <- fish_data %>% 
  filter(survey == 'WCGBTS') %>% 
  dplyr::rename(
    Lon = Long,
    Catch_KG = Wt,
    spp  = Sci
  ) %>%
  mutate(AreaSwept_km2 = AreaSept_ha / 1e2) %>% 
  select(Year, Lat, Lon, Vessel, AreaSwept_km2, Catch_KG,spp) %>% 
  set_names(tolower(colnames(.))) %>% 
  na.omit() %>% 
  as.data.frame() #note that this breaks if it's a tibble, should make a bit more flexible


species_tallies <- raw %>% 
  group_by(year, spp) %>% 
  summarise(nobs = length(spp),
            num_zeros = sum(catch_kg == 0),
            num_pos = sum(catch_kg > 0)) %>% 
  group_by(spp) %>% 
  summarise(missed = any(num_pos == 0)) %>% 
  filter(missed ==F)

spp_to_include <- species_tallies$spp


raw <- raw %>% 
  filter(spp %in% spp_to_include) %>% 
    mutate(vessel = as.factor(vessel),
         spp = as.factor(spp))

test_all <-
  vasterize_index(
  raw_data = raw,
  run_dir = run_dir,
  nstart = 100,
  region = 'California_current',
  n_x = 1000
  )
```
OK I think D_xcy is the multispecies output

Now need to figure out how you actually interpret and get the multi-species outputs

The key is in the extraction step of `PlotResultsOnMap_Fn.r`

# Test Model Fit

go back to your old friend alaska pollock as a test. 

```{r}

alaska_pollock <- fish_data %>% 
  filter(survey == 'EBSBTS', Sci == 'Gadus_chalcogrammus') %>%  dplyr::rename(
    Lon = Long,
    Catch_KG = Wt,
    spp  = Sci
  ) %>%
  mutate(AreaSwept_km2 = 1 , Vessel = 'missing') %>% 
  select(Year, Lat, Lon, Vessel, AreaSwept_km2, Catch_KG,spp) %>% 
  set_names(tolower(colnames(.))) %>% 
  as.data.frame() %>% 
  na.omit() %>% 
  filter(year >=2012) %>% 
    mutate(vessel = as.factor(vessel),
         spp = as.factor(spp))  #note that this breaks if it's a tibble, should make a bit more flexible

set.seed(53)
load(paste0(run_dir,'vast_alaska_pollock.Rdata'))

vast_alaska_pollock <-
  vasterize_index(
  raw_data = alaska_pollock,
  run_dir = run_dir,
  nstart = 100,
  region = 'Eastern_Bering_Sea',
  n_x = 100
  )

ggmap::qmplot(x = approx_long, y = approx_lat, data = vast_alaska_pollock$spatial_list$loc_x_lat_long, color = 'red')


save(file = paste0(run_dir,'vast_alaska_pollock.Rdata'),vast_alaska_pollock)


```

Now one question is what the hell CRS to use, since I'm still really confused by how to actually use UTM coordinates. 

OK! that works, you're up and running now. Now, for the implementation of all this, I want to be able to do two things. easily move from lat-lon of more normal data to the UTM coordinates  of the knots, for "nearest neighbor" calculations, and easily move back to lat-lon for easier plotting and integration with other data systems. 

The stored eastings and northings from SDGLMM are "flipped" in some instances, and so aren't techincally right. You build a new function in there called convert_utm_to_ll_fn that deals with that, and now store the actual lat-longs of the knot coordinates. 

Now the good news is the `nn2` doesn't really require any units, so you can define nearest neighbor by lat-lon, since I don't really get how to deal with the trans-zone thing otherwise. 

Now that you've got this going,  let's build the database by merging GFW with 
FishData

```{r build-database}

ebs_trawlers <- gfw_data %>% 
  filter(survey == "ebsbts_gfw") %>% 
  unnest() %>% 
  filter(inferred_label_allyears == 'trawlers')

skynet_data <- ebs_trawlers %>%
  group_by(year, rounded_lat, rounded_lon) %>% 
  summarise(num_vessels = length(unique(mmsi)),
            total_hours = sum(total_hours),
            total_engine_hours = sum(total_hours * inferred_engine_power),
            dist_from_port = mean(mean_distance_from_port),
            dist_from_shore = mean(mean_distance_from_shore),
            mean_vessel_length = mean(inferred_length),
            no_take_mpa = any(is.na(mpant) == F),
            restricted_use_mpa =  any(is.na(mparu) == F)
            ) %>% 
  ungroup() %>%
  mutate(
    vessel_hours = total_engine_hours * num_vessels,
    any_fishing = total_engine_hours > 0) %>%
  group_by(rounded_lat, rounded_lon) %>%
  arrange(year) %>%
  mutate(
    total_engine_hours_lag1 = lag(total_engine_hours, 1),
    total_engine_hours_lag2 = lag(total_engine_hours, 2),
    total_engine_hours_lag3 = lag(total_engine_hours, 3),
    port_engine_hours = dist_from_port * total_engine_hours,
    port_hours = dist_from_port * total_hours,
    port_numbers = dist_from_port * num_vessels,
    shore_numbers = dist_from_shore * num_vessels,
    shore_hours = dist_from_shore * total_hours,
    shore_engine_hours = dist_from_shore * total_engine_hours,
    large_vessels = num_vessels * mean_vessel_length
  ) 

# gfw_data <- skynet_data

fish_data <- vast_alaska_pollock$spatial_densities

fish_knots <- vast_alaska_pollock$spatial_list$loc_x_lat_long


skynet_data <-
create_skynet_data(
gfw_data = skynet_data,
fish_data = fish_data,
fish_knots = fish_knots,
surveys = 'EBSBTS',
topo_data = topo_data,
wind_data = wind_data,
chl_data = chl_data,
wave_data = wave_data,
sst_data = sst_data
)

```

Booya, you're back in business.  now

## Interpolate environmental data

Environmental data doesn't seem to match up perfectly with the other data; you get a lot of missing entries. Let's try and fill in the gaps a bit, though obviuosly you'll need to test sensitivity of results to these choices. 

So, for example, year 2012, rounded_lat = 53.75, rounded_lon = -166.25 is lestaed as NA for chl. What's going on there?


```{r}

a <- wave_data %>% 
  filter(survey == 'EBSBTS') %>% 
  select(survey, wave_height ) %>% 
  unnest()


 skynet_data %>% 
   group_by(rounded_lat, rounded_lon) %>% 
   summarise(percent_missing = mean(is.na(wind_speed))) %>% 
   ggplot(aes(rounded_lon, rounded_lat, fill = factor(percent_missing))) + 
   geom_tile()

  b <- skynet_data %>% 
   group_by(rounded_lat, rounded_lon) %>% 
   summarise(percent_missing = mean(is.na(mean_altitude)),
             th = sum(total_hours)) 
  
  ggmap::qmplot(rounded_lon, rounded_lat, color = percent_missing, data = b)
 
ggmap::qmplot(rlon, rlat, color = mean_chlorophyll, data = a )

```

Aha, so part of the problem is simply a bounding box problem

sst is bounding box

Wave data is just a different resolution



## prepare model

The goal here is a tidy framework for running various models. 

A model will be defined by the independent variable, the dependent variable,  the data that go into it, and the structure of the model that is fit. 

So step one will be a function that takes the supplied data and prepares it for the model. 

From there, another function will take the data and the desired model form and run the model, returning a few useful outputs (the model, rmse, whatever)

So looking at the "decent" model fits that you got in run-skynet-poc.R

That model was using raw, not log, densities. No center and scaling of variables. 100 knots, not 1000. Filtered out places with total_engine_hours = 0, density = 0, is.na(density) = F. 



```{r}

skynet_names <- colnames(skynet_data)

do_not_scale <- c('year','rounded_lat','rounded_lon','no_take_mpa', 'restricted_use_mpa','knot','distance','any_fishing') # variables that should not be centered and scaled

do_scale <- skynet_names[ !skynet_names %in% do_not_scale]

skynet_data <- skynet_data %>%
  filter(total_hours > 0) %>% 
  ungroup() %>%
 purrrlyr::dmap_at(do_scale, ~ (.x - mean(.x, na.rm = T)) / (2 * sd(.x, na.rm = T))) #center and scale variables that should be

dep_var <- c('log_density')

ind_vars <-
  c(paste(
  skynet_names[!skynet_names %in% c(dep_var,
  'year',
  'rounded_lat',
  'rounded_lon',
  'knot',
  'distance')], collapse = '+'),
  paste(
 c('total_hours' ,
    'large_vessels' ,
    'port_hours' ,
    'port_engine_hours' ,
    'port_numbers' ,
    'shore_hours' ,
    'shore_engine_hours' ,
    'shore_numbers' ,
    'dist_from_port' ,
    'dist_from_shore' ,
    'num_vessels' ,
    'mean_vessel_length'),
 collapse = '+'
  ),
   paste(
 c(
    'large_vessels' ,
    'port_numbers' ,
    'dist_from_port' ,
    'dist_from_shore' ,
    'mean_vessel_length'),
 collapse = '+'
  )
  )

models <- 'rf'

test_train_data <- modelr::crossv_kfold(skynet_data, k = 2)

skynet_models <-  purrr::cross_df(list(
  dep_var = dep_var,
  ind_vars = list(ind_vars[[2]]),
  temp = list(test_train_data),
  model = models
)) %>% 
  unnest(temp, .drop = F) %>% 
  mutate(ind_vars = map(ind_vars, unlist))


```

Booya, now let's try fitting...

```{r}

sfm <- safely(fit_skynet)

skynet_models <- skynet_models %>%
  mutate(fitted_model = pmap(
  list(
  dep_var = dep_var,
  ind_vars = ind_vars,
  model = model,
  training = train, 
  testing = test
  ),
  sfm,
  fitcontrol_number = 10,
  fitcontrol_repeats = 10))

wee <- map(skynet_models$fitted_model,'result')

a <- wee[[1]]

a$model$finalModel

arg <- skynet_models$test[[1]] %>% 
  as_data_frame() %>% 
  na.omit() %>% 
  ungroup() %>% 
  mutate(ld_hat = predict(a$model$finalModel, newdata = .))

arg %>% 
  ggplot(aes(log_density, ld_hat)) + 
  geom_point() + 
  geom_abline(aes(intercept = 0, slope = 1))


huh <- a$model$trainingData %>% 
  mutate(ld_hat = a$model$finalModel$predicted)

huh %>% 
  ggplot(aes(.outcome, ld_hat)) + 
  geom_point() + 
  geom_abline(aes(intercept = 0, slope = 1))


```


And you have a working model! Let spend some time now reminding yourself and writing down what the actual diagnostics should be here. 

```{r}
wtf <- huh %>% 
  summarise(r2 = 1 - (sum((.outcome - ld_hat)^2) / sum((.outcome - mean(.outcome))^2)),
            mse = mean((.outcome - ld_hat) ^2 ),
            vary = var(.outcome)
            ) %>% 
  mutate(r22 = 1 - mse / vary)

R2 <- 1 - (sum((actual-predicted)^2)/sum((actual-mean(actual))^2))

r22 <- 1 - (last(a$model$finalModel$mse) / var(huh$.outcome))


wtf <- lm(mpg ~ hp, data = mtcars)

mse <- modelr::rmse(wtf, mtcars)^2

vary <- var(mtcars$mpg)

1 - mse / vary

```

```{r}

set.seed(131)

    fit_control <- trainControl(method = 'repeatedcv',
                                number = 5,
                                repeats = 5)

ozone.rf <- caret::train(Ozone ~.,
             data = airquality, 
             na.action = na.omit, 
             tnControl = fit_control, 
             do.trace = T)

ozone.rf <- randomForest(Ozone ~ ., data=airquality, mtry=3,
                         importance=TRUE, na.action=na.omit)

ozone.rf$rsq


```



## fit models 

Fit a series of models on the data at different aggregation levels

## Diagnose models










