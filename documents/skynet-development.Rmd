---
title: "Skynet Development"
output: html_notebook
---

This is a sketchpad for developing the tools and code needed to evaluate project skynet. As much as possible, use functions to do all the critical steps that you will then call into use for the formal run script. Use this space to think outloud and examine/debug outputs of scripts. 

# Load things

```{r}
set.seed(42)
knitr::opts_chunk$set(message = F, warning = F)
library(bigrquery)
library(lubridate)
library(tmap)
library(leaflet)
library(rgdal)
library(FishData)
library(ggmap)
library(tidyverse)
library(stringr)
library(viridis)
library(purrr)
library(trelliscopejs)

# demons::load_functions('../functions')

run_name <- 'testing'

run_description <- 'development of full paper'

run_dir <- file.path('../results', run_name, '')

if (dir.exists(run_dir) == F) {
  dir.create(run_dir)
}

write(run_description, file = paste0(run_dir, 'description.txt'))


```

# Build databases

This section will be used to devlop functions to get Fishdata and GFW data, and compile the "final" databases that you will use. 

## FishData

Let's start with getting the FishData, since you'll use the locations from FishData to decide which places you want to aggregate and scrape from GFW. 

Let's focus on getting the compelte database up and running now, so that the actual script can have an option to either load the compiled data from box, or to reload it from scratch. 

`download_catch_rates` is the function you need from FishData, the only problem is that it's a little unclear what the potential list of surveys are. 

Looking through FishData, seems like Alaska is really what you have available to you. SOAB. You're going to need some non-alaska places, but at least you also do have West Coast. There is also some irish data availabe in `download_datras`

```{r}

fishdata_names <- tribble(
  ~survey, ~survey_name,
  #--/--
  'EBSBTS', 'eastern bering sea bottom trawl survey',
  'WCGBTS','west coast groundfish bottom trawl survey',
  'WCGHL', 'West_coast_groundfish_hook_and_line',
  'GOABTS', 'gulf of alaska bottom trawl survey',
  'AIBTS', 'aluetian islands bottom trawl survey'
)

```


Note: the error_tol is a bit finicky. Somewhere in the missing zeros thing it's messing up the addition of new things by a few decimal places. Change to 1e-2 and that seems to do it, off by a few decimals, but not really a problem. 

```{r, eval = F}

fish.data <- fishdata_names %>% 
  mutate(survey_data = map(survey, ~download_catch_rates(.x, species_set = 50,
                           error_tol = 1e-2)))

fish.data <- unnest(fish.data)
  
save(file = '../data/fish_data.Rdata', fish.data)
  
```

OK, that gives you the data that you need across all the surveys. Now, you need to get the GFW data that fall under those coordinates.... This should be fun

Let's make some plots just to check your data a bit


```{r}
load("../data/fish_data.Rdata")

fish.maps <- fish.data %>% 
    set_names(tolower(colnames(.))) %>% 
  mutate(rlat = round(lat, 2),
         rlon = round(long,2)) %>% 
  group_by(survey, year, sci, rlat, rlon) %>% 
  summarise(mean_density = mean(wt, na.rm = T)) %>% 
  group_by(survey, year, rlat, rlon) %>% 
  summarise(total_mean_density = sum(mean_density)) %>% 
  na.omit()

mapfoo <- function(dens){
  
  qmplot(
  x = rlon,
  y = rlat,
  data = dens,
  color = log(total_mean_density),
  maptype = 'toner-lite',
  source = 'google'
) +
  scale_color_viridis(guide = F) +
  facet_wrap( ~ year) +
  theme_classic() + 
  labs(caption = 'Total Mean Species Density')
  
}


a <- fish.maps %>% 
  ungroup() %>% 
  filter(rlon < 0,year >=2012) %>% 
  nest(-survey, .key = densities) %>% 
  mutate(density_map = map_plot(densities, mapfoo))

trelliscope(a, name = 'blah', panel_col = 'density_map', self_contained = F)
  
  

```

Great, that all looks good (finally), though there is something a little off with the AIBTS data on the other side of the international date line... should examine that a little later. 

## GFW data

Now what you need are the bounding boxes for each of the regions, to provide a bit of constraint to your GFW queries. 

```{r}

survey.bbox <- fish.data %>% 
  filter(Long <0) %>% 
  group_by(survey) %>% 
  summarise(min_year = min(Year, na.rm = T),
            max_year = max(Year, na.rm = T),
            min_lat = min(Lat, na.rm = T),
            max_lat = max(Lat, na.rm = T),
            min_lon = min(Long, na.rm = T),
            max_lon = max(Long, na.rm = T))

survey.bbox

survey_names <- paste0(fishdata_names$survey %>% tolower(),'_gfw')

```

OK I went into google bigquery and set up a series of queries that produce tables of the form *<survey>_gfw*, and saved them under the skynet project in our bigquery account. Let's try and grab those things now. 

```{r, eval = F}


gfw.data <- data_frame(survey = survey_names)

project <- "ucsb-gfw"

fishing_connection <- DBI::dbConnect(bigrquery::dbi_driver(),
                         project = project,
                         dataset = 'skynet')

gfw.data <- gfw.data %>% 
  mutate(gfw.data = map(survey, ~fishing_connection %>%
  tbl(.x) %>%
  collect(n = Inf) %>%
  select(-b_mmsi,-b_year) %>%
  set_names(str_replace_all(colnames(.), '(a_)|(b_)', '')) %>%
  filter(is.na(on_fishing_list_nn) == F) %>%
  arrange(year, mmsi)

))

save(file = '../data/gfw_data.Rdata', gfw.data)

```


Examining the data a bit to make sure that there's some rhyme and reason to the data

```{r}

load(file = '../data/gfw_data.Rdata')

gfw_plot_foo <- function(dat){
  
  mapdat <- dat %>% 
    group_by(year, rounded_lat, rounded_lon) %>% 
    summarise(fishing_hours = sum(total_hours, na.rm = T)) %>% 
    ungroup() %>% 
    rename(rlon = rounded_lon, rlat = rounded_lat)

  
   qmplot(
  x = rlon,
  y = rlat,
  data = mapdat,
  color = log(fishing_hours),
  maptype = 'toner-lite',
  source = 'google'
) +
  scale_color_viridis(guide = F) +
  facet_wrap( ~ year) +
  theme_classic() + 
  labs(caption = 'Total Fishing Hours')
  
  
}

gfw.data <- gfw.data %>% 
  mutate(plots = map_plot(gfw.data, gfw_plot_foo))


trelliscope(
  gfw.data,
  name = 'blah',
  panel_col = 'plots',
  self_contained = F
  )
  

```

Ok, working the 0.25 degree unit seems to work nicely. Those GFW data are saved. 

## SST



```{r, eval = F}

query_erdap <- function(bbox, years = 2012:2016,
                        variable = 'Chl-a'){
  
 erdap.enviro <- download_ERDDAP(Lat_bounds = c(min = floor(bbox$min_lat), max = ceiling(bbox$max_lat)), Lon_bounds = c(min =
  floor(bbox$min_lon), max = floor(bbox$max_lon)), Date_bounds = c(min = "06-01", max = "10-01"),
  Year_set = years, Variable = variable, by = c(Lat = 1, Lon = 1, Day = 1))
 
 # download_ERDDAP(Lat_bounds = c(min = 53, max = 66), Lon_bounds = c(min =
 #  -180, max = -157), Date_bounds = c(min = "06-01", max = "10-01"),
 #  Year_set = 2009, Variable = "Chl-a", by = c(Lat = 1, Lon = 1, Day = 1))


  
}

chl.data <- survey.bbox %>% 
  slice(1) %>% 
  mutate(sst = pmap(list(min_year = pmax(2012,min_year), 
                         max_year = max_year,
                         min_lat = min_lat,
                         max_lat = max_lat, 
                         min_lon = min_lon,
                         max_lon = max_lon
                         ), query_erddap))

chl.data$sst[[1]] %>% View()


chl_plot_foo <- function(dat){


   qmplot(
  x = rlon,
  y = rlat,
  data = dat,
  color = log(mean_analysed_sst),
  maptype = 'toner-lite',
  source = 'google'
) +
  scale_color_viridis(guide = F) +
  facet_wrap( ~ year) +
  theme_classic() + 
  labs(caption = 'SST(c)')
  
  
}

chl.data <-  chl.data %>% 
  mutate(plots = map_plot(sst, chl_plot_foo))
  
trelliscope(
  chl.data,
  name = 'blah',
  panel_col = 'plots',
  self_contained = F
  )
  
chl.data$sst[[1]] %>% 
  group_by(year) %>% 
  mutate(mt = mean(mean_analysed_sst)) %>% 
  ggplot(aes(x = mean_analysed_sst, y = factor(year), group = factor(year), fill = (mt))) + 
  geom_joy() + 
  labs(c = 'Degrees C') +
  scale_fill_viridis(option = 'C', name = 'Mean Temperature (C)') + 
  hrbrthemes::theme_ipsum()




save(file = '../data/bad_chl_data', chl.data)


```



# Fit Models

Fit a series of models on the data at different aggregation levels

# Diagnose models










