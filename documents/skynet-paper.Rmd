---
title: "Fish from Space"
author: "Dan Ovando"
output:
  bookdown::html_document2: null
  bookdown::pdf_document2: null
  html_document: default
linkcolor: blue
bibliography: dissertation.bib
biblio-style: apalike
---

```{r, include=F}
knitr::opts_chunk$set(message = F, warning = F, echo = F ,
                      fig.width = 10, fig.height = 8, eval = T, cache = F)
```

```{r}

library(gbm)
library(randomForest)
library(stringr)
library(modelr)
library(broom)
library(hrbrthemes)
library(viridis)
library(sf)
library(ggmap)
library(caret)
library(extrafont)
library(tidyverse)


# run options -------------------------------------------------------------

run_name <- 'testing'

run_dir <- here::here('results',run_name)

fig_theme <- theme_ipsum(base_size = 14, axis_title_size = 18)

theme_set(fig_theme)

load(here::here("results", run_name,'skynet_data.Rdata'))

load(here::here("results", run_name,'processed_skynet_models.Rdata'))

load(here::here("data","vast_fish.Rdata"))

load(file = here::here("data","global_map.Rdata"))

load(file = here::here('data','fish_prices.Rdata'))

load(file = here::here('results',run_name,'gfw_data.Rdata'))

map_size <- 1
```


# Introduction

Effective fisheries management requires some ability for managers and stakeholders to estimate and react to the abundance of fish in the ocean. The history of fisheries science is largely concerned with developing and improving our ability to accomplish this difficult task, starting from early models of growth overfishing and leading up to multi-species bio-geo-economic models. While the field has made dramatic advances in our ability to assess fisheries, by and large we have found two solutions to this problem: Fit highly complex integrated statistical models to diverse data streams, or utilize increasing levels of statistical wizardry to try and squeeze more information out of limited data stream. This later practice has been both promising and concerning. The majority of fisheries in the world lack the resources for fully integrated stock assessments, and so depend on this world of "data limited stock assessment". While there has been tremendous growth in this field, a rough survey of the literature reveals that while the statistical methods have changed, nearly all DLAs rely on the same streams of information that would have been available to a fisheries scientist in the 1800s: lengths, captures, and catch per unit effort.

It is high time that we expanded not only the statistical methods of fisheries, but also the actual sources of data available for monitoring fish populations. GFW.... etc. 

This paper rests on a simple hypothesis: all else equal, fishermen would rather fish where there are more fish. Following this logic then, the amount and location of fishing effort observed must be in some way reflective of the abundance of fish at those locations at that time. To that end, we could simply use GFW to observe where fishermen are, and by extension estimate the relative change in abundance and distribution of fish in response to changes in the among and location of fishing effort. Unfortunately, life is not so kind. A vast array of other factors influence where and how much fishermen choose to fish, e.g. cost, local knowledge, past experience, and safety. The goal of this project then is to develop a model that estimates the abundance of fish as a function of observed fishermen behavior, controlling for other factors that influence the decision of where, when, and how much to fish. 


# Methods

## Data

Primary data were collected from two sources: Global Fishing Watch (GFW) provides our data on the amount and location of fishing effort, along with available excoriates such as vessel size, distance from shore/port, etc. Estimates of fish abundance in space and time were obtained from their relevant surveys through the `FishData` package. The following two data sections provide summaries of the data as well as descriptions of relevant data processing steps taken. 

### Survey Data

`FishData` provides access to numerous fishery independent research surveys throughout the world. For now, we focus on the surveys conducted along the west coast of North America

  - Eastern Bering Sea Bottom Trawl Survey (EBSBTS)
  - Aleutian Islands Bottom Trawl Survey (AIBTS)
  - Gulf of Alaska Bottom Trawl Survey (GOABTS)
  - West Coast Groundfish Bottom Trawl Survey (WCBTS)
  - West Coast Groundfish Hook and Line Survey
  
See Fig.\@ref(fig:fish-map) for map of available survey data

```{r fish-map, fig.cap='Estimated knot locations for each fishery independent survey'}

knots <- vast_fish %>%
  select(survey, vasterized_data) %>%
  mutate(knots = map(vasterized_data, c('spatial_list', 'loc_x_lat_long'))) %>%
  select(-vasterized_data) %>%
  unnest() %>%
  mutate(recenter_lon = ifelse(approx_long < 0, 180 + (180 - abs(approx_long)), approx_long))
  
  pacific_map <- global_map %>%
  as("Spatial") %>%
  maptools::nowrapRecenter() %>%
  sf::st_as_sf()
  
  
  map_knots <-  knots %>%
  dplyr::mutate(geometry = purrr::map2(recenter_lon, approx_lat, ~ sf::st_point(x = c(.x, .y), dim = 'XY'))) %>%
  ungroup() %>%
  mutate(geometry = sf::st_sfc(geometry, crs =
  "+proj=longlat +datum=WGS84 +no_defs")) %>%
  sf::st_sf()
  
  
  bbox <- sf::st_bbox(map_knots)
  
  knot_map <- map_knots %>%
  ggplot() +
  geom_sf(data = pacific_map, fill = 'grey60') +
  geom_sf(aes(color = survey), size = 1, alpha = 0.5) +
  coord_sf(xlim = c(bbox['xmin'], bbox['xmax']),
  ylim = c(bbox['ymin'], bbox['ymax'])) +
  scale_color_viridis_d()
  
  knot_map
      
```


Each of the surveys contains a variety of different species and time series of coverage

```{r}

fish_summary <- vast_fish %>% 
  select(survey, data) %>% 
  unnest() %>% 
  group_by(survey, spp, year) %>%
  summarise(mean_catch = mean(catch_kg, na.rm = T)) %>% 
  group_by(survey, spp) %>% 
  mutate(scaled_catch = (mean_catch - mean(mean_catch, na.rm = T))) %>% 
  ungroup()

fish_summary %>% 
  ggplot(aes(year, scaled_catch, color = spp)) + 
  geom_path(show.legend = F) + 
  facet_wrap(~survey, scales = 'free_y')

```



```{r}
fish_summary <- vast_fish %>% 
  select(survey, data) %>% 
  unnest() %>% 
  group_by(survey, spp, year) %>% 
  summarise(total_catch = sum(catch_kg),
            n = length(catch_kg))

fish_summary %>% 
  ggplot(aes(year, total_catch, fill = spp)) + 
  geom_col(show.legend = F) + 
  facet_wrap(~survey, scales = 'free_y')


```


Survey data come in their "raw" form, which requires standardization to account for differences in vessel characteristics, spatio-temporal correlation structures, and the presence of zeros. This standardization was performed using the `VAST` package, which implements a spatial delta-generalized linear mixed model across multiple species. The result of the VAST process provides standardized estimates of species abundance in space and time. 

Surveyed species also varied substantially in their abundance and economic importance. We mark species encountered in the surveys as "fished" if their name, or a synonym for their name identified through the `taxize` package in R, was found with the @FAOXX catch records. For each species, we also obtained price estimates using @Melnychuck2016 as well (Fig. \@ref(fig:price-plot)). 


```{r}

species_prices %>%
  filter(is.na(mean_exvessel_price) == F) %>% 
  mutate(species = fct_reorder(species, mean_exvessel_price))  %>% 
  ggplot(aes(species, mean_exvessel_price)) +
  geom_col() + 
  scale_y_continuous(labels = scales::dollar) + 
  coord_flip() + 
  theme(axis.text.y = element_text(size = 8))

```

### GFW Data

GFW data were obtained using `bigrquery` from the GFW servers. Data were aggregated to the resolution of year and nearest 0.25 degree lat/long, and for each vessel at this resolution we calculated the total fishing hours spent, average distance from shore, average distance from port, and whether that location is inside an MPA (and if so what kind). We also collected relevant data for that vessel such as its engine power, length, tonnage, and vessel type. 


```{r gfw-map, fig.cap="Map of scraped GFW effort data"}
gfw_effort <- gfw_data %>%
  unnest() %>% 
  group_by(year, rounded_lat, rounded_lon) %>% 
  summarise(total_effort = sum(total_hours)) %>% 
  mutate(recenter_lon = ifelse(rounded_lon < 0, 180 + (180 - abs(rounded_lon)), rounded_lon)) %>% 
  filter(total_effort > 0)
  
  
  
  pacific_map <- global_map %>%
  as("Spatial") %>%
  maptools::nowrapRecenter() %>%
  sf::st_as_sf()
  
  
  gfw_map <-  gfw_effort %>%
  dplyr::mutate(geometry = purrr::map2(recenter_lon, rounded_lat, ~ sf::st_point(x = c(.x, .y), dim = 'XY'))) %>%
  ungroup() %>%
  mutate(geometry = sf::st_sfc(geometry, crs =
  "+proj=longlat +datum=WGS84 +no_defs")) %>%
  sf::st_sf()
  
  
  bbox <- sf::st_bbox(gfw_map)
  
  gfw_map <- gfw_map %>%
  ggplot() +
  geom_sf(data = pacific_map, fill = 'grey60') +
  geom_sf(aes(color = log(total_effort)), size = 1, alpha = 0.5) +
  coord_sf(xlim = c(bbox['xmin'], bbox['xmax']),
  ylim = c(bbox['ymin'], bbox['ymax'])) +
  scale_color_viridis()
  
  gfw_map
```



### Merged Data

The GFW data were merged to the survey data by 1) clipping the GFW to only include observations that occurred within the perimeter of one of the surveys and 2) snapping each GFW observation to the nearest knot identified by VAST from the survey data. Since the survey data are at a courser resolution than the GFW data, this means that multiple GFW observations are usually associated with any one knot at any one time. 

We can also filter the data to ensure model quality and convergence

  - Only fished species are included from the survey data (under the assumption that fishermen are not responding to abundance of unfished species such as starfish)
  
  - Since all surveys are geared towards bottom dwelling species, only bottom-associated gears (in this case trawlers, pots and traps, and set gears (longline and gillnet)) are included
  
  - We only include species that were observed 10 or more times during each year of the survey to improve model convergence
  
  - VAST provides estimates of abundance in years without surveys by predicting based on spatio-temporal patterns. In order to reduce potential bias of models estimating the VAST process instead of underlying abundance, for now we only include survey data from years in which a survey was actively conducted. 


### Environmental Covariates

We augment with the GFW and FishData data with globally accessible environmental covariates, including remote sensing estimates of 

  - Chlorophyll 
  - Sea surface temperature
  - Wave height
  - Wind velocity
  - Bathymetry
  
All data were obtained from NOAA, and aggregated as needed to match the resolution of the GFW data (annual and .25 degrees). 


## Model

We can of course construct a wide array of candidate models that relate fish abundance and relevant covariates to fishing decisions, from simple distance-from-shore based cost corrections, to highly specified models of capital investment and knowledge accrual. The relevant question then is how do we test the performance of these alternative models? We accomplish this by pairing GFW data with fishery independent survey data (e.g. the West Coast Groundfish Bottom Trawl Survey) available from `FishData`XX. These data provide annual indices of abundance in space and time for a wide range of commercially important species.  

By pairing these data then, we can compare the ability of candidate models to use GFW data (and other globally available data, such as estimates of wind, waves, and productivity) to predict relevant abundance data available from FishData. As the ultimate goal of this model is to use GFW to predict abundance indices in places where fishery independent samples are not available,we will use out-of-sample predictive power as our metric of model performance, specifically root-mean squared error (RMSE) of data held out of the training of the model.

We fit a series of candidate models to the data, defined in terms of 

- Data Splitting: the data used for training and testing. 
    - Random splits (train on a random subset, predict the remaining subset)
    - Regional splits (e.g. train on eastern Bering sea, predict US west coast)
    - Temporal splits (train on 2010-2012, predict 2013-2014)

- Predictive goal:
    - Absolute abundance in space in time
    - Changes in abundance in space and time
  
- Model Mechanics: 
    - Machine learning (random forests, gradient boosted models)
    - Linear regression (mostly as a simplistic example)
    - Structural modeling (fitting of structural economic models, e.g. ideal free distribution)
    

We fit factorial combinations of each of these components, allowing us to evaluate model performance from a variety of relevant angles. The comparison between the machine learning and structural modeling approaches are worth discussing further for a brief moment. While different in their mechanics, all the candidate machine learning models are black box models whose sole objective is to maximize the predictive power of the model. The user specifies some model options, but the model decides which data are important and how those data relate to each other. This allows these algorithms to fit highly non-linear models (if the data demand it), without the need to specify exactly how factors such as costs, safety, and abundance interact to affect fishing behavior. As a result, machine learning models can serve as an effective benchmark for the best possible ability of GFW data to predict fish abundance. The disadvantage is that, while new techniques are emerging for interpreting machine learning model fits, they are inherently black boxes and as such do not permit us to really interpret the meaning of specific coefficients. 

This stands in contrast to the structural modeling approach, in which we write out a functional form for the relationship between between GFW data, and the fit that specified model to the data. This has the advantage of interpretability, but opens us up to errors in model specification. By running both, we can compare the machine learning and structural approach and see how much the interpretability of the structural model "costs" us in terms of predictive power, relative to the benchmark of the machine learning model. 

Each model was designed using a separate subset of data, so as to isolate  initial model design from the data that will eventually be used to judge model performance. 

Below I provide high-level summaries of each of the models, purely to aid in understanding (more detailed descriptions will be provided in future drafts)

## Machine Learning

Two machine learning algorithms, random forests (rf) and generalized boosted regression modeling (gbm), were evaluated. Tuning parameters of each method were selected using the `caret` package on an isolated set of data, and since the random forest performed better than the gbm model, for brevity's sake I will only present results of the random forest here. 

A random forest works by fitting a series of regression trees. Each regression tree takes a sub-sample of the training data, and a sub sample of the independent variables provided for model fitting. The algorithm then determines the variable and variable split (e.g. vessel size and vessel size above 30ft) that provides the greatest explanatory power in estimating density of the "out of bag" samples (the part of the training data that were not included in the tree), and creates that as the first node. The next two nodes are selected in the same process, and so on and so forth, down to a specified tree depth tuned through the `caret` package. Each tree provides a high-variance, low bias estimator of densities. The random forest then averages over hundreds of trees to reduce this variance and provide an improved estimate of density as a function of provided covariates. The advantage of this approach is that it makes no assumptions about error distributions or linearity of parameters, and actively pushes back against over fitting by prioritizing out-of-sample prediction, and sub sampling of the provided independent variables (lots and lots of literature on this). 

## OLS

As a counterbalance to the mysteries of the random forest algorithm, I also fit a simple linear OLS model to the data. Variable selection was made be backwards AIC selection (fitting the model with all parameters, dropping the least significant variable, refitting the model, repeat process until a minimum AIC value is reached). This is not a perfect model selection process, but again this serves simply to attempt to compare a reasonably tuned linear model to a reasonably tuned random forest. Though I will note that I did not include interaction terms at this time. 

## Structural 

Lastly, we fit a structural model in the form of @Miller2016 to the data. The key of this model assumption here is that fishermen conform to an ideal free distribution, and so marginal profits are equal across appropriate groups in space and time, at the unit of year and "knot", a knot being a geographic region of constant abundance estimated per the methods of @Thorson2016. 

Following @Miller2016, we consider marginal profits per unit effort as being 

$$ \pi_{y,k} = pCPUE_{y,k}e^{-qE_{y,k}} - c_{y,k}  $$

where for year *y* at knot *k*, *p* is price, CPUE is the index of abundance prior to any fishing effort occurring (our index of abundance, *q* is catchability, *E* is effort, and *c* are variable costs). 

@Miller2016 was primarily interested in estimating quota price aspects of *c*, taking as data *p*, *CPUE*, *E*, and other components of *c* (fuel, labor, ice, etc.). We are instead interested in estimating CPUE as a function of other variable, and so we can rearrange this equation as

$$ CPUE_{y,k} = \frac{\pi_{y,k} + c_{y,k}}{pe^{-qE_{y,k}}}$$

Similar to @Miller2016 we assume for now that $\pi_{y,k}$ is zero, though this is clearly not accurate given that many of the fisheries sampled by this model are highly regulated and in some cases rationalized (however, changing $\pi_{y,k}$ to positive values had little effect on the fit of the model). *p* is taken from @Melnychuk2016, and $E_{y,k}$ is observed from GFW. 

That leaves *q* and *c* as unknown parameters. While we do not have the high resolution logbook data available to @Miller2016, we could certainly obtain data on fuel and labor prices for this model. However, at this time, I simply assume that $c_{y,k}$ is a function of the distance of a knot *k* from port, the mean vessel size used at knot *k* in year *y*, and the interaction of these terms (under the assumption that cost of large vessels increase with distance). 

$$ c_{y,k} = \beta_{1}pd_{k} + \beta_{2}vs_{y,k} + \beta_{3}pd_{k}vs_{y,k} $$

Where $pd_{k}$ is the distance from port of knot *k* and $vs_{y,k}$ is the mean vessel length observed fishing at knot *k* in year *y*. 

We fit the model through maximum likelihood, per

$$[q, \beta_{1}, \beta_{2},\beta_{3} | log(CPUE_{y,k}()] \propto normal(log(CPUE_{y,k}) | f(q, \beta_{1}, \beta_{2},\beta_{3} ),\sigma)$$

Repeated initial values and a jittered while loop were used to ensure model convergence (though a full Bayesian implementation will improve on this at a later date). We could of course again include either informative priors on the $\beta$ parameters based on fuel and labor costs, or directly incorporate those data, but this provides a useful starting point. 

```{r, eval = F}

p <- 10
cpue <- 100
q <- .1
effort <- 0:100
cost <- 200

mp <- p * cpue * exp(-q*effort) - cost

plot(effort,mp)

```


# Results 

An organizing principle: 

Predict Space

Predict Abundance



## Figures

### Fig.1 Model performance at different resolutions

Paneled by test. 

To show at what resolutions the models work best for OOB prediction

### Fig. 2 Cody knock-off map

Map of the west coast with total CPUE trends to the left of it for each region, and cpue trend predictions by the model, with different colors for trained and predicted regions

### Fig.3 Map or something showing out-of-region performance, best model in all cases


## needs

## Notes

The factorial combinations of our data and models produces an ungainly number of results, and so for now I will highlight a few particularly interesting findings, specifically results for XX region, results for all regions, results from the temporal hold-out, both for the machine learning and structural modeling approaches. 

What's the structure of the paper

## Does more effort equal more fish?

This is Chris's point: at the simplest level, what can you get out of a linear regression of effort on biomass, and how does performance change with resolution?

Simplest figure: OOB RMSE at different spatial resolutions of this model

Find inflection point/point where the resolution gets so coarse that it's not useful but you've got an improvement in fit. That tells your benchmark "coarse" resolution model. 

Could also repeat similar breakout processes of different regions/spatial controls

## What about when we control for other things?

The simplest solution would be the results that you have now, if the above exercise turns out to be garbage. If there is a worthy strawman in the effort ~ fish relationship, then you could look at the performance of the finest resolution to the performance of at the inflection resolution above, and see how much of an improvement you get. 

## Select "best models" and show performance

Net CPUE trends by region, and GFW predicted model based on this, similar to Cody's CPUE map

Temporal prediction

Fill in the gap

Predict where we don't survey


# How good is good enough?

# Misc Results

## Correlations

We can examine simple correlations between our data and the estimated total density of fished species at a given knot in a given year (Fig. \@ref(fig:var-cor)). 

```{r var-cor, fig.cap= 'Smoothed visual correlations between covariates and total fished species density'}

skynet_data %>% 
  ungroup() %>% 
  dplyr::select(log_density, dist_from_port, total_hours,total_engine_power,mean_chlorophyll) %>% 
  gather('variable','value', -log_density) %>% 
  ggplot() + 
  geom_smooth(aes(value, log_density, color = variable), show.legend = F) + 
  labs(y = 'Total Density of Fished Species') +
  facet_wrap(~variable, scales = 'free') + 
  theme_minimal() +
  theme(axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        strip.text = element_text(size = 8),
        panel.spacing = unit(.1, 'lines'))
  

```


## In-location performance

```{r}

skynet_subset <- skynet_models %>% 
  select(model, data_subset, train_set,training_data, dep_var) %>%
  filter(dep_var == 'log_density')


skynet_subset_training <- skynet_subset %>% 
  select(model, data_subset,train_set, training_data) %>% 
  unnest() %>% 
  mutate(residuals = log_density - pred)

skynet_subset_training %>% 
  filter(train_set == 'random') %>% 
  ggplot(aes(log_density, pred, color = data_subset)) + 
  geom_point() + 
  geom_abline(aes(slope = 1, intercept = 0), color = 'red',linetype = 2) + 
  facet_grid(model ~ data_subset, scales = 'free_y')


```



## Train on Alaska Test on West Coast

```{r}

skynet_models <- skynet_models %>% 
  mutate(data_subset = ifelse(.$model == 'structural','skynet',data_subset)) %>% 
  filter(dep_var == 'log_density', model != 'gbm', data_subset == 'skynet')

```


```{r alaska-westcoast}

skynet_subset <- skynet_models %>% 
  filter(train_set == 'ebsbts')

skynet_subset_training <- skynet_subset %>% 
  select(model, data_subset, training_data) %>% 
  unnest() %>% 
  mutate(residuals = log_density - pred)

skynet_subset_testing <- skynet_subset %>% 
  select(model, data_subset, test_data) %>% 
  unnest() %>% 
  mutate(residuals = log_density - pred)
```



```{r alaska-ovp-plot, fig.cap='In-sample predicted vs observed log density for Alaska trained models'}

skynet_subset_training %>% 
  ggplot(aes(log_density, pred)) + 
  geom_point() + 
  geom_abline(aes(slope = 1, intercept = 0), color = 'red',linetype = 2) + 
  facet_grid(model ~ data_subset, scales = 'free_y')

```

Looking at spatial residuals


```{r alaska-residual-map, fig.cap='In-sample spatial residuals for Alaska trained models'}

skynet_map <- skynet_subset_training %>%
  group_by(model, data_subset, rounded_lon, rounded_lat) %>% 
  summarise(residuals = sum(residuals)) %>% 
  mutate(recenter_lon = ifelse(rounded_lon < 0, 180 + (180 - abs(rounded_lon)), rounded_lon)) %>% 
  ungroup()
  
  skynet_map <-  skynet_map %>%
  dplyr::mutate(geometry = purrr::map2(recenter_lon, rounded_lat, ~ sf::st_point(x = c(.x, .y), dim = 'XY'))) %>%
  ungroup() %>%
  mutate(geometry = sf::st_sfc(geometry, crs =
  "+proj=longlat +datum=WGS84 +no_defs")) %>%
  sf::st_sf()
  
  bbox <- sf::st_bbox(skynet_map)
  
  skynet_map <- skynet_map %>%
  ggplot() +
  geom_sf(data = pacific_map, fill = 'grey60') +
  geom_sf(aes(color = residuals), size = map_size, alpha = 0.5) +
  coord_sf(xlim = c(bbox['xmin'], bbox['xmax']),
  ylim = c(bbox['ymin'], bbox['ymax'])) +
  scale_color_gradient2(low = 'black',high = 'tomato', mid = 'steelblue', midpoint = 0) + 
  facet_grid(model ~ data_subset)
  
  skynet_map

```

We can also use the same models to predict completely out of-sample, in this case using the Alaska fit model to predict West Coast surveys. 

```{r alaska-ovp-testing, fig.cap='Observed vs predicted for data trained on Alaska and tested on West Coast'}

skynet_subset_testing %>% 
  ggplot(aes(log_density, pred)) + 
  geom_point() + 
  geom_abline(aes(slope = 1, intercept = 0), color = 'red',linetype = 2) + 
  facet_grid(model ~ data_subset, scales = 'free_y')

```



```{r alaska-residual-map-testing, fig.cap='Spatial residuals for models trained on Alaska and tested on West Coast'}

skynet_map <- skynet_subset_testing %>%
  # filter(model == 'rf') %>% 
  group_by(model, data_subset, rounded_lon, rounded_lat) %>% 
  summarise(residuals = sum(residuals)) %>% 
  mutate(recenter_lon = ifelse(rounded_lon < 0, 180 + (180 - abs(rounded_lon)), rounded_lon)) %>% 
  ungroup()
  
  skynet_map <-  skynet_map %>%
  dplyr::mutate(geometry = purrr::map2(recenter_lon, rounded_lat, ~ sf::st_point(x = c(.x, .y), dim = 'XY'))) %>%
  ungroup() %>%
  mutate(geometry = sf::st_sfc(geometry, crs =
  "+proj=longlat +datum=WGS84 +no_defs")) %>%
  sf::st_sf()
  
  bbox <- sf::st_bbox(skynet_map)
  
  skynet_map <- skynet_map %>%
  ggplot() +
  geom_sf(data = pacific_map, fill = 'grey60') +
  geom_sf(aes(color = residuals), size = map_size, alpha = 0.5) +
  coord_sf(xlim = c(bbox['xmin'], bbox['xmax']),
  ylim = c(bbox['ymin'], bbox['ymax'])) +
  scale_color_gradient2(low = 'black',high = 'tomato', mid = 'steelblue', midpoint = 0) + 
  facet_grid(model ~ data_subset)
  
  skynet_map

```

## Train on Random Test on Random

```{r random-random}

skynet_subset <- skynet_models %>% 
  filter(train_set == 'random') 

skynet_subset_training <- skynet_subset %>% 
  select(model, data_subset, training_data) %>% 
  unnest() %>% 
  mutate(residuals = log_density - pred)

skynet_subset_testing <- skynet_subset %>% 
  select(model, data_subset, test_data) %>% 
  unnest() %>% 
  mutate(residuals = log_density - pred)
```




```{r random-ovp-plot, fig.cap='Observer vs predicted for random subset of data'}

skynet_subset_training %>% 
  ggplot(aes(log_density, pred)) + 
  geom_point() + 
  geom_abline(aes(slope = 1, intercept = 0), color = 'red',linetype = 2) + 
  facet_grid(model ~ data_subset, scales = 'free_y')

```



```{r random-residual-map, fig.cap='Spatial residuals for data trained on a random subset'}

skynet_map <- skynet_subset_training %>%
  group_by(model, data_subset, rounded_lon, rounded_lat) %>% 
  summarise(residuals = sum(residuals)) %>% 
  mutate(recenter_lon = ifelse(rounded_lon < 0, 180 + (180 - abs(rounded_lon)), rounded_lon)) %>% 
  ungroup()
  
  skynet_map <-  skynet_map %>%
  dplyr::mutate(geometry = purrr::map2(recenter_lon, rounded_lat, ~ sf::st_point(x = c(.x, .y), dim = 'XY'))) %>%
  ungroup() %>%
  mutate(geometry = sf::st_sfc(geometry, crs =
  "+proj=longlat +datum=WGS84 +no_defs")) %>%
  sf::st_sf()
  
  bbox <- sf::st_bbox(skynet_map)
  
  skynet_map <- skynet_map %>%
  ggplot() +
  geom_sf(data = pacific_map, fill = 'grey60') +
  geom_sf(aes(color = residuals), size = map_size, alpha = 0.5) +
  coord_sf(xlim = c(bbox['xmin'], bbox['xmax']),
  ylim = c(bbox['ymin'], bbox['ymax'])) +
  scale_color_gradient2(low = 'black',high = 'tomato', mid = 'steelblue', midpoint = 0) + 
  facet_grid(model ~ data_subset)
  
  skynet_map

```


```{r random-ovp-plot-testing, fig.cap='Observed vs predicted for models trained on a random subset and tested on a hold-out subset'}

skynet_subset_testing %>% 
  ggplot(aes(log_density, pred)) + 
  geom_point() + 
  geom_abline(aes(slope = 1, intercept = 0), color = 'red',linetype = 2) + 
  facet_grid(model ~ data_subset, scales = 'free_y')

```



```{r random-residual-map-testing, fig.cap='Spatial residuals for model trained a random subset and tested on a random subset'}

skynet_map <- skynet_subset_testing %>%
  group_by(model, data_subset, rounded_lon, rounded_lat) %>% 
  summarise(residuals = sum(residuals)) %>% 
  mutate(recenter_lon = ifelse(rounded_lon < 0, 180 + (180 - abs(rounded_lon)), rounded_lon)) %>% 
  ungroup()
  
  skynet_map <-  skynet_map %>%
  dplyr::mutate(geometry = purrr::map2(recenter_lon, rounded_lat, ~ sf::st_point(x = c(.x, .y), dim = 'XY'))) %>%
  ungroup() %>%
  mutate(geometry = sf::st_sfc(geometry, crs =
  "+proj=longlat +datum=WGS84 +no_defs")) %>%
  sf::st_sf()
  
  bbox <- sf::st_bbox(skynet_map)
  
  skynet_map <- skynet_map %>%
  ggplot() +
  geom_sf(data = pacific_map, fill = 'grey60') +
  geom_sf(aes(color = residuals), size = map_size, alpha = 0.5) +
  coord_sf(xlim = c(bbox['xmin'], bbox['xmax']),
  ylim = c(bbox['ymin'], bbox['ymax'])) +
  scale_color_gradient2(low = 'black',high = 'tomato', mid = 'steelblue', midpoint = 0) + 
  facet_grid(model ~ data_subset)
  
  skynet_map

```


## Train on Past Test on Future



```{r past-present}

skynet_subset <- skynet_models %>% 
  filter(train_set == 'year_leq_than_2014', dep_var == 'log_density')

skynet_subset_training <- skynet_subset %>% 
  select(model, data_subset, training_data) %>% 
  unnest() %>% 
  mutate(residuals = log_density - pred)

skynet_subset_testing <- skynet_subset %>% 
  select(model, data_subset, test_data) %>% 
  unnest() %>% 
  mutate(residuals = log_density - pred)
```



```{r past-ovp-plot, fig.cap='In-sample predicted vs observed log density for past trained models'}

skynet_subset_training %>% 
  ggplot(aes(log_density, pred)) + 
  geom_point() + 
  geom_abline(aes(slope = 1, intercept = 0), color = 'red',linetype = 2) + 
  facet_grid(model ~ data_subset, scales = 'free_y')

```

Looking at spatial residuals


```{r past-residual-map, fig.cap='In-sample spatial residuals for Alaska trained models'}

  skynet_map <- skynet_subset_training %>%
    group_by(model, data_subset, rounded_lon, rounded_lat) %>% 
    summarise(residuals = sum(residuals)) %>% 
    mutate(recenter_lon = ifelse(rounded_lon < 0, 180 + (180 - abs(rounded_lon)), rounded_lon)) %>% 
    ungroup()
    
    skynet_map <-  skynet_map %>%
    dplyr::mutate(geometry = purrr::map2(recenter_lon, rounded_lat, ~ sf::st_point(x = c(.x, .y), dim = 'XY'))) %>%
    ungroup() %>%
    mutate(geometry = sf::st_sfc(geometry, crs =
    "+proj=longlat +datum=WGS84 +no_defs")) %>%
    sf::st_sf()
    
    bbox <- sf::st_bbox(skynet_map)
    
    skynet_map <- skynet_map %>%
    ggplot() +
    geom_sf(data = pacific_map, fill = 'grey60') +
    geom_sf(aes(color = residuals), size = map_size, alpha = 0.5) +
    coord_sf(xlim = c(bbox['xmin'], bbox['xmax']),
    ylim = c(bbox['ymin'], bbox['ymax'])) +
    scale_color_gradient2(low = 'black',high = 'tomato', mid = 'steelblue', midpoint = 0) + 
    facet_grid(model ~ data_subset)
    
    skynet_map

```



```{r past-ovp-testing, fig.cap='Observed vs predicted for data trained on past and tested on future'}

skynet_subset_testing %>% 
  ggplot(aes(log_density, pred, color = factor(year))) + 
  geom_point(alpha = 0.5) + 
  geom_abline(aes(slope = 1, intercept = 0), color = 'red',linetype = 2) + 
  facet_grid(model ~ data_subset, scales = 'free_y')

```



```{r past-residual-map-testing, fig.cap='Spatial residuals for models trained on past and tested on future'}

skynet_map <- skynet_subset_testing %>%
  group_by(model, data_subset, rounded_lon, rounded_lat) %>% 
  summarise(residuals = sum(residuals)) %>% 
  mutate(recenter_lon = ifelse(rounded_lon < 0, 180 + (180 - abs(rounded_lon)), rounded_lon)) %>% 
  ungroup()
  
  skynet_map <-  skynet_map %>%
  dplyr::mutate(geometry = purrr::map2(recenter_lon, rounded_lat, ~ sf::st_point(x = c(.x, .y), dim = 'XY'))) %>%
  ungroup() %>%
  mutate(geometry = sf::st_sfc(geometry, crs =
  "+proj=longlat +datum=WGS84 +no_defs")) %>%
  sf::st_sf()
  
  bbox <- sf::st_bbox(skynet_map)
  
  skynet_map <- skynet_map %>%
  ggplot() +
  geom_sf(data = pacific_map, fill = 'grey60') +
  geom_sf(aes(color = residuals), size = map_size, alpha = 0.5) +
  coord_sf(xlim = c(bbox['xmin'], bbox['xmax']),
  ylim = c(bbox['ymin'], bbox['ymax'])) +
  scale_color_gradient2(low = 'black',high = 'tomato', mid = 'steelblue', midpoint = 0) + 
  facet_grid(model ~ data_subset)
  
  skynet_map

```



## More things

The random forest allows us to classify each variable by its "importance", a measure of the degree to which the fit of the model decreases when that variable is omitted from the model. 

(Fig. \@ref(fig:var-imp)). 

Plot of variable importance 

```{r}

ml_models <- skynet_models %>% 
  filter(model %in% c('rf','gbm')) %>%
  mutate(varimp = map(results, ~varImp(.x$finalModel) %>% as.data.frame() %>% mutate(variable = rownames(.))))

varimp <- ml_models %>% 
  select(model, data_subset,varimp) %>% 
  unnest() %>% 
  group_by(variable) %>% 
  mutate(mean_importance = mean(Overall)) %>% 
  arrange(desc(mean_importance)) %>% 
  ungroup() %>% 
  mutate(variable = fct_reorder(variable, mean_importance)) %>% 
  filter(model == 'rf')


varimp %>% 
  ggplot() + 
  geom_boxplot(aes(variable, Overall, fill = model)) + 
  coord_flip() +
  facet_wrap( ~ data_subset)




```


Plot of r2/mse by model type

```{r}

skynet_models %>% 
  filter(data_subset == 'skynet') %>% 
  ggplot(aes(pmax(0,psuedo_r2_training), fill = model)) + 
  geom_histogram(show.legend = F) + 
  facet_wrap(model~dep_var, scales = 'free') + 
  theme(axis.text.x = element_text(size = 8),
        axis.text.y = element_text(size = 8))

```

```{r}

skynet_models %>% 
  filter(model == 'rf', test_sets == 'historic')

skynet_models %>% 
    filter(data_subset == 'skynet') %>% 
  ggplot(aes(pmax(0,psuedo_r2), fill = model)) + 
  geom_histogram(show.legend = F) + 
  facet_wrap(model~dep_var, scales = 'free') + 
  theme(axis.text.x = element_text(size = 8),
        axis.text.y = element_text(size = 8))

```


Time series of abundance observed vs predicted


We can also construct partial dependence plots, which visualize the marginal effect of each independent variable (Fig. \@ref(fig:partial-plot)). 

```{r partial-plot, fig.cap='Partial dependence plots of variables in random forest', eval = F}

partial_foo <- function(variable) {
  
  eval(parse(text = paste0('a <- randomForest::partialPlot(skynet_models$results[[1]]$finalModel
, skynet_models$training_data[[1]] %>% as.data.frame() %>% remove_missing() %>% as.data.frame(),', variable,', plot = F)')))
  

}


partial_plot <- data_frame(variable = skynet_models$results[[1]]$finalModel$xNames) %>% 
  mutate(varname = map(variable, as.name)) %>% 
  mutate(partials = map(.$varname, partial_foo))

partial_plot <- partial_plot %>% 
  mutate(partial = map(.$partials, as_data_frame)) %>% 
  select(variable, partial) %>% 
  unnest()


partial_plot %>% 
  ggplot() + 
  geom_line(aes(x,y, color = variable), show.legend = F) + 
  labs(y = 'Predicted Density') +
  facet_wrap(~variable, scales = 'free')

```


# Summary

So far the GFW data show a limited but promising ability to predict CPUEs outside of sample (both through K-fold cross validation and through omission of the most recent data). 

There is clearly a lot of work to be done from here, and I think that improvements in model fit can be achieved through rescaling of coefficients (e.g. we don't really care about absolute CPUE, but rather the trend). We will also need to explore the ability of the model to perform across different aggregations of fleets and species. The structural model can also be improved by integration of additional data on fuel and labor costs. 

Early results suggest then that this idea isn't insane. The most rigorous approach then will be a careful development and comparison of machine learning and structural models. However so far the machine learning approach is in the front of the pack by a decent distance. 

# Works Cited


