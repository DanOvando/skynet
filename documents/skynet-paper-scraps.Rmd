---
title: "skynet-paper-scraps"
author: "Dan Ovando"
date: "7/29/2018"
output: html_document
---

## Extras

### Which data transformation is best?

Holding everything constant except for the test-training split.

```{r}

best_data_subset <- best_fits %>%
  filter(dep_var == "density", unfished_only == FALSE) %>%
  group_by(data_subset) %>%
  mutate(median_r2 = median(r2_training, na.rm = T)) %>%
  ungroup() %>%
  mutate(data_subset = fct_reorder(data_subset, median_r2)) %>%
  arrange(desc(median_r2))

best_data_subset %>%
  ggplot() +
  geom_point(position = "dodge",aes(data_subset,r2_training), alpha = 0.5) +
  geom_point(aes(data_subset, median_r2), color = "red", size = 2) +
  coord_flip()

best_data_subset <- "skynet"


best_fits <- best_fits %>%
  filter(data_subset == best_data_subset)
```

Given similar median/mean performance but more variables range of other data splits, sticking with skynet as your default.

### Broad Model Performance

How well do each of the models perform across the range of scenarios evaluated? We can judge this by examining the OOS R^2^ between the observed densities and the OOS predictions for those densities. As expected, the machine learning models perform vastly better on average than either the structural or linear models. However, even the machine learning models struggled in OOS performance in some instances, for example when the data were trained on the west coast and used to predict densities in Alaska (Fig.\@ref(fig:perf-plot))



```{r perf-plot, fig.cap = "Boxplots of R^2^ between observed and out-of-sample predicted densities for each model across all model configurations. Facet name reflects the testing split"}

best_fits %>%
  group_by(test_set) %>%
  mutate(mean_r2 = mean(r2, na.rm = T)) %>%
  ungroup() %>%
  mutate(test_set = fct_reorder(test_set, mean_r2)) %>%
  gather(r2_source, r2_value, r2, r2_training) %>%
  ggplot(aes(test_set, r2_value)) +
  geom_line() +
  geom_point(aes( color = r2_source), size = 4) +
  coord_flip() +
  theme(axis.title.y = element_blank()) +
  labs(y = bquote(R^2)) +
  scale_color_manual(name = "Data Split",labels = c("Testing","Training"),values = wes_palette("Zissou1"))


```


Demonstrates that what you train on matters a lot... one question, what happens if you predict alaska with the model trained on the random west coast?


### Predicting Spatial Abudance


How well can the model predict abundance at locations that are not included in the model training but are in the same geographic region as the training data? To test this, we split the data into randomized training (75% of the data) and testing (25%) splits. We train each model (using v-fold cross validation of the training data where needed for tuning) on the training data, and then use the trained model to predicted the testing data. Since the data are spatial and temporal, we expect that the model should perform fairly well at this task; densities at one location and time are correlated with densities at a nearby location and time, and therefore a model fit on a subset of data should do a good job of predicting omitted neighbors, but this is a useful starting point for model assessment.

```{r}
spatial_performance <- best_fits %>%
  filter(test_set == "random")  %>% {
    .$test_data[[1]]
  } %>%
  filter(surveyed_year == T) %>%
  group_by(survey) %>%
  mutate(sd_density = sd(density, na.rm = T)) %>%
  group_by(rounded_lat, rounded_lon, survey) %>%
  summarise(resid = mean(pred - density),
            scaled_resid = pmax(-2,pmin(2,resid / mean(sd_density))))
```


For visual clarity, we present results upscaled to the 100km^2^ resolution. At each 100km^2^ cell, we calculate the residual as the difference in predicted and observed densities of fish. The median of the residuals is `r median(spatial_performance$resid)` tons, though the mean is `r mean(spatial_performance$resid)`, driven negative by a long left tail in the residuals, corresponding to a region in the north central Eastern Bering Sea where the model dramatically under-predicted the observed abundance of fish. However for most regions we see that the model does a good job or predicting both the abundance and spatial distribution of fish (Fig. \@ref(fig:spatial-plot))

 XXNEED TO DO MEAN BY YEAR OR JUST FINAL YEAR, OTHERWISE SUMMING MORE RESIDUALS FOR SOME PLACESXX

```{r spatial-plot, fig.cap = "Spatial residuals predicted by GBM model (log(density) is dependent variable). Negative residuals (red) indicate model underpredicted the true density, positive residuals (blue) indicate the model overpredicted. Data aggregated at 100km^2^ resolution"}



spatial_performance_plot <- spatial_performance %>%
      mutate(recenter_lon = ifelse(rounded_lon < 0, 180 + (180 - abs(rounded_lon)), rounded_lon)) %>%
  dplyr::mutate(geometry = purrr::map2(recenter_lon, rounded_lat, ~ sf::st_point(x = c(.x, .y), dim = 'XY'))) %>%
  ungroup() %>%
  mutate(geometry = sf::st_sfc(geometry, crs =
                                 "+proj=longlat +datum=WGS84 +no_defs")) %>%
  sf::st_sf()

bbox <- sf::st_bbox(spatial_performance_plot)

alaska_bbox <- sf::st_bbox(spatial_performance_plot %>% filter(survey != "wcgbts"))

wc_bbox <- sf::st_bbox(spatial_performance_plot %>% filter(survey == "wcgbts"))


resid_hist <- spatial_performance %>%
  ggplot(aes(scaled_resid)) +
  geom_density(color = "black", fill = "grey", alpha = 0.5) +
  labs(x = "Scaled Residuals", caption = "Residuals divided by standard deviation of observed densities") +
  theme(axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        plot.margin = unit(c(0,0,0,0),"cm"),
        axis.text.x = element_text(size = 8))



alaska_spatial_residual_plot <- ggplot() +
  geom_sf(data = spatial_performance_plot %>% filter(survey != "wcgbts"), aes(color = scaled_resid),size = 0.5, alpha = 0.75) +
  geom_sf(data = pacific_map, shape = 21) +
  coord_sf(xlim = c(alaska_bbox['xmin'], alaska_bbox['xmax']),
  ylim = c(alaska_bbox['ymin']*.95, alaska_bbox['ymax'] * 1.1), expand = F) +
  scale_color_gradient2(low = "tomato", high = "steelblue",midpoint = 0, mid = "grey", name = "Scaled Residuals", guide = guide_colorbar(frame.colour = "black")) +
map_theme +
  labs(caption = "Alaska") +
  theme(legend.key.height = unit(1,"cm"))


wc_spatial_residual_plot <- ggplot() +
  geom_sf(data = spatial_performance_plot %>% filter(survey == "wcgbts"), aes(color = scaled_resid), size = 0.5, alpha = 0.75) +
  geom_sf(data = pacific_map) +
  coord_sf(xlim = c(wc_bbox['xmin']*.98, wc_bbox['xmax']),
  ylim = c(wc_bbox['ymin'], wc_bbox['ymax'] * 1.05), expand = F) +
  scale_color_gradient2(low = "tomato", high = "steelblue",midpoint = 0, mid = "grey", name = "Residuals", guide = "none") +
map_theme +
  labs(caption = "West Coast")


(
  wc_spatial_residual_plot + alaska_spatial_residual_plot
  ) / resid_hist  + plot_layout(nrow = 2, ncol = 1, widths = c(2,1),heights = c(2.5,1)) & theme(plot.margin = unit(rep(0.01,4), "points"))
```

### Filling in the Gaps

### Predicting out-of-region Spatial Abundance

In Fig.\@ref(fig:spatial-plot), we are essentially using the model to fill simulated gaps in the data through our test and training splits.  However, this is not the ultimate goal of a project such as this: Ideally, we want to be able to use data such as those provided by GFW to estimate spatio-temporal abundance trends in regions completely outside of areas in which we do have fishery independent research surveys. While we do not (yet, looking at you west Africa trawl surveys....) have fishery independent data from vastly different regions to train on, we can at least split up the data that we do have.

In this case, we will now split the data into a training set comprised of all of the surveys around Alaska (ebsbts, aibts, goabts), and a test set comprised of the wcgbts. With that data, we then repeat the same exercise as the last step (calculating spatial residuals and assessing).

Our estimates of absolute density in the testing data (the west coast groundfish bottom trawl survey) are positively biased. This is perhaps unsurprising: observed densities are higher in the Alaska region than they are in the west coast region, and therefore a model trained on Alaska can be expected to predict higher biomass than might be actually observed in a lower density ecosystem (Fig.\@ref(fig:oor-plot)).


```{r spatial-res-plot, fig.cap = "Scaled spatial resisduals from a model fit to a coarse grid, applied to a finer grid"}

train_performance <- best_fits %>%
  filter(test_set == "random", data_subset == "skynet")  %>%{
    .$training_data[[1]]
  } %>%
  mutate(resid = pred - density) %>%
  mutate(split = "Training")

test_performance <- best_fits %>%
  filter(test_set == "spatial_alaska", data_subset == "skynet")  %>% {
    .$test_data[[1]]
  } %>%
  mutate(resid = pred - density) %>%
  mutate(split = "Testing")

spatial_performance <- train_performance %>%
  bind_rows(test_performance) %>%
    filter(surveyed_year == T) %>%
  group_by(survey) %>%
  mutate(scaled_resid = resid / sd(density))

spatial_performance_plot <- spatial_performance %>%
      mutate(recenter_lon = ifelse(rounded_lon < 0, 180 + (180 - abs(rounded_lon)), rounded_lon)) %>%
  dplyr::mutate(geometry = purrr::map2(recenter_lon, rounded_lat, ~ sf::st_point(x = c(.x, .y), dim = 'XY'))) %>%
  ungroup() %>%
  mutate(geometry = sf::st_sfc(geometry, crs =
                                 "+proj=longlat +datum=WGS84 +no_defs")) %>%
  sf::st_sf()

bbox <- sf::st_bbox(spatial_performance_plot)


resid_hist <- spatial_performance %>%
  ggplot(aes(scaled_resid,fill = split)) +
  geom_density(alpha = 0.75) +
  labs(x = "Scaled Residuals", title = "A)") +
  theme(axis.title.y = element_blank(),
        axis.text.y = element_blank()) +
  theme(legend.position = "right") +
  scale_fill_discrete(name = '')


spatial_residual_plot <- ggplot() +
  geom_sf(data = spatial_performance_plot %>% filter(split == "Testing"), aes(color = scaled_resid), size = 0.5, alpha = 0.75) +
  geom_sf(data = pacific_map) +
  coord_sf(xlim = c(bbox['xmin'], bbox['xmax']),
  ylim = c(bbox['ymin']*.75, bbox['ymax'] * 1.1), expand = F) +
  scale_shape(guide = FALSE) +
  scale_color_gradient2(low = "tomato", high = "steelblue",midpoint = 0, mid = "lightgrey", name = "Residuals") +
map_theme +
  labs(title = "B)")


  resid_hist + spatial_residual_plot + plot_layout(ncol = 2, nrow = 1, widths = c(1,2))

```

Given this understandable bias, we can also consider to what extent the model is able to predict the **relative** spatial density (i.e. regions with relatively density vs those with relatively lower density), even if the absolute spatial densities are off. To accomplish this, we scale the densities in each region relative to the maximum density observed in that region.

Re-scaling the densities substantially reduces the bias in the testing data, indicating that the model is much better at estimating the relative spatial density of fish within a region than the absolute value in regions that vary substantially in their mean density from the training region (Fig.\@ref(fig:rel-plot)).

```{r aggregate}


resolution_km2 <- 100

  sub_skynet_models <- skynet_models %>%
    mutate(
      run_name = glue::glue(
        "{.$dep_var}--{.$model}--{.$weight_surveys}--{.$test_sets}--{.$data_subset}"
      )
    ) %>%
  filter(model %in% c(best_ml_model),
         dep_var == "density",
         data_subset == "skynet",
         test_set %in% c("random","spatial","alaska", "random_west_coast", "west_coast","year_greq_than_2014"),
         variables == "gfw_only")  %>%
    mutate(test_data = map(test_data, ~.x %>% filter(surveyed_year == T)),
           training_data = map(training_data, ~.x %>% filter(surveyed_year == T)))


downscaled_results <-
    cross_df(list(
      run_name = sub_skynet_models$run_name,
      resolution = resolution_km2
    )) %>%
    left_join(
      sub_skynet_models %>% select(
        run_name,
        test_data,
        training_data,
        dep_var,
        model,
        weight_surveys,
        train_set,
        test_set,
        data_subset
      ),
      by = "run_name"
    ) %>%
    arrange(run_name)

  downscaled_results <-  downscaled_results %>%
    mutate(
      training_grid  = map2(
        training_data,
        resolution,
        create_grid,
        lon_name = rounded_lon,
        lat_name = rounded_lat
      )
    ) %>%
    mutate(
      training_data = map2(
        training_data,
        training_grid,
        snap_to_grid,
        old_lon_name = rounded_lon,
        old_lat_name = rounded_lat,
        new_lon_name = lon,
        new_lat_name = lat,
        dep_var = "density"
      )
    )

    downscaled_results <-  downscaled_results %>%
    mutate(
      testing_grid  = map2(
        test_data,
        resolution,
        create_grid,
        lon_name = rounded_lon,
        lat_name = rounded_lat
      )
    ) %>%
    mutate(
      test_data = map2(
        test_data,
        testing_grid,
        snap_to_grid,
        old_lon_name = rounded_lon,
        old_lat_name = rounded_lat,
        new_lon_name = lon,
        new_lat_name = lat,
        dep_var = "density"
      )
    )
```


```{r out-of-region-plot, fig.cap = "Relative out-of-region "}

train_performance <- downscaled_results %>%
  select(test_set,train_set, training_data) %>%
  unnest() %>%
  group_by(train_set) %>%
  mutate(rel_pred = agg_pred / max(agg_pred),
         rel_density = agg_mean_density / max(agg_mean_density)) %>%
  mutate(split = "Training")

test_performance <- downscaled_results %>%
  select(test_set,train_set, test_data) %>%
  unnest() %>%
  group_by(test_set) %>%
  mutate(rel_pred = agg_pred / max(agg_pred),
         rel_density = agg_mean_density / max(agg_mean_density)) %>%
  mutate(split = "Testing")

spatial_performance <- train_performance %>%
  bind_rows(test_performance) %>%
  ungroup() %>%
    filter(!str_detect(test_set, "year_")) %>%
  mutate(set_order = as.numeric(factor(test_set))) %>%
  mutate(test_set = fct_reorder(test_set, set_order),
         train_set = fct_reorder(train_set, set_order))

trained_plot <- spatial_performance %>%
  filter(split == "Training") %>%
  ggplot(aes(rel_density, rel_pred)) +
  geom_point() +
  facet_wrap(~ train_set, strip.position = "left", nrow = n_distinct(spatial_performance$test_set), ncol = 1 ) +
  labs(title = "Trained on...", y = "Predicted")  +
    theme(axis.title.x = element_blank())


tested_plot <- spatial_performance %>%
  filter(split == "Testing") %>%
  ggplot(aes(rel_density, rel_pred)) +
  geom_point() +
  facet_wrap(~ test_set, strip.position = "right", nrow = n_distinct(spatial_performance$test_set), ncol = 1 ) +
  labs(title = "Tested on...", x = "Observed") +
  theme(axis.title.y = element_blank())

trained_plot + tested_plot &
  theme(
  panel.spacing = unit(10, "points"),
  axis.text.x = element_blank(),
  axis.text.y = element_blank(),
  strip.text = element_text(size = 7),
  plot.margin = unit(rep(.1, 4), "lines")
  )


```


### Predicting Trends

Understanding the spatial distribution of fish is important since it can help us understand expansion, contraction, and shifts in fish ranges, whether caused by fishing pressure of environmental factors such as climate change.

From the perspective understanding population dynamics though, it is also critical to understand trends in overall population abundance. To that extent, how well does this model based on fishing effort predict trends in total species abundance? To accomplish this, in each year we take our estimated and observed densities (ton/km^2^) per block, multiply the density in each block by the area of that block, and then sum the estimated and observed abundances in each block.

We begin by examining the ability of the model to replicate trends observed in the testing data, using the "random" data splitting scenario  (where the testing data and training data cover the same geographic and temporal ranges). Under these somewhat ideal circumstances, we see that the model is able to predict the trends in abundance for both the training and testing data (Fig.\@ref(fig:random-time-plot)).


```{r random-time-plot, fig.cap = "Mean scaled observed and predicted trends in total abundance over time for the training and test splits (where splits are randomly sampled)"}


train_performance <- downscaled_results %>%
    filter(test_set == "spatial") %>% {
    .$training_data[[1]]
  } %>%
  mutate(split = "Training")

test_performance <- downscaled_results %>%
      filter(test_set == "spatial") %>% {

    .$test_data[[1]]
  } %>%
  mutate(split = "Testing")

performance <- train_performance %>%
  bind_rows(test_performance) %>%
  group_by(approx_survey, year, split) %>%
  summarise(observed = sum(agg_mean_density * resolution_km2),
            predicted= sum(agg_pred * resolution_km2)) %>%
  gather(source, value, observed:predicted) %>%
  ungroup() %>%
  group_by(approx_survey, split, source) %>%
  mutate(cs_value = (value - mean(value)))

performance %>%
  ggplot(aes(year,cs_value, color = approx_survey, linetype = source, shape = source)) +
  geom_point(size = 2) +
  geom_line(size = 1) +
  scale_color_viridis_d(guide = FALSE) +
  facet_grid(approx_survey~split, scales = "free_y") +
  theme(panel.spacing = unit(10,"points"),
        axis.text.y = element_blank())



```


We have already considered the ability of the model to predict abundance in regions excluded from the training data (Fig.\@ref(fig:oor-plot)). How then does the model perform for time periods excluded from the training data? For this, we split the training data into the years 2012 and 2013, leaving 2014, 2015, and 2016 for the testing data. Our results show that the model performs relatively poorly at predicting trends in the time periods omitted from the model (Fig.\@ref(fig:oot-plot)).


XX the reason you were summing here before was becuase you were working at the 25km2 resolution, which made it kosher XX revisit this


```{r oot-plot, fig.cap="Observed and predicted abundance trends in each of the survey regions. Vertical red dashed line indicates break between training data (to the left) and testing data (at and to the right of the line)"}

time_train_performance <- downscaled_results %>%
  filter(test_set == "year_greq_than_2014") %>% {
    .$training_data[[1]]
  } %>%
      mutate(state = ifelse(approx_survey == "wcgbts",if_else(lat > 46.25, "washington", if_else(lat > 42, "oregon", "california")),"Alaska")) %>%
  group_by(state, year) %>%
  summarise(observed = sum(agg_mean_density * 25),
            predicted =  sum(agg_pred * 25)) %>%
  gather(source,value, observed:predicted) %>%
  ungroup() %>%
  mutate(split = "training")

time_test_performance <- downscaled_results %>%
    filter(test_set == "year_greq_than_2014") %>% {
    .$test_data[[1]]
  } %>%
      mutate(state = ifelse(approx_survey == "wcgbts",if_else(lat > 46.25, "washington", if_else(lat > 42, "oregon", "california")),"Alaska")) %>%
  group_by(state, year) %>%
summarise(observed = sum(agg_mean_density * 25),
            predicted =  sum(agg_pred * 25)) %>%
  gather(source,value, observed:predicted) %>%
  ungroup() %>%
  mutate(split = "testing")

time_test_performance %>%
  bind_rows(time_train_performance) %>%
  ungroup() %>%
  group_by(state, source) %>%
  mutate(sd_value = sd(value),
         mean_value  = mean(value)) %>%
  ungroup() %>%
  mutate(cs_value = (value - mean_value) / sd_value) %>%
  ggplot(aes(year, value, color = source)) +
  geom_vline(aes(xintercept = 2014), color = "red", linetype = 2, alpha = 0.75) +
  geom_line() +
  geom_point() +
  facet_wrap(~state, scales = "free_y")

```



## Value of Information

Now, comapre GFW only to GFW + environment

```{r}

skynet_models %>%
  filter(data_subset == "skynet", model == best_ml_model,
         test_set == "random", dep_var == "density") %>%
  select(variables, r2, r2_training) %>%
  gather(r2_source, r2_value, -variables) %>%
  ggplot(aes(variables, r2_value, fill = r2_source)) +
  geom_col(position = "dodge")

skynet_models %>%
  filter(data_subset == "skynet", model == best_ml_model,
         test_set == "random", dep_var == "density") %>%
  select(variables, rmse, rmse_training) %>%
  gather(rmse_source, rmse_value, -variables) %>%
  ggplot(aes(variables, rmse_value, fill = rmse_source)) +
  geom_col(position = "dodge")




```


# Conclusions/Discussion

## Diagnostics

```{r unfished-better-plot}
best_fits %>%
  filter(data_subset %in% c("skynet", "unfished_skynet")) %>%
  group_by(test_sets, unfished_only) %>%
  summarise(max_r2 = max(r2_training)) %>%
  ungroup() %>%
  group_by(test_sets) %>%
  mutate(mean_max_r2 = mean(max_r2)) %>%
  ungroup() %>%
  mutate(test_sets = fct_reorder(test_sets, mean_max_r2)) %>%
  ggplot(aes(test_sets,max_r2)) +
  geom_line() +
  geom_point(shape = 21, size = 4, aes(fill = unfished_only)) +
  coord_flip()
```




## Correlations

We can examine simple correlations between our data and the estimated total density of fished species at a given knot in a given year (Fig. \@ref(fig:var-cor)).

```{r var-cor, eval = F,fig.cap= 'Smoothed visual correlations between covariates and total fished species density'}

skynet_data %>%
  ungroup() %>%
  dplyr::select(log_density, dist_from_port, total_hours,total_engine_power,mean_chlorophyll) %>%
  gather('variable','value', -log_density) %>%
  ggplot() +
  geom_smooth(aes(value, log_density, color = variable), show.legend = F) +
  labs(y = 'Total Density of Fished Species') +
  facet_wrap(~variable, scales = 'free') +
  theme_minimal() +
  theme(axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        strip.text = element_text(size = 8),
        panel.spacing = unit(.1, 'lines'))


```


## More things

```{r, eval = F}

#cody-style map plot, pretty but not much added information for all the work

 skynet_box <- skynet_data %>%
    group_by(knot, year, rounded_lat, rounded_lon) %>%
    summarise(total_density = sum(density)) %>%
    ungroup() %>%
      mutate(recenter_lon = ifelse(rounded_lon < 0, 180 + (180 - abs(rounded_lon)), rounded_lon))


  skynet_map <-  skynet_box %>%
  dplyr::mutate(geometry = purrr::map2(recenter_lon, rounded_lat, ~ sf::st_point(x = c(.x, .y), dim = 'XY'))) %>%
  ungroup() %>%
  mutate(geometry = sf::st_sfc(geometry, crs =
                                 "+proj=longlat +datum=WGS84 +no_defs")) %>%
  sf::st_sf()

bbox <- sf::st_bbox(skynet_map)


base_map <- pacific_map %>%
ggplot() +
  geom_sf() +
  coord_sf(xlim = c(bbox['xmin'], bbox['xmax']),
  ylim = c(bbox['ymin']*.75, bbox['ymax'] * 1.1), expand = F) +
theme_ipsum() +
  theme(panel.grid = element_blank(),
        text = element_blank(),
         plot.margin = unit(c(0,1,0,1),"cm"))

survey_positions <- skynet_data %>%
  group_by(survey) %>%
  summarise(mean_lat = median(rounded_lat),
            mean_lon = median(rounded_lat)) %>%
  ungroup() %>%
  arrange(desc(mean_lat))

random_gbm_abundance <- abundance %>%
  filter(train_set == "random", model == "gbm") %>%
  group_by(year, approx_survey) %>%
  gather(abundance_source, abundance, contains("abundance")) %>%
  left_join(survey_positions, by = c("approx_survey" = "survey"))

trend_plot <- random_gbm_abundance %>%
  ungroup() %>%
  filter(str_detect(abundance_source, "scaled"),
         approx_survey != "goabts") %>%
  ggplot(aes(year + -mean_lon * 0.3, 2.5*abundance + mean_lat, color = approx_survey, linetype = abundance_source)) +
  geom_smooth(show.legend = F) +
  geom_point(show.legend = F, size = 1.5) +
  theme_ipsum() +
    theme(panel.grid = element_blank(),
        text = element_blank(),
        plot.margin = unit(c(0,0,0,0),"cm"))

  trend_map <-  ggdraw(base_map) +
  draw_plot(trend_plot, y = -.01,width = 0.7, height = 0.9)
```

### Fig.6 out-of-sample-and-time performance


```{r eval = F}
spatial_trend_performance <- spatial_performance %>%
  group_by(year, survey,split) %>%
  summarise(observed = sum(exp(log_density) * 100),
            predicted =  sum(exp(pred) * 100)) %>%
  gather(source,value, observed:predicted) %>%
  ungroup()

spatial_trend_performance %>%
  ggplot(aes(year,value, color = survey, linetype = source)) +
  geom_line() +
  facet_wrap(survey~split, scales = "free_y")

```

```{r eval = F}
spatial_trend_performance <- spatial_performance %>%
  group_by(year, survey,split) %>%
  summarise(observed = sum(exp(log_density) * 100),
            predicted =  sum(exp(pred) * 100)) %>%
  gather(source,value, observed:predicted) %>%
  ungroup()

spatial_trend_performance %>%
  group_by(survey, split,source) %>%
  mutate(cs_value =  (value - mean(value))  / sd(value)) %>%
  ungroup() %>%
  ggplot(aes(year,cs_value, color = survey, linetype = source)) +
  geom_line() +
  facet_wrap(survey~split, scales = "free_y")

```



The random forest allows us to classify each variable by its "importance", a measure of the degree to which the fit of the model decreases when that variable is omitted from the model.

(Fig. \@ref(fig:var-imp)).

Plot of variable importance

```{r eval = F}

ml_models <- skynet_models %>%
  filter(model %in% c('rf','gbm')) %>%
  mutate(varimp = map(results, ~varImp(.x$finalModel) %>% as.data.frame() %>% mutate(variable = rownames(.))))

varimp <- ml_models %>%
  select(model, data_subset,varimp) %>%
  unnest() %>%
  group_by(variable) %>%
  mutate(mean_importance = mean(Overall)) %>%
  arrange(desc(mean_importance)) %>%
  ungroup() %>%
  mutate(variable = fct_reorder(variable, mean_importance)) %>%
  filter(model == 'rf')


varimp %>%
  ggplot() +
  geom_boxplot(aes(variable, Overall, fill = model)) +
  coord_flip() +
  facet_wrap( ~ data_subset)




```


Plot of r2/mse by model type

```{r eval = F}

skynet_models %>%
  filter(data_subset == 'skynet') %>%
  ggplot(aes(pmax(0,psuedo_r2_training), fill = model)) +
  geom_histogram(show.legend = F) +
  facet_wrap(model~dep_var, scales = 'free') +
  theme(axis.text.x = element_text(size = 8),
        axis.text.y = element_text(size = 8))

```

```{r eval = F}

skynet_models %>%
  filter(model == 'rf', test_sets == 'historic')

skynet_models %>%
    filter(data_subset == 'skynet') %>%
  ggplot(aes(pmax(0,psuedo_r2), fill = model)) +
  geom_histogram(show.legend = F) +
  facet_wrap(model~dep_var, scales = 'free') +
  theme(axis.text.x = element_text(size = 8),
        axis.text.y = element_text(size = 8))

```


Time series of abundance observed vs predicted


We can also construct partial dependence plots, which visualize the marginal effect of each independent variable (Fig. \@ref(fig:partial-plot)).

```{r eval = F,partial-plot, fig.cap='Partial dependence plots of variables in random forest', eval = F}

partial_foo <- function(variable) {

  eval(parse(text = paste0('a <- randomForest::partialPlot(skynet_models$results[[1]]$finalModel
, skynet_models$training_data[[1]] %>% as.data.frame() %>% remove_missing() %>% as.data.frame(),', variable,', plot = F)')))


}


partial_plot <- data_frame(variable = skynet_models$results[[1]]$finalModel$xNames) %>%
  mutate(varname = map(variable, as.name)) %>%
  mutate(partials = map(.$varname, partial_foo))

partial_plot <- partial_plot %>%
  mutate(partial = map(.$partials, as_data_frame)) %>%
  select(variable, partial) %>%
  unnest()


partial_plot %>%
  ggplot() +
  geom_line(aes(x,y, color = variable), show.legend = F) +
  labs(y = 'Predicted Density') +
  facet_wrap(~variable, scales = 'free')

```
